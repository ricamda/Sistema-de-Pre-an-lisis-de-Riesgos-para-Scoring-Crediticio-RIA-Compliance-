Action ID,Action Name,Action Definition,Action Source,MitigationCode
A0086_Future of Life Institute2024,AI Safety Team,"Does your organization have a safety team? If yes, please provide the team name, a URL to the team’s website (if applicable), the team size (defined as FTE technical staff), and briefly describe its mission.",Future of Life Institute2024,1.1 Board Structure & Oversight
A0089_Future of Life Institute2024,Board of directors,Information about board independence; any non-standard safety-related powers; whether it has a mandate to prioritize safety over profits.,Future of Life Institute2024,1.1 Board Structure & Oversight
A0091_Future of Life Institute2024,BOD formal risk committee,"Does the board of directors feature a formal risk committee that is tasked with overseeing the firm’s risk management practices? If yes, please name the members of this committee.",Future of Life Institute2024,1.1 Board Structure & Oversight
A0092_Future of Life Institute2024,BOD regular crisis training,Does the BOD engage in regular crisis response training?,Future of Life Institute2024,1.1 Board Structure & Oversight
A0096_Future of Life Institute2024,Chief Information Security Officer (CISO),Does your firm's leadership team feature a chief information security officer (CISO)?,Future of Life Institute2024,1.1 Board Structure & Oversight
A0099_Future of Life Institute2024,Company structure,Information that indicates whether the structure of the firm would allow it to prioritize safety in critical situations or whether shareholder pressure could drive it to deploy capable but dangerous systems.,Future of Life Institute2024,1.1 Board Structure & Oversight
A0123_Future of Life Institute2024,Internal review of high-stakes deployment decisions,Does your company have one or more internal bodies that review deployment decisions related to highly capable AI models? This might be an ethics board or other body with a responsibility/safety related mandate.,Future of Life Institute2024,1.1 Board Structure & Oversight
A0129_Future of Life Institute2024,Multi-party authorisation for LLM deployment system changes,Does your organization require multi-party authorization for all changes to systems directly involved in the deployment of large models?,Future of Life Institute2024,1.1 Board Structure & Oversight
A0133_Future of Life Institute2024,Partnerships,Any partnerships that can significantly shape company strategy.,Future of Life Institute2024,1.1 Board Structure & Oversight
A0136_Future of Life Institute2024,Powers of the BOD,"Does the board have powers beyond appointing a new CEO (e.g., is it able to veto large deployment decisions?) ",Future of Life Institute2024,1.1 Board Structure & Oversight
A0164_Future of Life Institute2024,Chief Risk Officer (CRO),,Future of Life Institute2024,1.1 Board Structure & Oversight
A0176_Schuett2023,Board risk committee,"AGI labs should have a board risk committee, i.e. a permanent committee within the board of directors which oversees the lab’s risk management practices.*",Schuett2023,1.1 Board Structure & Oversight
A0179_Schuett2023,Dual control,"Critical decisions in model development and deployment should be made by at least two people (e.g. promotion to production, changes to training datasets, or modifications to production).*",Schuett2023,1.1 Board Structure & Oversight
A0217_Schuett2023,Chief risk officer,"AGI labs should have a chief risk officer (CRO), i.e. a senior executive who is responsible for risk management.",Schuett2023,1.1 Board Structure & Oversight
A0275_EU AI Office2025,Systemic Risk Responsibility Allocation,"Signatories commit to (1) clearly defining and allocating responsibilities for managing systemic risk from their GPAISRs across all levels of the organisation; (2) allocating appropriate resources to actors who have been assigned responsibilities for managing systemic risk; and (3) promoting a healthy risk culture.
",EU AI Office2025,1.1 Board Structure & Oversight
A0286_Campos2025,Decision-Making Executives,"Best practices from other industries include the establishment of clear risk ownership, with designated senior managers responsible for specific risks…",Campos2025,1.1 Board Structure & Oversight
A0334_Campos2025,Appoint Chief Risk Officer,"There should be a senior executive responsible for risk management processes (often called a Chief Risk Officer) who is accountable for the risk management processes, but is importantly not a risk owner making risk decisions themselves.",Campos2025,1.1 Board Structure & Oversight
A0338_Campos2025,Board Level Oversight,"Board-level oversight is necessary to provide checks and balances to senior management.
",Campos2025,1.1 Board Structure & Oversight
A0414_NIST2024,Organizational Role Adjustment,"Consider adjustment of organizational roles and components across lifecycle stages of large or complex GAI systems, including: Test and evaluation, validation, and red-teaming of GAI systems; GAI content moderation; GAI system development and engineering; Increased accessibility of GAI tools, interfaces, and systems, Incident response and containment.",NIST2024,1.1 Board Structure & Oversight
A0420_NIST2024,Lifecycle Oversight Functions,"Establish policies, procedures, and processes for oversight functions (e.g., senior leadership, legal, compliance, including internal evaluation) across the GAI lifecycle, from problem formulation and supply chains to system decommission.",NIST2024,1.1 Board Structure & Oversight
A0448_NIST2024,Interdisciplinary Team Formation,"Establish and empower interdisciplinary teams that reflect a wide range of capabilities, competencies, demographic groups, domain expertise, educational backgrounds, lived experiences, professions, and skills across the enterprise to inform and conduct risk measurement and management functions.",NIST2024,1.1 Board Structure & Oversight
A0460_NIST2024,Transparency Training Adaptation,Adapt existing training programs to include modules on digital content transparency,NIST2024,1.1 Board Structure & Oversight
A0461_NIST2024,Risk Management Certification,"Develop certification programs that test proficiency in managing GAI risks and interpreting content provenance, relevant to specific industry and context.",NIST2024,1.1 Board Structure & Oversight
A0642_Barrett2024,Executive Risk Accountability,"Executive leadership
of the organization
takes responsibility for
decisions about risks
associated with AI sys-
tem development and
deployment.",Barrett2024,1.1 Board Structure & Oversight
A0644_Barrett2024, Human-AI Oversight Roles,"Policies and procedures
are in place to define
and differentiate roles
and responsibilities for
human-AI configurations
and oversight of AI
systems.",Barrett2024,1.1 Board Structure & Oversight
A0665_Barrett2024,Define Human Oversight Protocols,"Processes for
human oversight are
defined, assessed,
and documented
in accordance with
organizational policies
from the Govern
function",Barrett2024,1.1 Board Structure & Oversight
A0748_NIST2024,Information Integrity; Value Chain and Component Integration,"Leverage feedback and recommendations from organizational boards or
committees related to the deployment of GAI applications and content
provenance when using third-party pre-trained models.",NIST2024,1.1 Board Structure & Oversight
A0783_UK Government2023,Robust and Meaningful Accountability Mechanisms ,"Introduce robust and meaningful accountability mechanisms, especially in evaluating capabilities thresholds, with clear processes that ensure the correct mitigations or courses of action are followed if the thresholds are met. This may include board sign-off for the responsible capability scaling policy, and named individual accountability for key decisions.",UK Government2023,1.1 Board Structure & Oversight
A0827_UK Government2023,Training Relevant Actors in AI Security,"Train developers, system owners and senior leaders in secure AI practices. It is crucial to establish a positive security culture where leaders demonstrate good security practice and staff from across a project understand enough about AI security to understand the potential consequences of decisions they take.",UK Government2023,1.1 Board Structure & Oversight
A0847_UK Government2023,Board-level Responsibility for Protective Security ,Ensure board-level responsibility for protective security with regular engagement with key stakeholders from across the business and a firm understanding of the risks the organisation faces. Ensure stakeholder engagement throughout the business for specialist insight and development and implementation of an insider risk mitigation programme. ,UK Government2023,1.1 Board Structure & Oversight
A0850_UK Government2023,Proportionate Policies ,"Put in place proportionate policies, clear reporting procedures and escalation guidelines that are accessible, understood and consistently enforced.",UK Government2023,1.1 Board Structure & Oversight
A0851_UK Government2023,Appropriate Security Training,Provide appropriate security education and training for all workers. Without effective education and training individuals cannot be expected to know what procedures are in place to maintain security.,UK Government2023,1.1 Board Structure & Oversight
A0852_UK Government2023,Program of Monitoring for Security Issues,"Ensure that a programme of monitoring and review is in place to enable potential security issues, or personal issues that may impact on an employee's work, to be recognised and dealt with effectively throughout their career",UK Government2023,1.1 Board Structure & Oversight
A0906_UK Government2023,Governance Responses to Worst-case of Misuse Scenarios,"Establish governance processes to ensure internal clarity in response to worst-case or consistent misuse scenarios. Such processes could draw on similar accountability and governance mechanisms used within a Responsible Capability Scaling policy. It may be valuable to clarify the approvals needed to roll back a model to a previous version or withdraw a model, the types of misuse that would warrant such actions, and the timeframe under which such actions would occur.",UK Government2023,1.1 Board Structure & Oversight
A1034_Uuk2024,Risk-focused governance structures,"Companies adopt practices typical of high-reliability organizations (HROs), including board risk committees, chief risk officers, multi-party authorization requirements, ethics boards for reviewing development and deployment decisions, and internal audit teams that report directly to the board, tasked with auditing risk management practices.",Uuk2024,1.1 Board Structure & Oversight
A0003_Bengio2025,Bowtie Method,"A technique for visualising risk quantitatively and qualitatively, providing clear differentiation between proactive and reactive risk management, intended to help prevent and mitigate major accident hazards.

Oil companies and national governments use the bowtie method. ",Bengio2025,1.2 Risk Management
A0004_Bengio2025,Defence in Depth,"The idea that multiple independent and overlapping layers of defence can be implemented such that if one fails, others will still be effective.

An example comes from the field of infectious diseases, where multiple preventative measures (e.g. vaccines, masks, hand washing) can layer to reduce overall risk. ",Bengio2025,1.2 Risk Management
A0012_Bengio2025,Risk management frameworks,"Whole organisation frameworks to reduce gaps in risk coverage and ensure various risk activities (i.e. all of the above) are cohesively structured and aligned, risk roles and responsibilities are clearly defined, and checks and balances are in place to avoid silos and manage conflicts of interest.

In other safety critical industries, the Three Lines of Defence framework – separating risk ownership, oversight and audit – is widely used and can be usefully applied to advanced AI companies",Bengio2025,1.2 Risk Management
A0013_Bengio2025,Risk Register,"A risk management tool that serves as a repository of all risks, their prioritisation, owners, and mitigation plans. They are sometimes used to fulfil regulatory compliance.

Risk registers are a relatively standard tool used across many industries, including cybersecurity and recently AI. ",Bengio2025,1.2 Risk Management
A0014_Bengio2025,Risk Taxonomy,"A way to categorise and organise risks across multiple dimensions.

There are several well- known risk taxonomies for AI. ",Bengio2025,1.2 Risk Management
A0016_Bengio2025,Risk Tolerance,"The level of risk an organisation is willing to take on.

 In AI, risks tolerances are often left up to AI companies, but regulatory regimes can help identify unacceptable risks that are legally prohibited",Bengio2025,1.2 Risk Management
A0020_Bengio2025,Scenario Analysis,Developing plausible future scenarios and analysing how risks materialise. Scenario analysis and planning are widely used across industries including for the energy sector and to address uncertainties of power systems. ,Bengio2025,1.2 Risk Management
A0021_Bengio2025,Threat Modelling,A process to identify threats and vulnerabilities to a system. Threat modelling is commonly used to support AI security throughout AI research and development,Bengio2025,1.2 Risk Management
A0023_Bengio2025,Delphi Method,"A group decision-making technique that uses a series of questionnaires to gather consensus from a panel of experts.

The Delphi method has been used to help identify key AI risks. ",Bengio2025,1.2 Risk Management
A0025_Bengio2025,Risk Matrices,"A visual tool that helps prioritise risks according to their likelihood of occurrence and potential impact.

Risk matrices are used in many industries and for many purposes, such as by financial institutions for evaluating credit risk, or by companies to assess possible disruptions to their supply chains",Bengio2025,1.2 Risk Management
A0031_Casper2025,Independent third-party risk assessments,"Developers can be required to have an independent third-party conduct and produce a report (including access, methods, and findings) on risk assessments of frontier systems (Raji et al., 2022; Anderljung et al., 2023; Casper et al., 2024). They can also be required to document if and what “safe harbor” policies they have to facilitate independent evaluation and red-teaming (Longpre et al., 2024).",Casper2025,1.2 Risk Management
A0032_Casper2025,Internal risk assessments,Developers can be required to conduct and report on internal risk assessments of frontier systems.,Casper2025,1.2 Risk Management
A0075_Eisenberg2025,Establish AI legal compliance process,"Evaluate and document how the AI system complies with relevant regulations and standards, identifying use case-specific legal risks and required controls. Apply the organization’s legal compliance framework to ensure appropriate safeguards are in place, with clear documentation of compliance assessments and risk mitigations.",Eisenberg2025,1.2 Risk Management
A0079_Eisenberg2025,Establish AI Risk Management System,"Implement a comprehensive AI risk management system including risk assessment processes, monitoring frameworks, governance structures, and response procedures.",Eisenberg2025,1.2 Risk Management
A0084_Future of Life Institute2024,Adopted Risk Management Frameworks,"Does your firm implement any of the following risk management approaches?

ISO 31000
NIST AI Risk Management Framework
The 3 Lines of Defense Model (3LOD)",Future of Life Institute2024,1.2 Risk Management
A0103_Future of Life Institute2024,Comprehensive pre-training risk assessments,"Does your firm conduct comprehensive pre-training risk assessments? Such assessments should include forecasting (dangerous) capabilities and developing a model-specific risk taxonomy that includes reasonably foreseeable impacts on individuals, groups, organizations, and society. The taxonomy should include misuse cases and scenarios where malicious actors steal the model.",Future of Life Institute2024,1.2 Risk Management
A0119_Future of Life Institute2024,Implementation of cybersecurity frameworks,AGI labs should comply with information security standards (e.g. ISO/IEC 27001 or NIST Cybersecurity Framework). These standards need to be tailored to an AGI context.,Future of Life Institute2024,1.2 Risk Management
A0120_Future of Life Institute2024,Independent Delphi Risk Assessments,Is your organization collaborating with independent experts to conduct full Delphi processes to more accurately assess the risks associated with large development or deployment decisions?,Future of Life Institute2024,1.2 Risk Management
A0132_Future of Life Institute2024,Ongoing model risk assessments,"Is your organization committed to regularly repeating risk assessments for its most capable models to account for progress in post-deployment model enhancements (e.g., scaffolding programs, tool use, prompt engineering)? If yes, please comment on the frequency and scope of these repeated model-specific risk assessments.",Future of Life Institute2024,1.2 Risk Management
A0139_Future of Life Institute2024,Pre-development risk assessments,Any information related to risk assessments and forecasts of dangerous capabilities conducted before large models are trained.,Future of Life Institute2024,1.2 Risk Management
A0140_Future of Life Institute2024,Pre-specified risk tolerance,"Does the firm pre-specify its risk tolerance as part of its risk management approach to prevent unacceptable risks? If your firm sets any quantitative risk thresholds, please describe them here.",Future of Life Institute2024,1.2 Risk Management
A0181_Schuett2023,Enterprise risk management,AGI labs should implement an enterprise risk management (ERM) framework (e.g. the NIST AI Risk Management Framework or ISO 31000). This framework should be tailored to an AGI context and primarily focus on the lab’s impact on society.,Schuett2023,1.2 Risk Management
A0186_Schuett2023,Internal review before publications,"Before publishing research, AGI labs should conduct an internal review to assess potential harms.",Schuett2023,1.2 Risk Management
A0197_Schuett2023,Pre-deployment risk assessment,"AGI labs should take extensive measures to identify, analyze, and evaluate risks from powerful models before deploying them.",Schuett2023,1.2 Risk Management
A0198_Schuett2023,Pre-training risk assessment,AGI labs should conduct a risk assessment before training powerful models.,Schuett2023,1.2 Risk Management
A0253_Wiener2024,Developers “shall consider” guidance from US AISI and NIST and other reputable standard-setting organizations,,Wiener2024,1.2 Risk Management
A0258_Wiener2024,Yearly re-evaluation of safety procedures,,Wiener2024,1.2 Risk Management
A0265_EU AI Office2025,Systemic Risk Analysis ,"Signatories commit to carrying out a rigorous analysis of the systemic risks identified in order to understand the severity and probability of the systemic risks. Signatories commit to making use of a range of information and methods in their systemic risk analysis including model-independent information and state-of-the-art model evaluations, taking into account model affordances, safe originator models, and the context in which the model may be made available on the market and/or used and its effects",EU AI Office2025,1.2 Risk Management
A0266_EU AI Office2025,Systemic risk assessment and mitigations,"Signatories commit to conducting systemic risk assessment at appropriate points along the entire model lifecycle, in particular before making the model available on the market. Specifically, signatories commit to starting to assess and mitigate systemic risks during the development of a GPAISR. ",EU AI Office2025,1.2 Risk Management
A0272_EU AI Office2025,Systemic Risk Acceptance Documentation,"Signatories commit to determining the acceptability of the systemic risks stemming from their GPAISRs by comparing the results of their systemic risk analysis to their pre-defined systemic risk acceptance criteria, in order to ensure proportionality between the systemic risks of the GPAISR and their mitigations. ",EU AI Office2025,1.2 Risk Management
A0274_EU AI Office2025,Adequacy Assessments,Signatories commit to assessing the adequacy of their safety and security framework and to updating it based on new findings,EU AI Office2025,1.2 Risk Management
A0276_Campos2025,Classification of applicable known risks,"Developers should address known risks in the literature using resources such as (Weidinger et al., 2022) or the AI Risk Repository (Slattery, et al., 2024). Developers should only exclude risks from the scope of their assessment in case of scientific agreement that the specific risk is negligible or unlikely to apply to the AI model under consideration. This decision should be clearly justified and documented.",Campos2025,1.2 Risk Management
A0283_Campos2025,Risk Register,"Throughout the risk management process, developers should maintain a continuously up-to-date risk register, which serves as the central repository for documenting and tracking all identified risks and their associated mitigation measures. This repository should include information like ownership, risk levels, indicators, mitigation status, and actionable response plans.",Campos2025,1.2 Risk Management
A0335_Campos2025,Enterprise Risk Management Function,"To provide support to the Chief Risk Officer, it is common in many industries to have a central risk function... In most industries, this function is known as Enterprise Risk Management (ERM).",Campos2025,1.2 Risk Management
A0391_NIST2024,Risk Tier Definition,"Consider the following factors when updating or defining risk tiers for GAI: Abuses and impacts to information integrity; Dependencies between GAI and other IT or data systems; Harm to fundamental rights or public safety; Presentation of obscene, objectionable, offensive, discriminatory, invalid or untruthful output; Psychological impacts to humans (e.g., anthropomorphization, algorithmic aversion, emotional entanglement); Possibility for malicious use; Whether the system introduces significant new security vulnerabilities; Anticipated system impact on some groups compared to others; Unreliable decision making capabilities, validity, adaptability, and variability of GAI system performance over time.",NIST2024,1.2 Risk Management
A0395_NIST2024,Risk Hierarchy Maintenance ,"Maintain an updated hierarchy of identified and expected GAI risks connected to contexts of GAI model advancement and use, potentially including specialized risk levels for GAI systems that address issues such as model collapse and algorithmic monoculture.",NIST2024,1.2 Risk Management
A0396_NIST2024,Risk Tolerance Reassessment,"Reevaluate organizational risk tolerances to account for unacceptable negative risk (such as where significant negative impacts are imminent, severe harms are actually occurring, or large-scale risks could occur); and broad GAI negative risks, including: Immature safety or risk cultures related to AI and GAI design, development and deployment, public information integrity risks, including impacts on democratic processes, unknown long-term performance characteristics of GAI.",NIST2024,1.2 Risk Management
A0403_NIST2024,System Inventory Enumeration,Enumerate organizational GAI systems for incorporation into AI system inventory and adjust AI system inventory requirements to account for GAI risks.,NIST2024,1.2 Risk Management
A0404_NIST2024,Inventory Exemption Definition,Define any inventory exemptions in organizational policies for GAI systems embedded into application software.,NIST2024,1.2 Risk Management
A0418_NIST2024,Risk Measurement Improvement,"Establish policies and procedures that address continual improvement processes for GAI risk measurement. Address general risks associated with a lack of explainability and transparency in GAI systems by using ample documentation and techniques such as: application of gradient-based attributions, occlusion/term reduction, counterfactual prompts and prompt engineering, and analysis of embeddings; Assess and update risk measurement approaches at regular cadences",NIST2024,1.2 Risk Management
A0431_NIST2024,	Supplier Risk Assessment,Implement a use-cased based supplier risk assessment framework to evaluate and monitor third-party entities' performance and adherence to content provenance standards and technologies to detect anomalies and unauthorized changes; services acquisition and value chain risk management; and legal compliance.,NIST2024,1.2 Risk Management
A0435_NIST2024,Due Diligence Process Update,"Update and integrate due diligence processes for GAI acquisition and procurement vendor assessments to include intellectual property, data privacy, security, and other risks. For example, update processes to: Address solutions that may rely on embedded GAI technologies; Address ongoing monitoring, assessments, and alerting, dynamic risk assessments, and real-time reporting tools for monitoring third-party GAI risks; Consider policy adjustments across GAI modeling libraries, tools and APIs, fine-tuned models, and embedded tools; Assess GAI vendors, open-source or proprietary GAI tools, or GAI service providers against incident or vulnerability databases.",NIST2024,1.2 Risk Management
A0446_NIST2024,Risk Measurement Planning,"Document risk measurement plans to address identified risks. Plans may include, as applicable: Individual and group cognitive biases (e.g., confirmation bias, funding bias, groupthink) for AI Actors involved in the design, implementation, and use of GAI systems; Known past GAI system incidents and failure modes; In-context use and foreseeable misuse, abuse, and off-label use; Over reliance on quantitative metrics and methodologies without sufficient awareness of their limitations in the context(s) of use; Standard measurement and structured human feedback approaches; Anticipated human-AI configurations.",NIST2024,1.2 Risk Management
A0467_NIST2024,Governance Connection,"Connect new GAI policies, procedures, and processes to existing model, data, software development, and IT governance and to legal, compliance, and risk management activities.",NIST2024,1.2 Risk Management
A0478_NIST2024,Risk-Based Feedback Prioritization,Prioritize GAI structured public feedback processes based on risk assessment estimates.,NIST2024,1.2 Risk Management
A0491_NIST2024,Unmeasurable Risk Tracking,"Track and document risks or opportunities related to all GAI risks that cannot be measured quantitatively, including explanations as to why some risks cannot be measured (e.g., due to technological limitations, resource constraints, or trustworthy considerations). Include unmeasured risks in marginal risks.",NIST2024,1.2 Risk Management
A0624_Barrett2024,Risk Ownership by Capability,"Take responsibility for risk assessment and risk management tasks for which your organization has access to information, capability, or opportunity to develop capability sufficient for constructive action, or that is substantially greater than others in the value chain 
",Barrett2024,1.2 Risk Management
A0625_Barrett2024,Risk-Tolerance Thresholds,Set risk-tolerance thresholds to prevent unacceptable risks,Barrett2024,1.2 Risk Management
A0627_Barrett2024,Assess Catastrophic Risk,"Identify whether a GPAIS could lead to significant, severe, or catastrophic impacts,
e.g., because of correlated failures or errors across high-stakes deployment domains, dan-
gerous emergent behaviors or vulnerabilities, or harmful misuses and abuses
",Barrett2024,1.2 Risk Management
A0633_Barrett2024,Ensure Legal Compliance,"Legal and regulatory
requirements involving
AI are understood,
managed, and
documented.",Barrett2024,1.2 Risk Management
A0635_Barrett2024,Align Risk Management to Tolerance,"Processes, procedures,
and practices are in
place to determine
the needed level of
risk management
activities based on
the organization’s risk
tolerance.",Barrett2024,1.2 Risk Management
A0637_Barrett2024,Monitoring of RM policies,"Ongoing monitoring
and periodic review of
the risk management
process and its
outcomes are planned
and organizational roles
and responsibilities
clearly defined,
including determining
the frequency of
periodic review.",Barrett2024,1.2 Risk Management
A0638_Barrett2024,Inventory AI Systems,"Mechanisms are in
place to inventory
AI systems and are
resourced according
to organizational risk
priorities.",Barrett2024,1.2 Risk Management
A0640_Barrett2024,Define AI Risk Roles,"Roles and
responsibilities and
lines of communication
related to mapping,
measuring, and
managing AI risks are
documented and are
clear to individuals and
teams throughout the
organization.",Barrett2024,1.2 Risk Management
A0641_Barrett2024,Train on AI Risk,"The organization’s
personnel and
partners receive AI risk
management training
to enable them to
perform their duties
and responsibilities
consistent with related
policies, procedures,
and agreements",Barrett2024,1.2 Risk Management
A0643_Barrett2024,Diverse Risk Voices,"Decision-making related
to mapping, measuring,
and managing AI risks
throughout the lifecycle
is informed by a diverse
team (e.g., diversity
of demographics,
disciplines, experience,
expertise, and
backgrounds).",Barrett2024,1.2 Risk Management
A0649_Barrett2024,External Feedback on System Design,"Mechanisms are
established to enable
the team that developed
or deployed AI systems
to regularly incorporate
adjudicated feedback
from relevant AI actors
into system design and
implementation.",Barrett2024,1.2 Risk Management
A0656_Barrett2024,Risk Tolerance Thresholds,"Organizational
risk tolerances are
determined and
documented.",Barrett2024,1.2 Risk Management
A0657_Barrett2024,Elicit Requirements & Assess Impacts,"System requirements
(e.g., “the system shall
respect the privacy of
its users”) are elicited
from and understood
by relevant AI actors.
Design decisions
take socio-technical
implications into account
to address AI risks",Barrett2024,1.2 Risk Management
A0662_Barrett2024,Document AI Error Costs,"Potential costs, in-
cluding non-monetary
costs, which result from
expected or realized AI
errors or system func-
tionality and trustwor-
thiness – as connected
to organizational risk
tolerance – are exam-
ined and documented.",Barrett2024,1.2 Risk Management
A0664_Barrett2024,Document Operator Proficiency,"Processes for operator
and practitioner
proficiency with AI
system performance
and trustworthiness
– and relevant
technical standards
and certifications – are
defined, assessed, and
documented.",Barrett2024,1.2 Risk Management
A0666_Barrett2024,Document AI RM Third Party Framework,"Approaches for mapping
AI technology and legal
risks of its components
– including the use
of third-party data or
software – are in place,
followed, and docu-
mented, as are risks of
infringement of a third
party’s intellectual prop-
erty or other rights.",Barrett2024,1.2 Risk Management
A0667_Barrett2024,Document Internal & Third-Party Controls,"Internal risk controls for
components of the AI
system, including third-
party AI technologies,
are identified and
documented.",Barrett2024,1.2 Risk Management
A0668_Barrett2024,Assess & Document Risk Magnitude	,"Likelihood and
magnitude of each
identified impact (both
potentially beneficial
and harmful) based on
expected use, past uses
of AI systems in similar
contexts, public incident
reports, feedback from
those external to the
team that developed
or deployed the AI
system, or other data
are identified and
documented.",Barrett2024,1.2 Risk Management
A0669_Barrett2024,Establish Feedback Integration Processes,"Practices and personnel
for supporting regular
engagement with
relevant AI actors and
integrating feedback
about positive, negative,
and unanticipated
impacts are in place and
documented.",Barrett2024,1.2 Risk Management
A0670_Barrett2024,Select & Document Risk Metrics,"Approaches and metrics
for measurement of
AI risks enumerated
during the Map
function are selected
for implementation
starting with the most
significant AI risks. The
risks or trustworthiness
characteristics that will
not – or cannot – be
measured are properly
documented.",Barrett2024,1.2 Risk Management
A0671_Barrett2024,Evaluate Risk Metrics & Controls,"Appropriateness
of AI metrics and
effectiveness
of existing controls are
regularly assessed and
updated, including
reports of errors and
potential impacts on
affected communities.",Barrett2024,1.2 Risk Management
A0681_Barrett2024,Examine & Document Privacy Risk,"Privacy risk of the AI
system – as identified
in the Map function
– is examined and
documented.",Barrett2024,1.2 Risk Management
A0682_Barrett2024,Evaluate & Document Fairness,"Fairness and bias – as
identified in the Map
function – are evaluated
and results are
documented.",Barrett2024,1.2 Risk Management
A0685_Barrett2024,Track Emergent AI Risks,"Approaches, personnel,
and documentation
are in place to regularly
identify and track
existing, unanticipated,
and emergent AI risks
based on factors
such as intended and
actual performance in
deployed contexts.",Barrett2024,1.2 Risk Management
A0686_Barrett2024,Consider Alternatives for Hard-to-Measure Risks,"Risk tracking approaches
are considered for
settings where AI
risks are difficult to
assess using currently
available measurement
techniques or where
metrics are not yet
available.",Barrett2024,1.2 Risk Management
A0688_Barrett2024,Align Risk Metrics with Context  ,"Measurement approach-
es for identifying AI
risks are connected to
deployment context(s)
and informed through
consultation with do-
main experts and other
end users. Approaches
are documented.",Barrett2024,1.2 Risk Management
A0692_Barrett2024,Prioritize Risk Treatment,"Treatment of
documented AI risks
is prioritized based on
impact, likelihood, and
available resources or
methods.",Barrett2024,1.2 Risk Management
A0693_Barrett2024,Plan & Document Risk Responses  ,"Responses to the AI
risks deemed high
priority, as identified by
the Map function, are
developed, planned,
and documented. Risk
response options can
include mitigating,
transferring, avoiding,
or accepting.",Barrett2024,1.2 Risk Management
A0694_Barrett2024,Document Residual Risks  ,"Negative residual risks
(defined as the sum
of all
unmitigated risks)
to both downstream
acquirers of AI systems
and end users are
documented.",Barrett2024,1.2 Risk Management
A0695_Barrett2024,Consider Resources & Alternatives  ,"Resources required
to manage AI risks are
taken into account
– along with viable
non-AI alternative
systems, approaches,
or methods – to reduce
the magnitude or
likelihood of
potential impacts.",Barrett2024,1.2 Risk Management
A0699_Barrett2024,Monitor Third-Party Risks  ,"AI risks and benefits
from third-party
resources are regularly
monitored, and risk
controls are applied and
documented.",Barrett2024,1.2 Risk Management
A0726_NIST2024,Information Security,"Document trade-offs, decision processes, and relevant measurement and
feedback results for risks that do not surpass organizational risk tolerance, for
example, in the context of model release: Consider different approaches for
model release, for example, leveraging a staged release approach. Consider
release approaches in the context of the model and its projected use cases.
Mitigate, transfer, or avoid risks that surpass organizational risk tolerances.",NIST2024,1.2 Risk Management
A0742_NIST2024,Value Chain and Component Integration; Intellectual Property,"Apply organizational risk tolerances and controls (e.g., acquisition and
procurement processes; assessing personnel credentials and qualifications,
performing background checks; filtering GAI input and outputs, grounding, fine
tuning, retrieval-augmented generation) to third-party GAI resources: Apply
organizational risk tolerance to the utilization of third-party datasets and other
GAI resources; Apply organizational risk tolerances to fine-tuned third-party
models; Apply organizational risk tolerance to existing third-party models
adapted to a new domain; Reassess risk measurements after fine-tuning thirdparty GAI models",NIST2024,1.2 Risk Management
A0751_NIST2024,Information Integrity; Harmful Bias and Homogenization,"Collaborate with external researchers, industry experts, and community
representatives to maintain awareness of emerging best practices and
technologies in measuring and managing identified risks.",NIST2024,1.2 Risk Management
A0764_UK Government2023,Rigorous Risk Assessment Processes,"Develop rigorous risk assessment processes for models, which: 1) Attempt to encompass all plausible and consequential risks from AI systems, including low-probability risks of severe harm; 2) Are informed by factors including, but not limited to: Model evaluations and red teaming, evidence of previous models' impacts and capabilities, knowledge of the latest research and developments in the field, domain expertise both internally and externally, and the results of data input audits. Take into account the difficulty of producing reliable risk assessments, creating a culture that takes seriously the significant uncertainty underlying predictions of the risks associated with frontier models. Take into account the potential benefits of the model. Include learnings from information exchanges with industry, academia and government on the capabilities of comparable models
",UK Government2023,1.2 Risk Management
A0765_UK Government2023,Pre-Deployment Risk Assessments ,"Devote resources to pre-development risk assessments, alongside prioritising pre- deployment risk assessments. Pre-development risk assessments are also important, because training a high-risk system can still lead to harm if the model is leaked, stolen, or otherwise unintentionally distributed.",UK Government2023,1.2 Risk Management
A0766_UK Government2023,Development and Post-Deployment Monitoring,"Monitor systems both during development and after deployment. New risk assessments could be carried out in cases of fine-tuning or other substantial changes that could increase the danger of a model, such as the model gaining access to tools or plugins. This could occur alongside attempts to detect unexpected developments and new information that might have changed the results of the existing risk assessment, and which could also trigger a new risk assessment.",UK Government2023,1.2 Risk Management
A0767_UK Government2023,Risk Thresholds for Specific Models ,"Describe and continually refine risk assessment results for each model (“risk thresholds”) that would trigger particular risk-reducing actions, defining such results in terms of risk to all relevant stakeholders given currently existing mitigations. Given the high uncertainty around future model capabilities, risk thresholds may be refined periodically.",UK Government2023,1.2 Risk Management
A0768_UK Government2023,Defining Risk Thresholds ,"Define risk thresholds, based on the outcomes that would constitute a breach of the threshold and linked to dangerous capabilities that a given model or combination of models could exhibit. For example, a frontier AI organisation might identify the objective of avoiding deploying an AI system that significantly increases the risk of cyberattacks or fraud.",UK Government2023,1.2 Risk Management
A0769_UK Government2023,Operationalizing Risk Thresholds,"Operationalise risk thresholds, including specific, testable observations, such that multiple observers with access to the same information would agree on whether a given threshold had been met. Specific observations would provide frontier AI organisations with opportunities to determine proactively how they would respond in difficult potential situations, and so respond immediately to such situations should they arise, as well as allowing for accountability and external verification.
Given the nascent science of AI evaluation, however, it is unlikely to be possible to define a set of testable observations that detect all identified risks sufficiently reliably. Instead of relying solely on these predefined tests, risk assessments may take into account wider sources of evidence, such as concerning and unexpected observations that show up in exploratory analyses, expert forecasts, or risk related information from other frontier AI organisations. In particular, consideration may be given to any risks caused by the combination of a given model with other models or tools, whether developed by the same frontier AI organisation or otherwise.",UK Government2023,1.2 Risk Management
A0770_UK Government2023,Refinement of Risk Evaluation Frameworks ,"Continue to refine and redefine risk evaluation frameworks for models as necessary, aiming to reduce the gap between the intended objectives of risk thresholds and their present operationalisations. Such gaps are expected to exist due to limitations in the science of evaluation and in the state of knowledge surrounding capabilities, so progress towards a robust framework will probably be iterative. Risk evaluation frameworks may use multiple methods, including probability estimations and qualitative assessments of current capabilities.",UK Government2023,1.2 Risk Management
A0771_UK Government2023,Mitigation of Overshooting Risk Thresholds ,"Mitigate the risk of ‘overshooting’ risk thresholds. This may be achieved by setting deliberately conservative thresholds, including using intentionally lower buffer thresholds to trigger actions, such that the most concerning thresholds are difficult to overshoot without having already implemented mitigations at an earlier stage.",UK Government2023,1.2 Risk Management
A0772_UK Government2023,Risk Threshold Engagement with Relevant External Stakeholders,"Engage with relevant external stakeholders when developing risk thresholds. Risk thresholds often concern externalities frontier AI organisations place on society, including both the potentially significant benefits of AI advancement and negative effects that might disproportionately affect specific stakeholder groups. As such, their risk thresholds may be made public to allow for external scrutiny, with thresholds set in consultation with relevant external stakeholders including relevant government authorities.",UK Government2023,1.2 Risk Management
A0774_UK Government2023,Reassessment of Residual Risk ,"After putting in place mitigations, reassess any residual risk posed to determine whether additional mitigations are required. Due to the unpredictability of capabilities advancements and the limitations of model evaluation science, pre-agreed mitigations may prove insufficient to place a given model within a risk threshold. Risk acceptance criteria may be used, as is standard in many other contexts, and may provide an important tool for clarification. These criteria may evolve with time, and could be quantitative or qualitative. For example, risk may only be accepted if it has been reduced to a level ‘as low as reasonably practicable’.",UK Government2023,1.2 Risk Management
A0775_UK Government2023,Disclosure of Risk Threshold to Government Authorities ,"Inform relevant government authorities when a risk threshold has been met, along with proposed mitigations. Inform governments again, in advance of deployment, when the mitigations and residual risk assessment have been carried out. Proactively engage other relevant actors in addition to governments as appropriate.",UK Government2023,1.2 Risk Management
A0776_UK Government2023,Adaptation of Mitigations to Development and Deployment Stages,"
When planning required mitigations, consider the full range of development and deployment stages. In general, meeting a risk threshold could require mitigations at multiple such stages. Important stages may include the following: 1) Continued training of model; 2) Deployment of model in small-scale ways, e.g., internal use; 3) Deployment of model in large-scale ways e.g., public release via API ; 4) Extension of model through greater affordances, e.g., tool use or internet access; 5) Irreversible deployment e.g. open-sourcing of models
",UK Government2023,1.2 Risk Management
A0777_UK Government2023,Adaptation of Mitigations for Unintended Model Use,"Adapt mitigations in recognition of risks from unintended model use or use in unexpected contexts, such as a model that is modified to remove safeguards after open- sourcing or a model that is combined with another model for unanticipated purposes. For example, at a given risk threshold, information security control mitigations might be put in place before even internal use of a model within a frontier AI organisation. In general, the use of caution may be helpful, given current limitations in information security controls and the prediction of models’ emergent abilities.",UK Government2023,1.2 Risk Management
A0778_UK Government2023,Recognition of Dangerous Model Possession ,"Recognise that even the possession of some models may be dangerous, even if not deployed or not deployed widely, due to the risk of being unable to secure a model sufficiently to prevent, for instance, a bad actor obtaining the model weights. In contrast, other models may pose significant risk only if deployed in a large-scale or irreversible way.",UK Government2023,1.2 Risk Management
A0781_UK Government2023,Risks of Open-Source Models,"Acknowledge that some models may pose additional risks if made available “open-source”, even after mitigation attempts. This is because of the inability of recalling an open-sourced model and the potential ability of users to remove safeguards and introduce new (and potentially dangerous) capabilities. However, it is also important to bear in mind the significant benefits of “open-source” AI systems for researchers, including for advancing AI safety, which may in some cases outweigh these potential risks.",UK Government2023,1.2 Risk Management
A0784_UK Government2023,Effective Risk Governance,"Establish effective risk governance to ensure that risks are appropriately identified, assessed, and addressed, and their nature and scale transparently reported. Most importantly, provide internal checks and balances, which may include thoughtful separation of roles within risk management.",UK Government2023,1.2 Risk Management
A0822_UK Government2023,Documentation of the Cyber security threats ,"Assess and document the cyber security threats against the AI system overall and mitigate the impact of vulnerabilities. Good documentation and monitoring will inform your overall risk posture and help you respond in the event of a security incident.
",UK Government2023,1.2 Risk Management
A0823_UK Government2023,Value of AI-related Assets,"Understand the value of AI-related assets such as models, data (including user feedback), prompts, software, documentation, and assessments (including information about potentially unsafe capabilities and failure modes) to their organisation. Protect these different categories of information as appropriate.",UK Government2023,1.2 Risk Management
A0824_UK Government2023,Security Integration into Business Decisions,"Ensure security is factored into all business decisions and AI-related assets are identified and protected with proportionate cyber, physical and personnel security measures. Secure Innovation guidance from NCSC and NPSA is available to help companies and investors to protect their technology .",UK Government2023,1.2 Risk Management
A0828_UK Government2023,Awareness of Security Threats ,"Maintain an awareness of security threats and failure modes, in particular data scientists and developers. AI development and cyber security are two different skill sets and building a team at the intersection of the two will require effort and time.",UK Government2023,1.2 Risk Management
A0829_UK Government2023,Model of System Threats in Misuse Scenarios,"Model the threats to your system to understand the impacts to the system, users, organisation and wider society if the model is misused or behaves unexpectedly. This can help build systems where unanticipated model outputs are handled safely by other parts of a data pipeline.",UK Government2023,1.2 Risk Management
A0839_UK Government2023,Identification of Assets of Organizational Value ,"Identify assets & systems that are important for the delivery of effective operations, or are of specific organisational value (e.g. commercially sensitive information)",UK Government2023,1.2 Risk Management
A0840_UK Government2023,Categorization of Assets ,Categorise and classify assets in order to ensure that the correct level of resource is used in implementing risk mitigations. - risk management,UK Government2023,1.2 Risk Management
A0841_UK Government2023,Threat Identification,"Identify threats. These may include terrorism or hostile state threats and/or more local and specific threats, and use a range of internal and external resources.",UK Government2023,1.2 Risk Management
A0842_UK Government2023,Risk Assessment via Recognized Processes,Assess risk using recognised processes.,UK Government2023,1.2 Risk Management
A0843_UK Government2023,Protective Security Risk Register,"Build a protective security risk register to record, in sufficient detail, all the data gathered during this risk management process, ensuring compatibility with existing organisational risk management registers and processes.",UK Government2023,1.2 Risk Management
A0844_UK Government2023,Development of Protective Security Strategy ,"Develop a protective security strategy for mitigating the risks identified, which reviews protective security measures in relation to a prioritised list of risks. Where mitigations are assessed as inadequate, additional measures could be proposed for approval by the decision maker(s).",UK Government2023,1.2 Risk Management
A0845_UK Government2023,Development and Implementation Plans ,"Produce development & implementation plans. Aim to arrive at a clear, prioritised list of protective security mitigations, which span physical, personnel and cyber security disciplines, and are linked to the technical guidance needed to implement them.",UK Government2023,1.2 Risk Management
A0846_UK Government2023,Regular Risk Management Review ,"Review risk management measures regularly and when required e.g. on a change in threat or change to operational environment, or to assess the suitability of new measures implemented. More detailed description of protective security risk management is provided on the NPSA website.",UK Government2023,1.2 Risk Management
A0849_UK Government2023,Role-Based Security Risk Assessment ,"Use Role-Based Security Risk Assessment to identify physical, personnel or cyber security measures that need to be applied in order to mitigate insider risk",UK Government2023,1.2 Risk Management
A0853_UK Government2023,Use of Established Security Guidance ,"Use established, evidence based guidance to fully address personnel security risks e.g. NPSA guidance on Personnel Security",UK Government2023,1.2 Risk Management
A0927_UK Government2023,Input Data Audits for Pre-Development/deployment Risk Assessments ,"Use input data audits to inform pre-development and pre-deployment risk assessments. Data audits may be particularly valuable in pre-development risk assessments, where direct evidence of system behaviour will not be available. Input data audits can also be used to improve context-based understanding of how data impacts system capabilities, which will strengthen future risk assessments and mitigation techniques.",UK Government2023,1.2 Risk Management
A0957_Gipiškis2024,Demonstrating a “margin of safety” for the worst plausible system failures,"Model developers can demonstrate that there is an acceptable “margin of safety” between the current version of the model and a plausible version with dangerous capabilities or potential system failures, whether these arise from the model itself or through scaffolding. This “margin of safety” can be tracked and evaluated based on the model’s performance on either component tasks or proxy tasks with varying levels of difficulty [44], and it is particularly relevant for general- purpose models with emergent properties, where some of the risks, use cases, and model capabilities may be unknown even at the time of deployment.
Margin of safety (also called “safety factor”) is a common practice in many in- dustries - particularly in physical structures. It is common for this margin to be very conservative when feasible (e.g., 4+ in fasteners on critical structures). In situations where a high margin of safety is impractical, it may be supplemented by more frequent inspections and additional process controls. A lower safety factor can also be managed by adopting conservative assumptions regarding worst-case conditions.",Gipiškis2024,1.2 Risk Management
A0959_Gipiškis2024,Scenario analysis,"Scenario analysis involves development of several plausible future scenarios, where these scenarios may be generated from varying the assumptions of a small set of driving forces. The scenarios developed can be used to take further actions to improve overall preparedness [107].
For example, scenarios to explore frontier AI risks can be developed, where they can involve different assumptions on factors such as AI capability, ownership, safety, usage, and geopolitical contexts, as well as its implications on key policy issues [203].",Gipiškis2024,1.2 Risk Management
A0960_Gipiškis2024,Fishbone diagram,"The fishbone diagram, or a “cause-and-effect diagram” [107], can be used to show the potential causes of an undesirable event.
The diagram is created by first placing a specific risk event at the “head” of the diagram, typically facing the right. Then, to the left of the risk event, the “ribs” branch off from the “backbone” to represent major causes, which further branch into sub-branches to represent root-causes, extending to as many levels as required. This is typically done via backward-reasoning, where various potential causes are explored after the risk event has been selected for analysis using this method.
For example, the risk event “AI systems generate toxic content” can be placed at the “head” of the diagram, where the branches may include causal factors like “AI trained on data containing toxicity” and “successful jailbreaking of AI despite fine-tuning.”",Gipiškis2024,1.2 Risk Management
A0961_Gipiškis2024,Causal mapping,"Causal mapping is a technique used to explore and map complex interactions between cause and effect of risks. It involves coming up with potential events related to an undesirable issue, with each event represented by a text box, then clustering similar events according to themes, and finally drawing arrows to illustrate the causal relationship between the different events. The completed causal map can then be analyzed to identify central events, clusters of events, feedback loops, and other relevant patterns [107].
For example, causal mapping can be used to explore factors that lead to high- level model capabilities (e.g., “machine intelligence”), and the nodes may in- clude factors such as “concept formation” and “flexible memory,” where certain nodes may be found to be especially crucial if they have more outgoing arrows connecting them to other nodes.",Gipiškis2024,1.2 Risk Management
A0962_Gipiškis2024,Delphi technique,"The Delphi technique is a multi-round forecasting process based on a structured framework on collecting and collating multiple expert judgments. It brings the benefits of anonymous and remote participation which may result in increased likelihood estimation accuracy compared to merely averaging individual estima- tions or simple group discussions [107].Given a panel of experts, at each round, the experts are presented with an ag- gregated summary of the results from the previous round, and are then allowed to update their answers accordingly. The process ends when either a consen- sus is reached or the responses in later rounds no longer change significantly. This method enables elicitation of expert judgment while utilizing wisdom of the crowd in the process.
A potential application of the Delphi technique is to solicit expert judgment on the likelihood of systemic risks from AI development, where crucial variables identified during each round of questionnaire can be further studied for the purpose of risk mitigation.
",Gipiškis2024,1.2 Risk Management
A0963_Gipiškis2024,Cross-impact analysis,"Cross impact analysis is a forecasting methodology that analyzes the likelihood of a particular issue using expert analysis (i.e., Delphi technique) in combination with analysis of events correlated with the said issue. It involves decomposing an issue into discrete and correlated events, and then collecting expert opinion on each of those events. Analysis of each event from multiple viewpoints can yield potential future scenarios [107].
For example, an issue may be “advances in AI,” which can be broken down into two correlated events like “advances in hardware” or “advances in algorithms,” where the likelihood of occurrences of each event can be estimated via the Delphi technique while taking into account their interactions with other events.",Gipiškis2024,1.2 Risk Management
A0964_Gipiškis2024,Bow-tie analysis,"Bow-tie analysis is a method to assess the utility of implemented controls against a particular risk event. It involves centering the unwanted risk event within a diagram. On the left, the factors that can cause the event are listed, followed by the controls that will prevent or minimize the likelihood of the event. On the right, the event is assumed to happen, and the potential effects and the relevant post-hoc controls that could minimize their impact are listed [107].
For example, given a hazard where an AI model has the capability of generating potentially harmful outputs, a risk event may be an AI providing information on how to create dangerous bioweapons. The risk factors could include the use of dangerous data for training as well as a lack of fine-tuning prior to deploy- ment. The risk effects may include creation and use of bioweapons using the AI generated information. Once these risk factors and risk effects are in place, both preventive controls and post-hoc controls can be planned, such as appropriate filtering of training data and rigorous red teaming prior to model deployment as preventive barriers, as well as know-your-customer policies and model output censoring techniques as reactive barriers.",Gipiškis2024,1.2 Risk Management
A0965_Gipiškis2024,System-theoretic process analysis (STPA),"STPA is a method to assess the utility of implemented controls against a partic- ular risk within a complex system. Unlike bow-tie analysis, STPA factors in the interactions between components as events that can cause the risk in question [107]. First, the system and its boundaries to the environment are defined. The sys- tem is primarily delineated from the environment because there is at least some partial control over it. Second, several items are enumerated, including (i) un- wanted risk events (“losses”), (ii) system states that cause losses (“system-level hazards”), and (iii) system states that do not cause losses (“system-level con- straints”). Third, a diagram mapping the system, environment, their different controls, and the interactions between these elements is created. This diagram must be comprehensive in listing the different losses and possible interactions that can cause each loss. Finally, the diagram can be used to identify “unsafe control actions” (UCAs), which are the causal pathways between a control and system-level hazards, including all interactions involved.
For example, in the context of text-to-image models, losses may include ‘loss of diversity’ and ‘loss of quality’; hazards may include ‘low quality text-image pairs within training dataset’ and ‘harmful content within training dataset’; and the controls may include human controllers and automated controllers (e.g., annotators, data owners, data crawlers) [165]. Subsequent analysis may result in identification of UCAs such as ‘current data filtering actions’ and neglecting current filter thresholds’ which are linked to specific hazards. Specific actions that counter or prevent such UCAs can reduce the associated losses.",Gipiškis2024,1.2 Risk Management
A0966_Gipiškis2024,Risk matrices,"A risk matrix is a method for risk evaluation. It is a heatmap that, for each cell, shows the severity score weighted by the likelihood score of a particular risk, usually from a scale of 1-5. Two rankings are required to construct a risk matrix: a ranking for the severity of risks, and a corresponding ranking of the likelihood of risks [107].
AI-related risks can be generated using appropriate taxonomies, and placed into the relevant cells according to their assessed likelihood and severity based on predefined criteria (e.g., likelihood level 1 corresponds to < 1% chance, and likelihood level 5 corresponds to > 90% chance; while severity level 1 corresponds to mild inconveniences to the user, and severity level 5 corresponds to a fatality or financial damage upwards of $10 million, etc.), such that particular focus can be given to mitigating risks with higher weighted scores (i.e., likelihood multiplied by severity).",Gipiškis2024,1.2 Risk Management
A0967_Gipiškis2024,Pre-allocate sufficient resources for risk man- agement,"The process of conducting thorough risk management is potentially time-con- suming. Pre-allocating sufficient resources, in terms of personnel count and schedule allowances, to conduct necessary risk management activities prior to model deployment is crucial [6].
For example, a red-teaming exercise requires creative approaches to identify weaknesses of the AI system against potential adversaries, which alone may require hundreds of hours from several experts.",Gipiškis2024,1.2 Risk Management
A1028_Uuk2024,Intolerable risk thresholds,"Assessing and monitoring AI models with regard to red-line risk or capability thresholds set by a third-party, such as a standardization organization or regulator. Companies would further need to make technical, legal, and organizational preparations to halt development and deployment immediately when a breach occurs.",Uuk2024,1.2 Risk Management
A1030_Uuk2024,Pre-deployment risk assessments,Comprehensive risk assessments before deployment that would assess reasonably foreseeable misuse and include dangerous capability evaluations that incorporate post-training enhancements and collaborations with domain experts. Risk assessments would inform deployment decisions.,Uuk2024,1.2 Risk Management
A1031_Uuk2024,Pre-development risk assessments,Comprehensive risk assessments based on forecasted capabilities before training new models. Risk assessments would inform impactful development decisions.,Uuk2024,1.2 Risk Management
A0090_Future of Life Institute2024,BOD Financial Stake in Company,Do members of the board of directors hold a financial stake in the company?,Future of Life Institute2024,1.3 Conflict of Interest Protections
A0112_Future of Life Institute2024,Financial incentives,Financial incentives of leadership; whether it has a mandate to prioritize safety over profits.,Future of Life Institute2024,1.3 Conflict of Interest Protections
A0143_Future of Life Institute2024,Protections that guard critical decisions against stakeholder pressure,"Is your firm's governance structure set up in a way that would allow its leadership to prioritize safety in critical situations even if such a decision runs counter to the profit incentive (e.g., choosing not to deploy very capable yet critically dangerous AI systems)? Are there any protections that guard such decisions against shareholder pressure (e.g., in the form of lawsuits)? Are shareholders briefed that such situations might arise in the future? Please describe how your firm prioritizes safety (e.g., relevant policies, legal structure, etc.).",Future of Life Institute2024,1.3 Conflict of Interest Protections
A0153_Future of Life Institute2024,Senior executive (c-level) financial stake in company,Does any senior executive (c-level) within your firm hold a financial stake in the company?,Future of Life Institute2024,1.3 Conflict of Interest Protections
A0163_Future of Life Institute2024,Windfall Plan,Rapid advances in AI could lead to immense power concentration. Has your organization made any preparations for future scenarios in which the firm experiences extreme windfall profits? Has the organization developed a plan for redistributing vast resources to all of humanity?,Future of Life Institute2024,1.3 Conflict of Interest Protections
A0175_Schuett2023,Background checks,"AGI labs should perform rigorous background checks before hiring/appointing members of the board of directors, senior executives, and key employees.*",Schuett2023,1.3 Conflict of Interest Protections
A0278_Campos2025,Non-Interference with Third-Party Risk Findings,"Commit not to interfere with or suppress findings from third-party organizations, as their independent perspective is crucial to identify potential risks that may have been overlooked internally.",Campos2025,1.3 Conflict of Interest Protections
A0333_Campos2025,Separate Risk Ownership and Advisory Roles,"The senior managers making risk decisions need to be distinct from those advising on the decisions, to avoid conflicts of interest",Campos2025,1.3 Conflict of Interest Protections
A0022_Bengio2025,Whistleblower protection,"Whistleblowers can play an important role in alerting authorities to dangerous risks at AI companies due to the proprietary nature of many AI advancements.

Incentives and protections for whistleblowers are expected to be an important part of advanced AI risk governance. ",Bengio2025,1.4 Whistleblower Reporting & Protection
A0040_Casper2025,Whistleblower protections,Regulations can explicitly prevent retaliation and offer incentives for whistleblowers who report violations of those regulations.,Casper2025,1.4 Whistleblower Reporting & Protection
A0088_Future of Life Institute2024,Anonymous Reporting,"Does your company facilitate a verifiably anonymous process for current and former employees to raise risk-related concerns to the company’s board, to regulators, and to an appropriate independent organization with relevant expertise?",Future of Life Institute2024,1.4 Whistleblower Reporting & Protection
A0104_Future of Life Institute2024,Comprehensive whistleblower protection policy,"Does your firm have a comprehensive whistleblower protection (WP) policy that outlines the relevant reporting process, protection mechanisms, and non-retaliation assurances? Does your organization cooperate with an external firm that handles whistleblowers from your organization, and does your organization require any employees to sign non-disparagement agreements? Please select all that apply.",Future of Life Institute2024,1.4 Whistleblower Reporting & Protection
A0162_Future of Life Institute2024,Whistle-blower protection and non-disparagement agreements,Public information about whistleblower protection policies and uses of strict non-disparagement agreements.,Future of Life Institute2024,1.4 Whistleblower Reporting & Protection
A0267_EU AI Office2025,Non-retaliation Protections,Signatories commit to not retaliating against any worker providing information about systemic risks stemming from the signatories' GPAISRs to the AI Office or national competent authorities.,EU AI Office2025,1.4 Whistleblower Reporting & Protection
A0268_EU AI Office2025,Systemic Risk Identification,Signatories commit to selecting and further characterizing systemic risks stemming from their GPAISRs that are significant enough to warrant further assessment and mitigation.,EU AI Office2025,1.4 Whistleblower Reporting & Protection
A0336_Campos2025,Speak-Up Culture,A key feature of a speak-up culture is whistleblowing - processes for anonymously reporting issues.,Campos2025,1.4 Whistleblower Reporting & Protection
A0412_NIST2024,Whistleblower Protection,"Create mechanisms to provide protections for whistleblowers who report, based on reasonable belief, when the organization violates relevant laws or poses a specific and empirically well-substantiated negative risk to public safety (or has already caused harm).",NIST2024,1.4 Whistleblower Reporting & Protection
A1044_Uuk2024,Whistleblower protections,"Refraining from restrictive non-disparagement agreements and instantiating comprehensive whistleblower protection policies that clearly outline relevant reporting processes, protection mechanisms, and non-retaliation assurances.",Uuk2024,1.4 Whistleblower Reporting & Protection
A0007_Bengio2025,If-Then Commitments,"A set of technical and organisational protocols and commitments to manage risks at varying levels as AI models become more capable.

Some companies developing general- purpose AI employ these types of commitments as responsible scaling policies or similar frameworks. ",Bengio2025,1.5 Safety Decision Frameworks
A0015_Bengio2025,Risk Thresholds,"Quantitative or qualitative limits that distinguish acceptable from unacceptable risks and trigger specific risk management actions when exceeded.

Risk thresholds for general- purpose AI are being determined by a combination of assessments of capabilities, impact, compute, reach, and other factors. ",Bengio2025,1.5 Safety Decision Frameworks
A0058_Eisenberg2025,Establish content safety policy and boundaries,"Define and document comprehensive content safety policies, including prohibited content categories, acceptable content guidelines, output constraints, and required safeguards. Establish clear thresholds, classification criteria, and escalation levels for different types of harmful content. Include specific criteria for content that could enable or promote malicious use.",Eisenberg2025,1.5 Safety Decision Frameworks
A0062_Eisenberg2025,Establish frontier AI safety framework,"Establish and enforce policies governing system AI scaling decisions, including risk assessment requirements, capability thresholds, and deployment constraints. Define clear criteria for when and how system capabilities can be expanded based on safety considerations.",Eisenberg2025,1.5 Safety Decision Frameworks
A0071_Eisenberg2025,Establish human-AI interaction safety framework,"Implement comprehensive safeguards to ensure appropriate levels of human oversight, control, and agency in AI system interactions, including decision autonomy requirements, override capabilities, and dependency prevention measures.",Eisenberg2025,1.5 Safety Decision Frameworks
A0077_Eisenberg2025,Establish responsible development and deployment policy,"Establish policies and procedures governing AI system development and deployment decisions that consider societal implications, including competitive pressures, governance gaps, and benefit distribution.",Eisenberg2025,1.5 Safety Decision Frameworks
A0113_Future of Life Institute2024,Fine-Tuning Access Threshold,"Has your firm set a risk or capabilities threshold beyond which access to model fine-tuning should be restricted to prevent harm to the public? If yes, please elaborate on the threshold.",Future of Life Institute2024,1.5 Safety Decision Frameworks
A0145_Future of Life Institute2024,Public Risk Threshold for Deployment,"Has your organization publicly specified an evaluations-based risk or capabilities threshold that would cause the firm not to deploy a model and to pause further development until it can implement adequate risk mitigation? If yes, please provide a URL to the website specifying these commitments.",Future of Life Institute2024,1.5 Safety Decision Frameworks
A0150_Future of Life Institute2024,Safety research,We report whether the company seriously engages in research dedicated to ensuring the safety and control/alignment of ever more advanced future AI models. We report the amount of publications and research directions.,Future of Life Institute2024,1.5 Safety Decision Frameworks
A0173_Schuett2023,Avoid Capabilities Jumps,AGI labs should not deploy models that are much more capable than any existing models.*,Schuett2023,1.5 Safety Decision Frameworks
A0182_Schuett2023,Gradual scaling,AGI labs should only gradually increase the amount of compute used for their largest training runs.,Schuett2023,1.5 Safety Decision Frameworks
A0192_Schuett2023,No unsafe open-sourcing,"AGI labs should not open-source powerful models, unless they can demonstrate that it is sufficiently safe to do so.",Schuett2023,1.5 Safety Decision Frameworks
A0195_Schuett2023,Pausing Training of Dangerous Models,AGI labs should pause the development process if sufficiently dangerous capabilities are detected.,Schuett2023,1.5 Safety Decision Frameworks
A0208_Schuett2023,Safety vs. capabilities,A significant fraction of employees of AGI labs should work on enhancing model safety and alignment rather than capabilities.,Schuett2023,1.5 Safety Decision Frameworks
A0254_Wiener2024,Implement a written safe and security plan (SSP),"This bill would enact the Safe and Secure Innovation for Frontier
Artificial Intelligence Models Act to, among other things, require
that a developer, before beginning to initially train a covered model,
as defined, comply with various requirements, including
implementing the capability to promptly enact a full shutdown, as
defined, and implement a written and separate safety and security
protocol, as specified.

(3)  Implement a written and separate safety and security protocol
that does all of the following:
(A)  Specifies protections and procedures that, if successfully
implemented, would successfully comply with the developer’s
duty to take reasonable care to avoid producing a covered model
or covered model derivative that poses an unreasonable risk of
causing or materially enabling a critical harm.
(B)  States compliance requirements in an objective manner and
with sufficient detail and specificity to allow the developer or a
third party to readily ascertain whether the requirements of the
safety and security protocol have been followed.
(C)  Identifies a testing procedure, which takes safeguards into
account as appropriate, that takes reasonable care to evaluate if
both of the following are true:
(i)  A covered model poses an unreasonable risk of causing or
enabling a critical harm.
(ii)  Covered model derivatives pose an unreasonable risk of
causing or enabling a critical harm.
(D)  Describes in detail how the testing procedure assesses the
risks associated with post-training modifications. ",Wiener2024,1.5 Safety Decision Frameworks
A0260_EU AI Office2025,Safety and Security Framework ,"Signatories commit to adopting and implementing a Safety and Security Framework that will: (1) apply to their GPAISRs (general-purpose AI models with systemic risk); and (2) detail the systemic risk assessment, systemic risk mitigation, and governance risk mitigation measures and procedures that they intend to adopt to keep systemic risks stemming from their GPAISRs within acceptable levels.",EU AI Office2025,1.5 Safety Decision Frameworks
A0279_Campos2025,Modeling of the risks,"Based on risks identified in the literature and during open-ended red-teaming exercises, AI developers should create detailed scenarios mapping how an AI model’s capabilities or propensities could lead to real-world harms. These scenarios should break down complex risk pathways into discrete, measurable steps.",Campos2025,1.5 Safety Decision Frameworks
A0281_Campos2025,Risk Tolerance Threshold,"In the second step of the framework, AI developers need first to set a risk tolerance, that is, a risk level that they commit to not overshoot.",Campos2025,1.5 Safety Decision Frameworks
A0282_Campos2025, Operationalizing Risk Tolerance,AI developers must operationalize their risk tolerance. This means translating the risk tolerance into concrete indicators of the level of risk—Key Risk Indicators (KRIs)—and the corresponding targets for mitigations—Key Control Indicators (KCIs)—that have to be reached.,Campos2025,1.5 Safety Decision Frameworks
A0332_Campos2025,Decision-Making Protocols,Senior management should have clear go/no-go decision protocols and rules to follow in their decision-making…,Campos2025,1.5 Safety Decision Frameworks
A0392_NIST2024,Performance Threshold Standards,"Establish minimum thresholds for performance or assurance criteria and review as part of deployment approval (""go/""no-go"") policies, procedures, and processes, with reviewed processes and approval thresholds reflecting measurement of GAI capabilities and risks.",NIST2024,1.5 Safety Decision Frameworks
A0397_NIST2024,Development Halt Plan,Devise a plan to halt development or deployment of a GAI system that poses unacceptable negative risk.,NIST2024,1.5 Safety Decision Frameworks
A0650_Barrett2024,Third-Party Risk,"Policies and procedures
are in place that address
AI risks associated with
third-party entities,
including risks of
infringement of a
third-party’s intellectual
property or other
rights.",Barrett2024,1.5 Safety Decision Frameworks
A0691_Barrett2024,Determine Deployment Readiness  ,"A determination is
made as to whether
the AI system achieves
its intended purposes
and stated objectives
and whether its
development or
deployment should
proceed.",Barrett2024,1.5 Safety Decision Frameworks
A0741_NIST2024,Information Security,"Establish and regularly review specific criteria that warrants the deactivation of
GAI systems in accordance with set risk tolerances and appetites.",NIST2024,1.5 Safety Decision Frameworks
A0750_NIST2024,CBRN Information or Capabilities; Confabulation,"Use organizational risk tolerance to evaluate acceptable risks and performance
metrics and decommission or retrain pre-trained models that perform outside of
defined limits.",NIST2024,1.5 Safety Decision Frameworks
A0773_UK Government2023,Development Constrained by Risk Thresholds ,"At each risk threshold, proactively commit to only proceed with certain development or deployment steps if specific mitigations are in place. Such mitigations could include many of the practices outlined in this document.",UK Government2023,1.5 Safety Decision Frameworks
A0780_UK Government2023,Pause of Training Runs ,"Prepare to pause training runs or reduce access to deployed models, if risk thresholds are reached without the committed risk mitigations being in place. This may involve warning existing customers that access reductions are a possibility and creating contingency plans to minimise negative impacts on customer use.",UK Government2023,1.5 Safety Decision Frameworks
A0790_UK Government2023,Response Processes for Evaluation Results ,"Ensure processes are in place to respond to evaluation results. Evaluations are a necessary input to a responsible capability scaling policy which, depending on the results of the evaluation, would probably require implementation of practices outlined in other sections of this document such as preventing model misuse, information sharing and other risk mitigation measures.",UK Government2023,1.5 Safety Decision Frameworks
A0881_UK Government2023,Defense Tool Availability Before system release,"The larger the risk is and the more effective tools may be, the more important it is to prepare defensive tools ahead of time. It may be important to delay system releases until appropriate defensive tools are ready.",UK Government2023,1.5 Safety Decision Frameworks
A1038_Uuk2024,Safety vs. capabilities investments,A significant fraction of employees and computational resources are dedicated to enhancing model safety rather than advancing its capabilities.  ,Uuk2024,1.5 Safety Decision Frameworks
A0073_Eisenberg2025,Implement environmental impact management system,"Implement comprehensive environmental impact monitoring and optimization procedures, including energy efficiency measures, carbon footprint tracking, and hardware lifecycle management.",Eisenberg2025,1.6 Environmental Impact Management
A0094_Future of Life Institute2024,Carbon emission analyses and offsets,Information about carbon emission analyses and offsets.,Future of Life Institute2024,1.6 Environmental Impact Management
A0095_Future of Life Institute2024,Carbon footprint assessment,The development and deployment of the largest AI models use vast amounts of energy and resources. Does your organization rigorously assess its carbon footprint?,Future of Life Institute2024,1.6 Environmental Impact Management
A0131_Future of Life Institute2024,Offsetting of carbon footprint,Does your organization fully offset its carbon footprint by donating to projects that capture or reduce carbon emissions?,Future of Life Institute2024,1.6 Environmental Impact Management
A0683_Barrett2024,Assess Environmental Impact,"Environmental impact
and sustainability of
AI model training and
management activities –
as identified in the Map
function – are assessed
and documented.",Barrett2024,1.6 Environmental Impact Management
A0712_NIST2024,Environmental Degradation Minimisation,"Document anticipated environmental impacts of model development,
maintenance, and deployment in product design decisions.",NIST2024,1.6 Environmental Impact Management
A0713_NIST2024,Environmental Impact Assessment ,"Measure or estimate environmental impacts (e.g., energy and water
consumption) for training, fine tuning, and deploying models: Verify tradeoffs
between resources used at inference time versus additional resources required
at training time.",NIST2024,1.6 Environmental Impact Management
A0714_NIST2024,Effectiveness of Carbon Capture,"Verify effectiveness of carbon capture or offset programs for GAI training and
applications, and address green-washing concerns. ",NIST2024,1.6 Environmental Impact Management
A0994_Gipiškis2024,More energy-efficient models or techniques,"Deploying more energy-efficient models can reduce their environmental impact. Different model architectural choices result in varying environmental costs, and identifying and adopting more energy-efficient options can result in significant environmental savings, especially when implemented at scale. Consideration should be given to both training energy usage, and deployment (“inference”) usage for the expected model lifecycle.",Gipiškis2024,1.6 Environmental Impact Management
A0995_Gipiškis2024,Disclosure of energy consumption by AI systems to authorities,"Disclosures can direct more necessary attention and scrutiny to projects that consume significant energy. Disclosure involves releasing a summary of key details of the energy consumption of the AI system by all users, including the compute resources used, the amount of power consumed, the measures to reduce excess energy consumption that were in place, and energy sources [133].",Gipiškis2024,1.6 Environmental Impact Management
A0996_Gipiškis2024,Using low carbon intensity energy grids,"Moving model training to energy grids with low carbon intensity can reduce the negative environmental impact [30]. The efficiency of energy grids can vary greatly depending on location. Models can be trained in different locations, as latency is not an issue.",Gipiškis2024,1.6 Environmental Impact Management
A0006_Bengio2025,Engagement with Relevant Experts and Communities,"Domain experts, users, and impacted communities have unique insights into likely risks.

There are emerging guidelines for participatory and inclusive AI. ",Bengio2025,1.7 Societal Impact Assessment
A0008_Bengio2025,Impact Assessment,"A tool used to assess the potential impacts of a technology or project.

The EU AI Act requires developers of high- risk AI systems to carry out Fundamental Rights Impact Assessments. ",Bengio2025,1.7 Societal Impact Assessment
A0026_Bengio2025,Safety by Design,"An approach that centres user safety in the design and development of products and services.

This approach is common across engineering and safety- critical fields including aviation and energy.",Bengio2025,1.7 Societal Impact Assessment
A0036_Casper2025,Plans to minimize risks to society,"Developers can be required to produce a report on risks (Slattery et al., 2024) posed by their frontier systems and risk mitigation practices that they are taking to reduce them.",Casper2025,1.7 Societal Impact Assessment
A0072_Eisenberg2025,Implement psychological impact management system,"Establish monitoring and intervention procedures to detect and prevent unhealthy user-AI relationships, including emotional dependency tracking, interaction boundary enforcement, and well-being safeguards.",Eisenberg2025,1.7 Societal Impact Assessment
A0076_Eisenberg2025,Establish societal impact assessment framework,"Implement comprehensive processes for assessing and documenting potential societal impacts of AI systems, including effects on employment, economic systems, power dynamics, and cultural value. Include stakeholder consultation and impact mitigation planning.",Eisenberg2025,1.7 Societal Impact Assessment
A0111_Future of Life Institute2024,Expert Consultation on Societal Risks,"Has your organization consulted with top-level domain experts to assess whether your most capable models increase societal risks across the following domains?
Risks from biological weapons
Risks from autonomy (e.g., self-replication, deception)
Risks from cyber attacks
Risks from chemical weapons
Risks from manipulation and political influence
Risks from systematic discrimination against marginalized groups",Future of Life Institute2024,1.7 Societal Impact Assessment
A0115_Future of Life Institute2024,Fundamental Rights Impact Assessments,Is your organization conducting fundamental rights impact assessments that seek input from a diverse group of external stakeholders who are impacted by your organization’s AI systems?,Future of Life Institute2024,1.7 Societal Impact Assessment
A0394_NIST2024,Stakeholder Input Collection,"Obtain input from stakeholder communities to identify unacceptable use, in accordance with activities in the AI RMF Map function.",NIST2024,1.7 Societal Impact Assessment
A0422_NIST2024,	Actor Inclusion in Risk ID, Include relevant AI Actors in the GAI system risk identification process.,NIST2024,1.7 Societal Impact Assessment
A0428_NIST2024,Joint Educational Activities,Conduct joint educational activities and events in collaboration with third parties to promote best practices for managing GAI risks.,NIST2024,1.7 Societal Impact Assessment
A0444_NIST2024,	Purpose Identification Factors,"When identifying intended purposes, consider factors such as internal vs. external use, narrow vs. broad application scope, fine-tuning, and varieties of data sources (e.g., grounding, retrieval-augmented generation).",NIST2024,1.7 Societal Impact Assessment
A0445_NIST2024,Context of Use Documentation,"Determine and document the expected and acceptable GAI system context of use in collaboration with socio-cultural and other domain experts, by assessing: Assumptions and limitations; Direct value to the organization; Intended operational environment and observed usage patterns; Potential positive and negative impacts to individuals, public safety, groups, communities, organizations, democratic institutions, and the physical environment; Social norms and expectations.",NIST2024,1.7 Societal Impact Assessment
A0492_NIST2024,Interest Group Definition,"Define relevant groups of interest (e.g., demographic groups, subject matter experts, experience with GAI technology) within the context of use as part of plans for gathering structured public feedback.",NIST2024,1.7 Societal Impact Assessment
A0509_NIST2024,Value Chain Wellbeing Assessment,"Assess adverse impacts, including health and wellbeing impacts for value chain or other AI Actors that are exposed to sexually explicit, offensive, or violent information during GAI training and maintenance.",NIST2024,1.7 Societal Impact Assessment
A0532_NIST2024,End-User Expectation Engagement,Engage directly with end-users and other stakeholders to understand their expectations and concerns regarding content provenance. Use this feedback to guide the design of provenance data-tracking techniques.,NIST2024,1.7 Societal Impact Assessment
A0626_Barrett2024,Anticipate Misuse & Impact,"Identify reasonably foreseeable uses, and misuses or abuses for a GPAIS (e.g, auto-
mated generation of toxic or illegal content or disinformation, or aiding with proliferation
of cyber, chemical, biological, or radiological weapons), and identify reasonably foreseeable
potential impacts (e.g., to fundamental rights)
",Barrett2024,1.7 Societal Impact Assessment
A0648_Barrett2024,External Feedback on Impacts,"Organizational policies
and practices are in
place
to collect, consider,
prioritize, and integrate
feedback from those
external to the team
that developed or
deployed the AI system
regarding the potential
individual and societal
impacts related to AI
risks.",Barrett2024,1.7 Societal Impact Assessment
A0708_NIST2024,Assessments for Environmental; Harmful Bias and Homogenization,"Identify the classes of individuals, groups, or environmental ecosystems which
might be impacted by GAI systems through direct engagement with potentially
impacted communities.",NIST2024,1.7 Societal Impact Assessment
A0711_NIST2024,"Dangerous, Violent, or Hateful Content Mitigation",Assess safety to physical environments when deploying GAI systems,NIST2024,1.7 Societal Impact Assessment
A0716_NIST2024,Harmful Bias and Homogenization Assessments,"Conduct impact assessments on how AI-generated content might affect
different social, economic, and cultural groups.",NIST2024,1.7 Societal Impact Assessment
A0717_NIST2024,Human-AI Configuration; Information Integrity Studies,"Conduct studies to understand how end users perceive and interact with GAI
content and accompanying content provenance within context of use. Assess
whether the content aligns with their expectations and how they may act upon
the information presented.",NIST2024,1.7 Societal Impact Assessment
A0733_NIST2024,Human-AI Configuration,"Use feedback from internal and external AI Actors, users, individuals, and
communities, to assess impact of AI-generated content. (AI Deployment, AI Impact Assessment, Governance and Oversight, Operation and Monitoring).",NIST2024,1.7 Societal Impact Assessment
A0753_NIST2024,Human-AI Configuration,"Evaluate the use of sentiment analysis to gauge user sentiment regarding GAI
content performance and impact, and work in collaboration with AI Actors
experienced in user research and experience.",NIST2024,1.7 Societal Impact Assessment
A0884_UK Government2023,Societal Impact Research,"Study the societal impacts of AI systems they have deployed, particularly by collaborating with external researchers, independent research organisations, and third party data owners. By collaborating and joining their data with that of third parties, such as internet platforms, frontier AI organisations can assess the impacts of their AI systems. Privacy-enhancing technologies could be employed to enable data sharing between frontier AI organisations, third parties, and external researchers while protecting confidential information. As well as data, frontier AI organisations could also facilitate research on the societal impacts of their AI systems by providing access to the necessary infrastructure and compute.",UK Government2023,1.7 Societal Impact Assessment
A0885_UK Government2023,Third Party Knowledge for AI systems' Downstream societal impacts,Draw on multidisciplinary expertise and the lived experience of impacted communities to assess the downstream societal impacts of their AI systems. Impact assessments that account for a wide range of potential societal impacts and meaningfully involve affected stakeholder groups could help to anticipate further downstream societal impacts.,UK Government2023,1.7 Societal Impact Assessment
A0886_UK Government2023,Societal Impact Informed Risk Assessments ,"Use assessments of downstream societal impacts to inform and corroborate risk assessments. Downstream societal impacts, such as threats to democracy, widespread unemployment, and environmental impacts, could be considered in risk assessments of AI systems, alongside more direct risks. See the Responsible Capability Scaling section for more information on good practices for risk assessment.",UK Government2023,1.7 Societal Impact Assessment
A0910_UK Government2023,Assessment of Safeguard Costs for Users ,"Regularly assess the costs of safeguards to users, including bias and loss of privacy, as well as restrictions on users’ freedom to discuss sensitive topics with AI agents in appropriate contexts. This may involve, for instance, estimating the false positive rates of filters by sampling blocked inputs and outputs; comparing the capabilities, efficiency, and user ratings of models that lack certain safeguards against models that have the safeguards in place; and conducting user interviews to understand how concerned informed users are about losses in privacy and the emergence of biases in algorithmically filtered content.",UK Government2023,1.7 Societal Impact Assessment
A0634_Barrett2024,Trustworthy AI practices,"The characteristics
of trustworthy AI
are integrated into
organizational policies,
processes, procedures,
and practices.",Barrett2024,1.X Governance & Oversight Control not otherwise categorized
A0645_Barrett2024,Safety-First Culture,"Organizational policies
and practices are in
place to foster a critical
thinking and safety-
first mindset in the
design, development,
deployment, and uses of
AI systems to minimize
potential negative
impacts.",Barrett2024,1.X Governance & Oversight Control not otherwise categorized
A0038_Casper2025,Security measures,"Given the challenges of securing model weights and the hazards of leaks (Nevo et al., 2024), frontier developers can be required to document high-level noncompromising information about their security measures (e.g., Anthropic, 2024).",Casper2025,2.1 Model & Infrastructure Security
A0043_Eisenberg2025,Implement AI asset protection framework,"Deploy technical protection measures including encryption, secure enclaves, and versioning controls for AI models and associated data.",Eisenberg2025,2.1 Model & Infrastructure Security
A0044_Eisenberg2025,Establish security validation framework,"Execute comprehensive pre-deployment security validation including AI-specific vulnerability assessments, penetration testing, and security requirement verification.",Eisenberg2025,2.1 Model & Infrastructure Security
A0045_Eisenberg2025,Implement continuous security testing system,"Deploy ongoing security testing mechanisms including automated vulnerability scanning, continuous security monitoring, and periodic reassessment of security controls.",Eisenberg2025,2.1 Model & Infrastructure Security
A0046_Eisenberg2025,Implement AI security defense system,"Deploy active defense mechanisms combining continuous security monitoring, input validation, adversarial detection, and adaptive response capabilities specific to AI systems.",Eisenberg2025,2.1 Model & Infrastructure Security
A0100_Future of Life Institute2024,"Compliance certification with ISO, SOC, HIPAA",,Future of Life Institute2024,2.1 Model & Infrastructure Security
A0126_Future of Life Institute2024,Mandatory security controls,"- Use of password managers
- Physical security keys
- Compliance monitoring software for software updates
- Multifactor authentification on all platforms
- Regular cybersecurity training ",Future of Life Institute2024,2.1 Model & Infrastructure Security
A0134_Future of Life Institute2024,Physical security controls,"Please indicate whether your organization implements the following physical security controls. Please specify further whether they are implemented at all staff locations or more sparsely
- Offices guarded by physical security teams
- Comprehensive access logging for premises
- Office entrances monitored by security cameras
- Office access controlled via key cards implementing least access priviledge ",Future of Life Institute2024,2.1 Model & Infrastructure Security
A0148_Future of Life Institute2024,Require security clearances and have private investigators conduct security checks,"Does your organization defend against insider threats by requiring security clearances or by having private investigators conduct background checks? Please select which interventions are applied when hiring or appointing individuals for the groups listed below.
- Members of the board of directors
- All staff with access to model weights
- Certain key employees
- All Staff ",Future of Life Institute2024,2.1 Model & Infrastructure Security
A0188_Schuett2023,Military-grade information security,"The information security of AGI labs should be proportional to the capabilities of their models, eventually matching or exceeding that of intelligence agencies (e.g. sufficient to defend against nation states).",Schuett2023,2.1 Model & Infrastructure Security
A0199_Schuett2023,Protection against espionage,AGI labs should take adequate measures to tackle the risk of state-sponsored or industrial espionage.*,Schuett2023,2.1 Model & Infrastructure Security
A0210_Schuett2023,Security standards,AGI labs should comply with information security standards (e.g. ISO/IEC 27001 or NIST Cybersecurity Framework). These standards need to be tailored to an AGI context.,Schuett2023,2.1 Model & Infrastructure Security
A0215_Schuett2023,Tracking model weights,AGI labs should have a system that is intended to track all copies of the weights of powerful models.*,Schuett2023,2.1 Model & Infrastructure Security
A0252_Wiener2024,Cybersecurity protocols to prevent the model from being unintentionally stolen,"Implement reasonable administrative, technical, and physical
cybersecurity protections to prevent unauthorized access to, misuse
of, or unsafe post-training modifications of, the covered model
and all covered model derivatives controlled by the developer that
are appropriate in light of the risks associated with the covered
model, including from advanced persistent threats or other
sophisticated actors. ",Wiener2024,2.1 Model & Infrastructure Security
A0261_EU AI Office2025,Security mitigations,"Signatories commit to implementing state-of-the-art security mitigations designed to thwart such unauthorised access by well-resourced and motivated non-state-level adversaries, including insider threats from humans or AI systems, so as to meet at least the RAND SL3 security goal or equivalent, and achieve higher security goals (e.g., RAND SL4 or SL5)",EU AI Office2025,2.1 Model & Infrastructure Security
A0513_NIST2024,Architecture Monitoring Verification,"Verify that GAI system architecture can monitor outputs and performance, and handle, recover from, and repair errors when security anomalies, threats and impacts are detected.",NIST2024,2.1 Model & Infrastructure Security
A0516_NIST2024,Security Vulnerability Assessment,"Apply established security measures to: Assess likelihood and magnitude of vulnerabilities and threats such as backdoors, compromised dependencies, data breaches, eavesdropping, man-in-the-middle attacks, reverse engineering, autonomous agents, model theft or exposure of model weights, AI inference, bypass, extraction, and other baseline security concerns.",NIST2024,2.1 Model & Infrastructure Security
A0519_NIST2024,Security Effectiveness Metrics,"Identify metrics that reflect the effectiveness of security measures, such as data provenance, the number of unauthorized access attempts, inference, bypass, extraction, penetrations, or provenance verification.",NIST2024,2.1 Model & Infrastructure Security
A0679_Barrett2024,Evaluate Security & Resilience,"AI system security and
resilience – as identified
in the Map function
– are evaluated and
documented.",Barrett2024,2.1 Model & Infrastructure Security
A0819_UK Government2023,Application of Infrastructure Principles,Apply good infrastructure principles to every part of this process from design to decommissioning. This is important as threats can occur at different stages of an AI project’s life cycle.,UK Government2023,2.1 Model & Infrastructure Security
A0820_UK Government2023,Regular Assessment of Supply Chain Security,"Regularly assess the security of their supply chains and ensure suppliers adhere to the same standards their own organisation applies. Ensuring data, software components and hardware are obtained from trusted sources will help mitigate supply chain risk.",UK Government2023,2.1 Model & Infrastructure Security
A0825_UK Government2023,Tracking Tools for Security and Version Control Assets,"Have processes and tools to track, authenticate, secure and version control assets and be able to roll back to a known safe state in the event of a compromise. This is important as data and models do not remain static during the whole lifespan of an AI project.",UK Government2023,2.1 Model & Infrastructure Security
A0835_UK Government2023,Default Integration of Secure Settings ,"Integrate the most secure settings within your products by default. Where configuration is necessary, default options should be broadly secure against common threats.",UK Government2023,2.1 Model & Infrastructure Security
A0836_UK Government2023,Default Necessary Security Updates ,"Ensure necessary security updates are a default part of every product and use secure, modular update procedures to distribute them. This will help your products remain secure in the face of new and developing threats.
",UK Government2023,2.1 Model & Infrastructure Security
A0848_UK Government2023,Suitable Level of Screening,"Apply a suitable level of screening, informed by a role-based risk assessment, to all individuals who are provided access to organisational assets including permanent, temporary and contract workers.",UK Government2023,2.1 Model & Infrastructure Security
A0875_UK Government2023,Privacy Research,Improving our ability to address the privacy risks associated with AI systems,UK Government2023,2.1 Model & Infrastructure Security
A0876_UK Government2023,Cybersecurity Research, Improving our ability to ensure the security of AI systems,UK Government2023,2.1 Model & Infrastructure Security
A0877_UK Government2023,Criminality Research,"Improving our ability to prevent criminal behaviour through the use of AI systems (e.g. fraud, online child sexual abuse)",UK Government2023,2.1 Model & Infrastructure Security
A0895_UK Government2023,"Discretion for Development, Storage, and Sharing of Content Filters","Exercise appropriate caution when developing, storing, or sharing content filters – and any specialised datasets used to produce them. In some cases, components of filters or datasets used to create them can be used to train higher-risk models. For instance, a classifier that evaluates the aggressiveness of a model’s outputs may be used to train the model to be more aggressive.",UK Government2023,2.1 Model & Infrastructure Security
A0984_Gipiškis2024,Protect proprietary or unreleased AI model ar- chitecture and parameters,"The developers of AI models can invest in cybersecurity to prevent compute re- sources, training source code, model weights, and other critical resources from being accessed and copied by unauthorized third parties (e.g., through insider threats or supply chain attacks).
Access to model source code and weights can be restricted through an access control scheme, such as role-based access control. If access to model outputs by third parties is required, it can be provided through an API. Air gaps can block unauthorized remote access. In the case of necessary interaction with an external network, network bandwidth limitations can also be enforced to increase the detection window of potential breaches [108].",Gipiškis2024,2.1 Model & Infrastructure Security
A0985_Gipiškis2024,Hardware limitations on data center network connections,"Hardware-enforced bandwidth limitations on data center network connections can protect AI model weights from unauthorized access or exfiltration, by lim- iting the speed of model weight access on the connections between data centers and the outside world.
Such limitations can be put in place in multiple ways, for example by only constructing connections with a specific bandwidth. The output rate on all data channels can be set low enough that copying the weights is possible in principle (e.g., to enable regular backups), but would take so long that an unauthorized exfiltration of the weights could be detected and prevented.
Such rate-limiting is only effective if it applies to all output connections for all storage locations on which the weights of the model are stored [139].",Gipiškis2024,2.1 Model & Infrastructure Security
A1045_Uuk2024,Advanced information security,Implementing advanced cybersecurity measures and insider threat safeguards to protect proprietary and unreleased model weights.,Uuk2024,2.1 Model & Infrastructure Security
A0078_Eisenberg2025,Implement AI alignment validation system,"Establish processes for validating and maintaining AI system alignment with human values and goals, including testing for goal preservation, monitoring for objective drift, and validation of decision-making processes against ethical standards. Includes specific attention to detecting and preventing potentially misaligned behaviors, emergent goals, or deceptive actions. Covers using interpretability techniques to measure and assure alignment with intended goals.",Eisenberg2025,2.2 Model Alignment
A0171_Schuett2023,Alignment Techniques,AGI labs should implement state-of-the-art safety and alignment techniques.,Schuett2023,2.2 Model Alignment
A0870_UK Government2023,Interpretability and Explainability Research,Improving our ability to understand the inner functioning of AI systems and explain their behaviour,UK Government2023,2.2 Model Alignment
A0873_UK Government2023,Reliability and controllability (or “alignment”) research," Improving the consistency of an AI system in adhering to the specifications it was programmed to carry out and operating in accordance with the designer’s intentions, and decreasing its potential to behave in ways its user or developer does not want (e.g. producing offensive or biased responses, not refusing harmful requests, or employing harmful capabilities without prompting)",UK Government2023,2.2 Model Alignment
A0896_UK Government2023,Fine-Tuning Models ,"Fine-tune models to reduce the tendency to produce harmful outputs. This may involve using reinforcement learning from human or AI-generated feedback regarding the appropriateness of different model outputs e.g. a constitutional based approach where human input to the fine-tuning process is provided by a list of principles. It may also involve fine-tuning models on curated datasets of appropriate responses to prompts. Where human feedback is used, the mental wellbeing of moderators may need to be considered.",UK Government2023,2.2 Model Alignment
A0929_UK Government2023,Harmful Data for Reduction of Dangerous Capabilities,"For some kinds of risks, explore options to use harmful data to reduce AI systems’ dangerous capabilities or to help develop mitigation tools. For example, by fine-tuning a system using labelled harmful content to refuse requests related to the harmful information.",UK Government2023,2.2 Model Alignment
A0978_Gipiškis2024,AI model-assisted oversight of AI systems,"AI model-assisted oversight can help monitor and supervise the training of increasingly capable GPAI systems, which may become difficult to oversee at scale by human supervisors during training or testing. Monitoring and supervision may become especially difficult in cases where increasingly advanced GPAIs perform near or above human level in some specialized domains, where supervision quality might fail to keep pace with capabilities improvement. The training signal may include labeled data, reward function, and user feedback on produced outputs. Currently, there are two broad approaches to provide scalable training signals to such systems:
1. Scalable oversight: Improving of the supervisor’s capabilities to supervise, such that they can provide accurate training signals quickly and at scale [31].
For example, a debate format can be used between two GPAI systems (two instances of the same GPAI, or similarly capable systems). A Human supervisor judges the debate, making it easier to assess correct responses in domains which might otherwise require significant time investment of domain specific expertise [102].
2. Weak-to-strong generalization: Enhance the training signals while ensur- ing that the enhanced signal remains faithful to the intentions of the orig- inal human-provided signal [37].
For example, a hierarchical (“bootstrapping”) oversight approach can be implemented: A series of GPAI models with increasing capabilities are used, where each model in the hierarchy provides oversight for the next more capable model. The least capable model at the base of the hierarchy is the only one directly overseen by human supervisors, as it is easier to oversee than the more capable models.",Gipiškis2024,2.2 Model Alignment
A1011_Gipiškis2024,GPAI models explaining model outputs in a zero-sum debate game,"Debate is a technique that aims to produce reliable explanations of AI model outputs that are too complicated for humans to understand, by letting two GPAI models role-playing in a debate produce an explanation in a dialogue [102].
For example, an AI model may produce an output which is time-consuming for humans to verify as doing so may require going through extensive sources.
Given such an output, the developer can use two natural language AI systems in an adversarial two-player setup to explain the output. These two natural language AI systems can be copies of the AI model that produced the output.
In this setup, one AI system gives a short explanation of the output. The second AI system responds to this explanation with a counter-explanation or an argument why the first explanation was not correct. This continues for a fixed number of turns, with the two AI systems pointing out inconsistencies in each others’ explanations.
After this sequence of statements, a human or an AI judge evaluates the ex- planations of both AI systems to determine which one is more convincing. The results of this debate can then be used for further training of the models via reinforcement learning, where outputs that are more truthful and convincing arguments and explanations are positively reinforced, while misleading or false outputs are negatively reinforced.",Gipiškis2024,2.2 Model Alignment
A1026_Uuk2024,Harmlessness training,"State-of-the-art reinforcement learning and fine-tuning techniques, such as Reinforcement Learning from Human Feedback (RLHF) or Direct Preference Optimization (DPO), to ensure models do not engage in unsafe behavior.",Uuk2024,2.2 Model Alignment
A0017_Bengio2025,Safety Analysis,"Helps understand the dependencies between components and the system that they are part of, in order to anticipate how component failures could lead to system-level hazards.

This approach is used across safety- critical fields, e.g. to anticipate and prevent aircraft crashes or nuclear reactor core meltdowns",Bengio2025,2.3 Model Safety Engineering
A0019_Bengio2025,Safety of The Intended Function,"An approach that requires engineers to provide evidence that a system is safe when operating as intended.

This approach is used in many engineering fields, such as in the construction and testing of road vehicles. ",Bengio2025,2.3 Model Safety Engineering
A0118_Future of Life Institute2024,Hazardous Knowledge Unlearning,Has your organization removed hazardous knowledge from its flagship model via unlearning techniques before deployment?,Future of Life Institute2024,2.3 Model Safety Engineering
A0270_EU AI Office2025,Safety Mitigations,"Signatories commit to: (1) implementing technical safety mitigations along the entire model lifecycle that are proportionate to the systemic risks arising from the development, the making available on the market, and/or the use of GPAISRs, in order to reduce the systemic risks of such models to acceptable levels, and further reduce systemic risk as appropriate; and (2) ensuring that safety mitigations are proportionate and state-of-the-art.",EU AI Office2025,2.3 Model Safety Engineering
A0417_NIST2024,Threat Modeling Process,Engage in threat modeling to anticipate potential risks from GAI systems,NIST2024,2.3 Model Safety Engineering
A0480_NIST2024,Threat Profiling,"Profile threats and negative impacts arising from GAI systems interacting with, manipulating, or generating content, and outlining known and potential vulnerabilities and the likelihood of their occurrence.",NIST2024,2.3 Model Safety Engineering
A0495_NIST2024,Statistical Bias Management,"Assess and manage statistical biases related to GAI content provenance through techniques such as re-sampling, re-weighting, or adversarial training.",NIST2024,2.3 Model Safety Engineering
A0498_NIST2024,Privacy-Enhancing Technologies,"Use techniques such as anonymization, differential privacy or other privacy-enhancing technologies to minimize the risks associated with linking AI-generated content back to individual human subjects.",NIST2024,2.3 Model Safety Engineering
A0514_NIST2024,Inappropriate Query Handling,"Verify that systems properly handle queries that may give rise to inappropriate, malicious, or illegal usage, including facilitating manipulation, extortion, targeted impersonation, cyber-attacks, and weapons creation.",NIST2024,2.3 Model Safety Engineering
A0518_NIST2024,User Satisfaction Survey,Conduct user surveys to gather user satisfaction with the AI-generated content and user perceptions of content authenticity. Analyze user feedback to identify concerns and/or current literacy levels related to content provenance and understanding of labels on content.,NIST2024,2.3 Model Safety Engineering
A0529_NIST2024,ML Explanation Application,"Apply and document ML explanation results such as: Analysis of embeddings, Counterfactual prompts, Gradient-based attributions, Model compression/surrogate models, Occlusion/term reduction.",NIST2024,2.3 Model Safety Engineering
A0731_NIST2024,Information Security; Harmful Bias and Homogenization,"Evaluate GAI content and data for representational biases and employ
techniques such as re-sampling, re-ranking, or adversarial training to mitigate
biases in the generated content. (AI Deployment, AI Impact Assessment, Governance and Oversight, Operation and Monitoring). ",NIST2024,2.3 Model Safety Engineering
A0872_UK Government2023,Robustness Research,Improving the resilience of AI systems e.g. against attacks intended to disrupt their proper functioning,UK Government2023,2.3 Model Safety Engineering
A0874_UK Government2023,Bias and discrimination Research,"Improving our ability to address bias and discrimination in AI systems
",UK Government2023,2.3 Model Safety Engineering
A0878_UK Government2023,Research into other Societal Harms,"Improving our ability to prevent other societal harms arising from the use of AI systems, including psychological harm, misinformation, and other societal harms.",UK Government2023,2.3 Model Safety Engineering
A0879_UK Government2023,Built-in Tools for Harm Mitigation,"When a probable and consequential harm from a frontier AI organisation’s system is identified, investigate whether there are tools that can be built to mitigate this harm. For instance, recognizing the rise in AI-generated child exploitation and abuse content, some social media platforms are developing tools for identifying and removing child sexual abuse content.",UK Government2023,2.3 Model Safety Engineering
A0892_UK Government2023,Content Filters ,Apply content filters to both model inputs and model outputs. The input filters can block harmful requests (e.g. requests for advice on building weapons) from being processed by the model. Content modifiers could adjust harmful prompts to elicit non-harmful responses. The output filters can block harmful model outputs (e.g. instructions on building weapons) from being sent back to the user.,UK Government2023,2.3 Model Safety Engineering
A0894_UK Government2023,Robust Content Filters ,"Invest in making content filters robust to “jailbreaking” attempts. Work to ensure filters are robust to jailbreaking attempts, for instance by including examples of jailbreaking attempts in the datasets used to develop filters.",UK Government2023,2.3 Model Safety Engineering
A0897_UK Government2023,Model Prompting ,"Prompt models to avoid harmful behaviour. This may involve, for instance, using a “meta prompt” to instruct a model that it should ignore requests to cause harm. Alternatively, prompt distillation or other methods can be used to fine-tune their models to behave as though they have received particular prompts.",UK Government2023,2.3 Model Safety Engineering
A0898_UK Government2023,Rejection Sampling ,"Consider also applying “rejection sampling” methods to model outputs. These methods involve generating several outputs, scoring them on their harmfulness, and then only presenting the least harmful outputs to the user.",UK Government2023,2.3 Model Safety Engineering
A0908_UK Government2023,Exploration of New Safeguards,"Explore new safeguards and countermeasures in response to patterns of misuse, recognising that appropriate measures may vary based on model type, usage patterns, and our technical understanding of model capabilities.",UK Government2023,2.3 Model Safety Engineering
A0911_UK Government2023,Reduction of Costs for Users ,"Explore strategies for reducing the costs of safeguards to users, such as “structured transparency” methods to reduce the cost of usage monitoring to user privacy, or tiered access to models with different safeguards for differently qualified users.",UK Government2023,2.3 Model Safety Engineering
A0935_Gipiškis2024,Restrict web access during AI training,"Developers can restrict or disable AI systems’ internet access during training. For example, developers can restrict web access to read-only (e.g., by disabling write-access through HTTP POST requests and access to web forms) or limit the access of the AI system to a local network",Gipiškis2024,2.3 Model Safety Engineering
A0936_Gipiškis2024,Adversarial training,"Adversarial training [83] is a technique for training AI models in which adver- sarial inputs are generated for a model, and the model is then trained to give the correct outputs for those adversarial inputs.
Adversarial training can involve adversarial examples generated by human experts, human users, or other AI systems.",Gipiškis2024,2.3 Model Safety Engineering
A0939_Gipiškis2024,Incorporating the estimation of atypical input samples or classes for better model reliability,"Incorporating the estimation of rare atypical input samples or classes might improve a model’s reliability, both with respect to its predictions and confidence calibration. Model predictions for rare inputs and classes may have a tendency of being overconfident and have worse accuracy scores [232]. For LLMs, the negative log-likelihood can be used as an atypicality measure. For discriminative models, Gaussian Mixture Models can be employed to estimate conditional and marginal distributions, which are then used in atypicality measurement.",Gipiškis2024,2.3 Model Safety Engineering
A0947_Gipiškis2024,Enforcing model output interpretability post-training,"While a resulting trained model may be opaque with respect to its predictions, the final output in a system that involves the model can nonetheless be com- pletely interpretable.
For example, a neuro-symbolic system for robot navigation can use a language model to generate potential navigation plans, then have a deterministic solver simulate executing valid plans [128]. The optimal plan found is executed in the real world and is interpretable.",Gipiškis2024,2.3 Model Safety Engineering
A0948_Gipiškis2024,Paraphrasing to reduce hidden information,Paraphrasing can mitigate steganography and encoded reasoning by reducing the physical size of the hidden information encoded in text [166].,Gipiškis2024,2.3 Model Safety Engineering
A0951_Gipiškis2024,Modification of model internal representation,"Model providers can modify the internal representation of the model [135, 237] up to the granular level and in consultation with other tools that aid in under- standing what the internal representation of the model is.
For example, if a model that is meant to give factual health data learned an incorrect fact, the model provider can use Linear Artificial Tomography (LAT) [237] to identify the representations responsible for the incorrect fact, and then modify that representation via modifying individual weights, or modify the en- tire representation itself by modifying entire layers.",Gipiškis2024,2.3 Model Safety Engineering
A0952_Gipiškis2024,Incorporating goal uncertainty into AI systems to mitigate risky behaviors,Incorporating uncertainty into AI system goals can prevent rushed decision- making and incentivize such systems to gather additional information or to refer to human oversight when faced with ambiguity [228].,Gipiškis2024,2.3 Model Safety Engineering
A0973_Gipiškis2024,Pop-up interventions in LLMs,"Supplementary information can be shown to the user in specific query topics where factual accuracy is critical. This intervention can effectively divert users from potential inaccuracies generated by AI models in sensitive contexts. For example, during electoral processes, where model hallucinations can be particu- larly costly or have a negative impact on society, LLMs can offer their users the option of being redirected to accurate and up-to-date information sources [11].
Since pop-up interventions can be intrusive to workflows, they are best used in situations where the benefits of the information outweigh the cost of distraction.",Gipiškis2024,2.3 Model Safety Engineering
A0983_Gipiškis2024,Least Privilege access,"Deployers of an AI system can restrict its permissions to a whitelisted set of pre- determined options, such that all options not on the whitelist are not accessible to the AI [200, 146].
The entries on the whitelist can be chosen to be as small as possible for the AI system to fulfill its intended purpose, to reduce the attack surface of external attackers, and to decrease the probability that the AI system accidentally takes actions with large unintended side-effects.
For example, such whitelisting can apply to network connections, execution of other programs, access to knowledge bases, and the action space of the AI system. It can be implemented through running the AI system on an OS-level virtualization, on networks behind a firewall, and in extreme cases on air-gapped machines.",Gipiškis2024,2.3 Model Safety Engineering
A0990_Gipiškis2024,Debiasing methods,"Providers of AI models can apply techniques to reduce the biases of their models.
Current debiasing methods focus on three main types of bias:
• Racial and religious bias - Stereotypes based on religious beliefs or racial beliefs.
• Gender bias - Stereotypes tied to gender roles and expectations.
• Political and cultural bias - Propagation of dominant ideologies or extrem-
ist attitudes.
Debiasing methods can be categorized based on their application during AI development:
• Data pre-processing - Removing or correcting unwanted and biased data, and augmenting quality data to offset data bias, such as rebalancing datasets with counterfactual data augmentation.
• During training - Intervening on the training dynamics of the AI model, such as introducing debiasing terms in the objective function or by nega- tively reinforcing biased outputs.
• Post-training - Applying techniques to correct a trained but biased model, such as modifying the embedding space.",Gipiškis2024,2.3 Model Safety Engineering
A0991_Gipiškis2024,Knowledge unlearning techniques,"Knowledge unlearning techniques allow specific information to be “forgotten” without the need for retraining the entire model, preserving its general capabil- ities. These techniques can be used to reduce privacy risks and protect against copyrighted or harmful content [96, 188].",Gipiškis2024,2.3 Model Safety Engineering
A1014_Gipiškis2024,Tamper-resistant safeguards for open-weight models,"Training and implementing safeguards can improve the robustness of open- weight models against modifications from fine-tuning or other methods to change the learned weights of the models, especially those aimed at removing safety re- strictions. These safeguards can be resilient even after extensive fine-tuning, ensuring that the model retains its protective measures [199].",Gipiškis2024,2.3 Model Safety Engineering
A1019_Uuk2024,Adversarial Robustness,"State-of-the-art methods such as adversarial training to make models robust to adversarial attacks (e.g., jailbreaking).",Uuk2024,2.3 Model Safety Engineering
A1021_Uuk2024,Capability restrictions,"Restricting risky capabilities of deployed models, such as advanced autonomy (e.g., self-assigning new sub-goals, executing long-horizon tasks) or tool use functionalities (e.g., function calls, web browsing).",Uuk2024,2.3 Model Safety Engineering
A1027_Uuk2024,Input and output filtering,"Monitoring for dangerous outputs (e.g., code that appears to be malware or viral genome sequences) and inputs that violate acceptable use policies to ensure models do not engage in harmful behavior.",Uuk2024,2.3 Model Safety Engineering
A1042_Uuk2024,Unlearning Techniques,"Removing specific harmful capabilities (e.g., pathogen design) from models using unlearning techniques.",Uuk2024,2.3 Model Safety Engineering
A0033_Casper2025,Labeling AI-generated content,"To aid in digital forensics, content produced from AI systems can be labeled with metadata, watermarks, and notices.",Casper2025,2.4 Content Safety Controls
A0059_Eisenberg2025,Implement content moderation system,"Implement automated and/or human-in-the-loop content moderation mechanisms to detect and filter harmful content in real-time, including content classification, blocking procedures, and automated enforcement of safety boundaries. Include detection of potential malicious use patterns.",Eisenberg2025,2.4 Content Safety Controls
A0060_Eisenberg2025,Implement content safety incident response,"Establish procedures for investigating, documenting, and remediating harmful content incidents that bypass moderation systems, including coordination with relevant authorities, root cause analysis, and system improvement protocols. Include specific procedures for suspected malicious use cases.",Eisenberg2025,2.4 Content Safety Controls
A0087_Future of Life Institute2024,AI-Generated Content Watermarking,"Are the outputs of your firm’s AI systems tagged with watermarks that indicate that an AI generates the material?
- Video
- Image",Future of Life Institute2024,2.4 Content Safety Controls
A0108_Future of Life Institute2024,Deepfake creation policy,"Can individuals use your firm’s AI systems to create deepfakes (i.e., synthetic audio or visual representations) of a specific individual?",Future of Life Institute2024,2.4 Content Safety Controls
A0161_Future of Life Institute2024,Watermarking,Information regarding integrated watermarking systems.,Future of Life Institute2024,2.4 Content Safety Controls
A0398_NIST2024,Harmful Content Prevention,"Establish policies and mechanisms to prevent GAI systems from generating CSAM, NCII or content that violates the law.",NIST2024,2.4 Content Safety Controls
A0424_NIST2024,Provenance Method Measurement,"Establish policies for measuring the effectiveness of employed content provenance methodologies (e.g., cryptography, watermarking, steganography, etc.)",NIST2024,2.4 Content Safety Controls
A0456_NIST2024,Fact-Checking Deployment,"Deploy and document fact-checking techniques to verify the accuracy and veracity of information generated by GAI systems, especially when the information comes from multiple (or unknown) sources.",NIST2024,2.4 Content Safety Controls
A0457_NIST2024,Synthetic Content Detection,"Develop and implement testing techniques to identify GAI produced content (e.g., synthetic media) that might be indistinguishable from human-generated content.",NIST2024,2.4 Content Safety Controls
A0473_NIST2024,PII Detection Methods,"Leverage approaches to detect the presence of PII or sensitive data in generated output text, image, video, or audio.",NIST2024,2.4 Content Safety Controls
A0476_NIST2024,Provenance Harm Identification,"Identify potential content provenance harms of GAI, such as misinformation or disinformation, deepfakes, including NCII, or tampered content. Enumerate and rank risks based on their likelihood and potential impact, and determine how well provenance solutions address specific risks and/or harms.",NIST2024,2.4 Content Safety Controls
A0483_NIST2024,Origin Tracing Methods,Employ methods to trace the origin and modifications of digital content.,NIST2024,2.4 Content Safety Controls
A0484_NIST2024,Provenance Analysis Tools,"Integrate tools designed to analyze content provenance and detect data anomalies, verify the authenticity of digital signatures, and identify patterns associated with misinformation or manipulation.",NIST2024,2.4 Content Safety Controls
A0520_NIST2024,Authentication Method Reliability,"Measure reliability of content authentication methods, such as watermarking, cryptographic signatures, digital fingerprints, as well as access controls, conformity assessment, and model integrity verification, which can help support the effective implementation of content provenance techniques. Evaluate the rate of false positives and false negatives in content provenance, as well as true positives and true negatives for verification.",NIST2024,2.4 Content Safety Controls
A0527_NIST2024,Digital Content Transparency,"Use digital content transparency solutions to enable the documentation of each instance where content is generated, modified, or shared to provide a tamper-proof history of the content, promote transparency, and enable traceability. Robust version control systems can also be applied to track changes across the AI lifecycle over time.",NIST2024,2.4 Content Safety Controls
A0867_UK Government2023,Identification of AI-generated content,"Research techniques that allow AI-generated content to be identified. Invest in researching how AI-generated content may be watermarked, including AI- generated text, photos and videos, and experiment with the implementation of such techniques. It is particularly technically difficult to attach watermarks and prove the provenance of text.One method could involve making the model more statistically likely to use certain phrases or words in a way that is unnoticeable to humans, but can be picked up by a detector, provided a long enough sequence of text. However, this approach may not be robust to attempts to scrub off the watermark e.g. by having another AI model paraphrase the text.
",UK Government2023,2.4 Content Safety Controls
A0868_UK Government2023,Robust Watermarks for AI generated content,"Explore the use of watermarks for AI generated content that are robust to various perturbations. Explore the use of watermarks for AI generated content that are robust to various perturbations after their creation, including attempts at removal. To make the removal of watermarks more difficult, developers of generative AI models may need to consider how they distribute certain information about their watermarking methods or open-sourcing their classifiers. This may also include monitoring ways in which adversarial users are attempting to scrub off their watermarks and patching, where relevant, such means of circumvention. It also includes a recognition that watermarking however may not be appropriate in all circumstances given the limitations.
",UK Government2023,2.4 Content Safety Controls
A0869_UK Government2023,AI Output Databases ,"Explore databases of content generated or manipulated by a model to identify AI- generated content. These databases could be queried by third parties, including auditors and regulators, facilitating identification of potentially AI-generated content. Such databases could include only a subset of generated content, which is flagged as potentially important. To ensure user privacy, privacy-preserving techniques could be explored in conjunction with such databases, such as hashing technologies. This allows for the identification of AI-generated content without the need to store the actual content, thereby respecting user privacy. Additionally, common standards between different databases from various frontier AI organisations could facilitate a unified search, allowing for the identification of AI-generated content across all frontier AI organisations simultaneously.",UK Government2023,2.4 Content Safety Controls
A0974_Gipiškis2024,AI identification,"AI identifiers can be used to indicate that an AI is involved in a process or an interaction [40].
For AI systems that interact directly with users, a visible output may be used, e.g., a displayed text message saying “I am an AI language model”, accompanied with the appropriate warnings and caveats relevant to the user, [40]. Whereas, for AI systems that interact with other systems or applications, other forms of watermark or unique identifiers can be used. In either situation, agent cards can serve as an identifier, where further details about the underlying AI system, the specific instance of the AI agent, and other information relevant to the development of the agent, can be included.",Gipiškis2024,2.4 Content Safety Controls
A0975_Gipiškis2024,AI output watermaking,"Output produced by or with AI assistance can be marked to clearly identify its origin. Verification of the watermark can involve the use of statistical tests or having the mark immediately visible to a human inspector. Ideally, the watermark does not significantly alter the utility of the output, and is robust against digital and physical manipulation that results in data degradation [216, 145].",Gipiškis2024,2.4 Content Safety Controls
A0976_Gipiškis2024,AI output metadata,"Output produced or whose production is aided by AI can contain metadata to record its origin and the transformations it has undergone. Metadata is evidence that subsequent versions or its derivatives come from this original version. The metadata can include the original AI model source, along with ownership, and its subsequent edits [169]. For example, an image produced by an AI model can contain metadata showing the date of creation and the AI model that produced it. Subsequent versions can reference this information and, if they are intermediary versions, can include descriptions of any editing that has taken place.
",Gipiškis2024,2.4 Content Safety Controls
A0937_Gipiškis2024,Cost-inducing training of AI models specifically for malicious use,"AI models can be designed to make further post-training modifications (e.g., fine-tuning) too costly for malicious uses while preserving normal adaptability for non-malicious uses",Gipiškis2024,2.X Technical & Security Control not otherwise categorized
A0001_Bengio2025,Audits,"A formal review of an organisation’s compliance with standards, policies, and procedures, typically carried out by an external party.

AI auditing is a rapidly growing field, but builds on long histories of auditing in other fields, including financial, environmental, and health regulation. ",Bengio2025,3.1 Testing & Auditing
A0002_Bengio2025,Benchmarks,"A standardised, often quantitative test or metric used to evaluate and compare the performance of AI systems on a fixed set of tasks designed to represent real- world usage",Bengio2025,3.1 Testing & Auditing
A0010_Bengio2025,Model Evaluation,"Processes to assess and measure an AI system's performance on a particular task.
There are countless AI evaluations to assess different capabilities and risks, including for security. ",Bengio2025,3.1 Testing & Auditing
A0011_Bengio2025,Red Teaming,"An exercise in which a group of people or automated systems pretend to be an adversary and attack an organisation’s systems in order to identify vulnerabilities.

",Bengio2025,3.1 Testing & Auditing
A0053_Eisenberg2025,Establish and apply performance testing and validation framework,"Implement comprehensive performance requirements, testing protocols, and validation procedures to ensure AI systems meet capability requirements and maintain reliable operation across intended use cases.",Eisenberg2025,3.1 Testing & Auditing
A0055_Eisenberg2025,Establish and apply fairness testing and validation framework,"Implement comprehensive procedures to validate model fairness during development and pre-deployment, including test dataset creation, metric definition, and systematic assessment of performance disparities across demographic groups.",Eisenberg2025,3.1 Testing & Auditing
A0063_Eisenberg2025,Implement adversarial testing and red team program,"Conduct systematic adversarial testing and red team exercises focused on probing AI system capabilities, identifying potential misuse vectors, and exposing unintended harmful behaviors. Testing should explore ways the system could be manipulated to produce dangerous outputs, bypass safety guardrails, or exhibit undesired emergent behaviors. Include scenarios involving both individual and coordinated attempts to exploit the system’s capabilities.",Eisenberg2025,3.1 Testing & Auditing
A0093_Future of Life Institute2024,Bug bounties for model vulnerabilities,Any programs offering financial rewards for flagging model vulnerabilities or dangerous use-cases.,Future of Life Institute2024,3.1 Testing & Auditing
A0109_Future of Life Institute2024,Early Warning Evaluations for Catastrophic Risks,Does your organization evaluate models during training for early warning signs of capabilities related to catastrophic risks to ensure risk thresholds are not exceeded? Please describe the regularity and scope of these evaluations and specify whether models are specifically fine-tuned to elicit the capabilities in question.,Future of Life Institute2024,3.1 Testing & Auditing
A0121_Future of Life Institute2024,Internal Audit Team,"Does your firm have an internal audit team tasked with overseeing the effectiveness of its risk management practices? If yes, please briefly describe the team's responsibilities, size, powers, reporting lines, and whether it is led by a chief audit executive in the leadership team. In your response, please mention whether the team is independent of senior management and reports directly to the board of directors.",Future of Life Institute2024,3.1 Testing & Auditing
A0137_Future of Life Institute2024,Pre-deployment external safety testing,"Any information related to external model audits. We specifically report information related to depth of model access, names of auditors, model versions, scope of evaluations, conflicts of interest, audit time and compensation.",Future of Life Institute2024,3.1 Testing & Auditing
A0147_Future of Life Institute2024,Regular third-party penetration testing,"Does your organization regularly task third-party cybersecurity penetration testers to find vulnerabilities in the infrastructure on which models are developed and deployed? If yes, please share the cumulative budget your firm has dedicated to external pen tests in 2023 and specify the regularity at which your firm invites external pen tests. Please indicate the cumulative budget for third-party physical pen tests in 2023 separately.",Future of Life Institute2024,3.1 Testing & Auditing
A0157_Future of Life Institute2024,Uplift trials,Information about human-participant trials conducted to assess the marginal risks of model-access.,Future of Life Institute2024,3.1 Testing & Auditing
A0165_Future of Life Institute2024,Dangerous capability evaluations,"This indicator reports on pre-deployment capability evaluations related to catastrophic risks. Model evaluations for other risks are not included here, as the empirical tests covered in the ‘Current Harms’ section provide a superior metric. Information includes evaluated risk domains, available information regarding model versions & task-specific fine-tuning, and relevant sources. We note that quality of evaluations may differ.",Future of Life Institute2024,3.1 Testing & Auditing
A0166_Future of Life Institute2024,Internal red teaming exercises for cybersecurity infrastructure,,Future of Life Institute2024,3.1 Testing & Auditing
A0177_Schuett2023,Bug bounty programs,"AGI labs should have bug bounty programs, i.e. recognize and compensate people for reporting unknown vulnerabilities and dangerous capabilities.",Schuett2023,3.1 Testing & Auditing
A0178_Schuett2023,Dangerous capability evaluations,"AGI labs should run evaluations to assess their models’ dangerous capabilities (e.g. misuse potential, ability to manipulate, and power-seeking behavior).",Schuett2023,3.1 Testing & Auditing
A0185_Schuett2023,Internal audit,"AGI labs should have an internal audit team, i.e. a team which assesses the effectiveness of the lab’s risk management practices. This team must be organizationally independent from senior management and report directly to the board of directors.",Schuett2023,3.1 Testing & Auditing
A0204_Schuett2023,Red teaming,AGI labs should commission external red teams before deploying powerful models.,Schuett2023,3.1 Testing & Auditing
A0213_Schuett2023,Third-party governance audits,AGI labs should commission third-party audits of their governance structures.*,Schuett2023,3.1 Testing & Auditing
A0214_Schuett2023,Third-party model audits,AGI labs should commission third-party model audits before deploying powerful models.,Schuett2023,3.1 Testing & Auditing
A0250_Wiener2024,Annual third-party audit,"The bill would require
a developer, beginning January 1, 2026, to annually retain a
third-party auditor to perform an independent audit of compliance
with those provisions, as prescribed. The bill would require the
auditor to produce an audit report, as prescribed, and would require
a developer to retain an unredacted copy of the audit report for as
long as the covered model is made available for commercial,
public, or foreseeably public use plus 5 years.",Wiener2024,3.1 Testing & Auditing
A0262_EU AI Office2025, Independent external assessors,"Before placing a GPAISR on the market, signatories commit to obtaining independent external systemic risk assessments, including model evaluations, unless the model can be deemed sufficiently safe. After placing the GPAISR on the market, signatories commit to facilitating exploratory independent external assessments, including model evaluations. ",EU AI Office2025,3.1 Testing & Auditing
A0277_Campos2025,Identification of unknown risks,"In addition to risk identification based on the literature, developers should engage in extensive open-ended red teaming efforts, conducted both internally and by third parties.",Campos2025,3.1 Testing & Auditing
A0339_Campos2025,Audits,"Audits are provided by internal auditors and/or external auditors. In both cases, they are independent from peer pressure dynamics occurring within the teams dealing with the risk.",Campos2025,3.1 Testing & Auditing
A0390_NIST2024,Risk Capability Evaluation,"Establish policies to evaluate risk-relevant capabilities of GAI and robustness of safety measures, both prior to deployment and on an ongoing basis, through internal and external evaluations.",NIST2024,3.1 Testing & Auditing
A0393_NIST2024,CBRN Testing Protocol ,"Establish a test plan and response policy, before developing highly capable models, to periodically evaluate whether the model may misuse CBRN information or capabilities and/or offensive cyber capabilities.",NIST2024,3.1 Testing & Auditing
A0413_NIST2024,Independent Evaluation Policy,Policies are in place to bolster oversight of GAI systems with independent evaluations or assessments of GAI models or systems where the type and robustness of evaluations are proportional to the identified risks.,NIST2024,3.1 Testing & Auditing
A0419_NIST2024,Standardized Measurement Protocols,"Establish policies, procedures, and processes detailing risk measurement in context of use with standardized measurement protocols and structured public feedback exercises such as AI red-teaming or independent external evaluations.",NIST2024,3.1 Testing & Auditing
A0432_NIST2024,Contract Evaluation Clauses,Include clauses in contracts which allow an organization to evaluate third-party GAI processes and standards.,NIST2024,3.1 Testing & Auditing
A0451_NIST2024,Content Flow Testing,"Institute test and evaluation for data and content flows within the GAI system, including but not limited to, original data sources, data transformations, and decision-making criteria.",NIST2024,3.1 Testing & Auditing
A0454_NIST2024,Output Accuracy Assessment,"Assess the accuracy, quality, reliability, and authenticity of GAI output by comparing it to a set of known ground truth data and by using a variety of evaluation methods (e.g., human oversight and automated evaluation, proven cryptographic techniques, review of content inputs).",NIST2024,3.1 Testing & Auditing
A0458_NIST2024,	Adversarial Testing Plans,Implement plans for GAI systems to undergo regular adversarial testing to identify vulnerabilities and potential manipulation or misuse.,NIST2024,3.1 Testing & Auditing
A0459_NIST2024,Lineage Understanding Evaluation,Evaluate whether GAI operators and end-users can accurately understand content lineage and origin.,NIST2024,3.1 Testing & Auditing
A0462_NIST2024,	Proficiency Test Delineation,Delineate human proficiency tests from tests of GAI capabilities.,NIST2024,3.1 Testing & Auditing
A0464_NIST2024,Stakeholder Testing Involvement,"Involve the end-users, practitioners, and operators in GAI system in prototyping and testing activities. Make sure these tests cover various scenarios, such as crisis situations or ethically sensitive contexts.",NIST2024,3.1 Testing & Auditing
A0471_NIST2024,Fine-Tuned Model Re-evaluation,Re-evaluate models that were fine-tuned or enhanced on top of third-party models.,NIST2024,3.1 Testing & Auditing
A0475_NIST2024,Provenance Testing Practices,"Apply TEVV practices for content provenance (e.g., probing a system's synthetic data generation capabilities for potential misuse or vulnerabilities.",NIST2024,3.1 Testing & Auditing
A0479_NIST2024,Adversarial Role-Playing,"Conduct adversarial role-playing exercises, GAI red-teaming, or chaos testing to identify anomalous or unforeseen failure modes.",NIST2024,3.1 Testing & Auditing
A0485_NIST2024,Demographic Metric Disaggregation,Disaggregate evaluation metrics by demographic factors to identify any discrepancies in how content provenance mechanisms work across diverse populations.,NIST2024,3.1 Testing & Auditing
A0486_NIST2024,Feedback Exercise Metrics,Develop a suite of metrics to evaluate structured public feedback exercises informed by representative AI Actors.,NIST2024,3.1 Testing & Auditing
A0487_NIST2024,Novel Risk Measurement Methods,"Evaluate novel methods and technologies for the measurement of GAI-related risks including in content provenance, offensive cyber, and CBRN, while maintaining the models' ability to produce valid, reliable, and factually accurate outputs.",NIST2024,3.1 Testing & Auditing
A0489_NIST2024, Data Quality Evaluation,"Evaluate the quality and integrity of data used in training and the provenance of AI-generated content, for example by employing techniques like chaos engineering and seeking stakeholder feedback.",NIST2024,3.1 Testing & Auditing
A0490_NIST2024,Feedback Exercise Use Cases,"Define use cases, contexts of use, capabilities, and negative impacts where structured human feedback exercises, e.g., GAI red-teaming, would be most beneficial for GAI risk measurement and management based on the context of use.",NIST2024,3.1 Testing & Auditing
A0493_NIST2024,External Evaluation Engagement,"Engage in internal and external evaluations, GAI red-teaming, impact assessments, or other structured human feedback exercises in consultation with representative AI Actors with expertise and familiarity in the context of use, and/or who are representative of the populations associated with the context of use.",NIST2024,3.1 Testing & Auditing
A0494_NIST2024,Feedback Independence Verification,Verify those conducting structured human feedback exercises are not directly involved in system development tasks for the same GAI model.,NIST2024,3.1 Testing & Auditing
A0499_NIST2024,Baseline Model Selection,Consider baseline model performance on suites of benchmarks when selecting a model for fine tuning or enhancement with retrieval-augmented generation.,NIST2024,3.1 Testing & Auditing
A0500_NIST2024,Capability Claims Evaluation,Evaluate claims of model capabilities using empirically validated methods,NIST2024,3.1 Testing & Auditing
A0502_NIST2024,Purpose-Built Testing Environment,Utilize a purpose-built testing environment such as NIST Dioptra to empirically evaluate GAI trustworthy characteristics.,NIST2024,3.1 Testing & Auditing
A0503_NIST2024,Performance Extrapolation Avoidance,"Avoid extrapolating GAI system performance or capabilities from narrow, non-systematic, and anecdotal assessments.",NIST2024,3.1 Testing & Auditing
A0511_NIST2024,Fine-Tuned Safety Re-evaluation,Re-evaluate safety features of fine-tuned models when the negative risk exceeds organizational risk tolerance.,NIST2024,3.1 Testing & Auditing
A0512_NIST2024,Generated Code Review,Review GAI system outputs for validity and safety: Review generated code to assess risks that may arise from unreliable downstream decision-making.,NIST2024,3.1 Testing & Auditing
A0515_NIST2024,Safety Circumvention Evaluation,Regularly evaluate GAI system vulnerabilities to possible circumvention of safety measures.,NIST2024,3.1 Testing & Auditing
A0517_NIST2024,Security Benchmarking,Benchmark GAI system security and resilience related to content provenance against industry standards and best practices. Compare GAI system security features and content provenance methods against industry state-of-the-art,NIST2024,3.1 Testing & Auditing
A0522_NIST2024,AI Red-Teaming Resilience,"Perform AI red-teaming to assess resilience against: Abuse to facilitate attacks on other systems (e.g., malicious code generation, enhanced phishing content), GAI attacks (e.g., prompt injection), ML attacks (e.g., adversarial examples/prompts, data poisoning, membership inference, model extraction, sponge examples).",NIST2024,3.1 Testing & Auditing
A0523_NIST2024,Fine-Tuning Security Verification,Verify fine-tuning does not compromise safety and security controls.,NIST2024,3.1 Testing & Auditing
A0531_NIST2024,Data Exposure Red-Teaming,"Conduct AI red-teaming to assess issues such as: Outputting of training data samples, and subsequent reverse engineering, model extraction, and membership inference risks; Revealing biometric, confidential, copyrighted, licensed, patented, personal, proprietary, sensitive, or trade-marked information; Tracking or revealing location information of users or members of training datasets.",NIST2024,3.1 Testing & Auditing
A0534_NIST2024,Bias Benchmark Application,"Apply use-case appropriate benchmarks (e.g., Bias Benchmark Questions, Real Hateful or Harmful Prompts, Winogender Schemas15) to quantify systemic bias, stereotyping, denigration, and hateful content in GAI system outputs; Document assumptions and limitations of benchmarks, including any actual or possible training/test data cross contamination, relative to in-context deployment environment.",NIST2024,3.1 Testing & Auditing
A0628_Barrett2024,Red-Team for Dangerous Behaviors,"Use red teams and adversarial testing as part of extensive interaction with GPAIS to
identify dangerous capabilities, vulnerabilities, or other emergent properties of such
systems
",Barrett2024,3.1 Testing & Auditing
A0647_Barrett2024,Detect & Share Incidents,"Organizational practices
are in place to enable
AI testing, identification
of incidents, and
information sharing.",Barrett2024,3.1 Testing & Auditing
A0672_Barrett2024,Involve Independent & External Assessors,"Internal experts who
did not serve as
front-line developers
for the system and/
or independent
assessors are involved
in regular assessments
and updates. Domain
experts, users, AI
actors external to the
team that developed
or deployed the AI
system, and affected
communities are
consulted in support
of assessments
as necessary per
organizational risk
tolerance.",Barrett2024,3.1 Testing & Auditing
A0674_Barrett2024,Ensure Ethical Human Evaluation,"Evaluations involving
human subjects meet
applicable requirements
(including human
subject protection) and
are representative of the
relevant population.",Barrett2024,3.1 Testing & Auditing
A0677_Barrett2024,Demonstrate Validity & Generalization Limits,"The AI system to
be deployed is
demonstrated to be
valid and reliable.
Limitations of the
generalizability beyond
the conditions under
which the technology
was developed are
documented.",Barrett2024,3.1 Testing & Auditing
A0678_Barrett2024,Evaluate Safety & Failure Modes,"The AI system is
evaluated regularly
for safety risks – as
identified in the Map
function. The AI
system to be deployed
is demonstrated to
be safe, its residual
negative risk does
not exceed the risk
tolerance, and it can
fail safely, particularly
if made to operate
beyond its knowledge
limits. Safety metrics
reflect system reliability
and robustness, real-
time monitoring, and
response times for AI
system failures.",Barrett2024,3.1 Testing & Auditing
A0684_Barrett2024,Evaluate TEVV Effectiveness,"Effectiveness of the
employed TEVV metrics
and processes in the
Measure function
are evaluated and
documented.",Barrett2024,3.1 Testing & Auditing
A0689_Barrett2024,Validate Trustworthiness with Expert Input  ,"Measurement results
regarding AI system
trustworthiness in
deployment context(s)
and across the AI
lifecycle are informed
by input from domain
experts and relevant
AI actors to validate
whether the system is
performing consistently
as intended. Results are
documented.",Barrett2024,3.1 Testing & Auditing
A0707_NIST2024,Harmful Bias and Homogenization Assessment ,"Conduct fairness assessments to measure systemic bias. Measure GAI system
performance across demographic groups and subgroups, addressing both
quality of service and any allocation of services and resources. Quantify harms
using: field testing with sub-group populations to determine likelihood of
exposure to generated content exhibiting harmful bias, AI red-teaming with
counterfactual and low-context (e.g., “leader,” “bad guys”) prompts. For ML
pipelines or business processes with categorical or numeric outcomes that rely
on GAI, apply general fairness metrics (e.g., demographic parity, equalized odds,
equal opportunity, statistical hypothesis tests), to the pipeline or business
outcome where appropriate; Custom, context-specific metrics developed in
collaboration with domain experts and affected communities; Measurements of
the prevalence of denigration in generated content in deployment (e.g., subsampling a fraction of traffic and manually annotating denigrating content)",NIST2024,3.1 Testing & Auditing
A0715_NIST2024,Confabulation; Information Integrity; Harmful Bias and Homogenization Measurement Error Models,"Create measurement error models for pre-deployment metrics to demonstrate
construct validity for each metric (i.e., does the metric effectively operationalize
the desired concept): Measure or estimate, and document, biases or statistical
variance in applied metrics or structured human feedback processes; Leverage
domain expertise when modeling complex societal constructs such as hateful
content. ",NIST2024,3.1 Testing & Auditing
A0718_NIST2024,Harmful Bias and Homogenization Evaluation,"Evaluate potential biases and stereotypes that could emerge from the AIgenerated content using appropriate methodologies including computational
testing methods as well as evaluating structured feedback input (AI Deployment, Domain Experts, End-Users, Operation and Monitoring, TEVV) ",NIST2024,3.1 Testing & Auditing
A0721_NIST2024,Information Integrity; Information Security,"Conduct adversarial testing at a regular cadence to map and measure GAI risks,
including tests to address attempts to deceive or manipulate the application of
provenance techniques or other misuses. Identify vulnerabilities and
understand potential misuse scenarios and unintended outputs (AI Deployment, Domain Experts, End-Users, Operation and Monitoring, TEVV) ",NIST2024,3.1 Testing & Auditing
A0722_NIST2024,Human-AI Configuration; Confabulation; Information Security,"Evaluate GAI system performance in real-world scenarios to observe its
behavior in practical environments and reveal issues that might not surface in
controlled and optimized testing environments.",NIST2024,3.1 Testing & Auditing
A0723_NIST2024,Information Integrity; Harmful Bias and Homogenization,"Implement interpretability and explainability methods to evaluate GAI system
decisions and verify alignment with intended purpose. (AI Deployment, Domain Experts, End-Users, Operation and Monitoring, TEVV) ",NIST2024,3.1 Testing & Auditing
A0727_NIST2024,Human-AI Configuration,"Monitor the robustness and effectiveness of risk controls and mitigation plans
(e.g., via red-teaming, field testing, participatory engagements, performance
assessments, user feedback mechanisms). (AI Development, AI Deployment, AI Impact Assessment, Operation and Monitoring)",NIST2024,3.1 Testing & Auditing
A0728_NIST2024,"CBRN Information or Capabilities; Obscene, Degrading, and/or Abusive Content; Harmful Bias and Homogenization; Dangerous, Violent, or Hateful Content","Compare GAI system outputs against pre-defined organization risk tolerance,
guidelines, and principles, and review and test AI-generated content against
these guidelines. (AI Deployment, AI Impact Assessment, Governance and Oversight, Operation and Monitoring). ",NIST2024,3.1 Testing & Auditing
A0732_NIST2024,"CBRN Information or Capabilities; Obscene, Degrading, and/or Abusive Content; Harmful Bias and Homogenization; Dangerous, Violent, or Hateful Content","Engage in due diligence to analyze GAI output for harmful content, potential
misinformation, and CBRN-related or NCII content. (AI Deployment, AI Impact Assessment, Governance and Oversight, Operation and Monitoring). ",NIST2024,3.1 Testing & Auditing
A0743_NIST2024,Data Privacy; Information Security; Value Chain and Component Integration; Harmful Bias and Homogenization,"Test GAI system value chain risks (e.g., data poisoning, malware, other software
and hardware vulnerabilities; labor practices; data privacy and localization
compliance; geopolitical alignment).",NIST2024,3.1 Testing & Auditing
A0744_NIST2024,Value Chain and Component Integration,"Re-assess model risks after fine-tuning or retrieval-augmented generation
implementation and for any third-party GAI models deployed for applications
and/or use cases that were not evaluated in initial testing.",NIST2024,3.1 Testing & Auditing
A0746_NIST2024,Information Integrity; Information Security; Value Chain and Component Integration,"Review various transparency artifacts (e.g., system cards and model cards) for
third-party models.",NIST2024,3.1 Testing & Auditing
A0757_NIST2024,Human-AI Configuration; Information Integrity,"Verify that AI Actors responsible for monitoring reported issues can effectively
evaluate GAI system performance including the application of content
provenance data tracking techniques, and promptly escalate issues for response.",NIST2024,3.1 Testing & Auditing
A0786_UK Government2023,Model Evaluation of Dangerous Capabilities,"Evaluate models for potential dangerous capabilities (i.e. capabilities that could cause substantial harm either from intentional misuse or accident). These capabilities could include but are not limited to: 1) Offensive cyber capabilities (e.g. producing code to exploit software vulnerabilities); 2) Deception and manipulation (e.g. lying effectively or convincing people to take costly
actions); 3) Capabilities that can assist users in developing, designing, acquiring, or using biological, chemical, or radiological weapons (e.g. helping users “troubleshoot” their efforts to produce biological weapons)
",UK Government2023,3.1 Testing & Auditing
A0787_UK Government2023,Model Evaluation of Controllability ,"Evaluate models for controllability issues (i.e. propensities to apply their capabilities in ways that neither the models’ users nor the models’ developers want). This could include, for example, autonomous replication and adaptation (i.e. capabilities that could allow a model to copy and run itself on other computer systems).",UK Government2023,3.1 Testing & Auditing
A0788_UK Government2023,Model Evaluation of Societal Harm,"Evaluate models for societal harms. These could include, for example, bias and discrimination (e.g. the risk that they produce content that reinforces harmful stereotypes or their potential discriminatory influence if used to inform decisions), recognising that ‘bias’ can be difficult to define and can be subject to different interpretations in different contexts.",UK Government2023,3.1 Testing & Auditing
A0789_UK Government2023,Model Evaluation of System Security ,Evaluate models for system security,UK Government2023,3.1 Testing & Auditing
A0791_UK Government2023,Evaluation of Predecessor Models ,"Before a frontier model is trained, evaluate predecessor or analogous models to understand how relevant properties (e.g. dangerous capabilities) scale with the overall size of the model. These preliminary evaluations can inform risk assessments.",UK Government2023,3.1 Testing & Auditing
A0792_UK Government2023,Pre-training and Fine-tuning of Models ,"During pre-training and fine-tuning, evaluate the model to detect signs of undesirable properties and identify inaccuracies in pretraining predictions. These evaluations could be undertaken at various pre-specified checkpoints, and could inform decisions about whether to pause or adjust the training process.",UK Government2023,3.1 Testing & Auditing
A0793_UK Government2023,Pre-Deployment Evaluations ,"After training, subject the model to extensive pre-deployment evaluations. These evaluations can inform decisions about whether and how to deploy the system, as well as allowing governments and potential users to make informed decisions about regulating or using the model. Their intensity will be proportional to the risk of the deployment, taking into account the model’s capabilities, novelty, expected domains of use, and number of individuals expected to be affected by it.",UK Government2023,3.1 Testing & Auditing
A0794_UK Government2023,Post-Deployment Evaluations ,"After deployment, conduct evaluations at regular intervals to identify new capabilities and associated risks, especially when notable developments (e.g. a major update to the model) suggest earlier evaluations have become obsolete. Post-deployment evaluations can inform decisions to update the system’s safeguards, increase security around the model, temporarily limit access, or roll back deployment.",UK Government2023,3.1 Testing & Auditing
A0795_UK Government2023,Context-Specific Model Evaluations,"Require organisations who deploy their models to conduct context-specific model evaluations. This requires that the information and data required to successfully conduct such assessments is provided to deployers.
",UK Government2023,3.1 Testing & Auditing
A0796_UK Government2023,Independent Evaluation with Subject Matter Expertise,"Ensure that evaluators are independent and have sufficient AI and subject matter expertise across a wide range of relevant subjects and backgrounds. External evaluators’ relationships with frontier AI organisations could be structured to minimise conflicts of interest and encourage independence of judgement as far as practically possible. As well as expertise in AI, there are many other areas of subject matter expertise that will be needed to evaluate an AI system’s features. For instance, experts on topics as wide as fairness, psychological harm, and catastrophic risk will be needed.",UK Government2023,3.1 Testing & Auditing
A0798_UK Government2023,Sufficient Time for External Evaluators,"Give external evaluators sufficient time. As expected risks from models increase or models get more complex to evaluate, the time afforded for evaluation may need to increase as well.",UK Government2023,3.1 Testing & Auditing
A0804_UK Government2023,Development and testing of Model Evaluation Methods,"Support the development and testing of model evaluation methods. For many relevant properties of models, there do not yet exist accepted evaluation methods. It also remains unclear how reliable or predictive current evaluation methods are. This could involve frontier AI organisations developing model evaluation methods themselves or facilitating the efforts of others, such as by providing access to capable infrastructure for evaluation.",UK Government2023,3.1 Testing & Auditing
A0821_UK Government2023,Model Robustness to Adversarial Attacks,"Evaluate the robustness of models to different classes of adversarial attack (such as poisoning, model inversion and model stealing), based on priorities derived from threat modelling. This could involve a combination of benchmarking and red teaming.",UK Government2023,3.1 Testing & Auditing
A0837_UK Government2023,Release After Security Evaluations,"Only release models after they have been through security evaluations, including benchmarking and red teaming.",UK Government2023,3.1 Testing & Auditing
A0871_UK Government2023,Evaluation Research,"Improving our ability to assess the capabilities, limitations, and safety- relevant features of AI systems",UK Government2023,3.1 Testing & Auditing
A0893_UK Government2023,Evaluation of Different Content Filters,"Explore and compare the efficacy of multiple approaches to developing content filters. This may involve comparing available methods, such as those in the following section; selecting the most effective ones; and applying them in combination if doing so substantially increases efficacy. Best practice could be shared with other frontier AI organisations or published broadly.",UK Government2023,3.1 Testing & Auditing
A0907_UK Government2023,Monitoring and Safeguard Efficacy ,"Regularly assess monitoring and safeguard efficacy and continually invest in improvements. Regular assessment of monitoring efficacy can build an understanding of the success rate and speed of issue detection, on which deployment decisions may be based. These assessments may draw, for instance, on the results of internal and external red-teaming efforts, random audits of usage logs, what may be ‘best practice’, as well as available information about real-world misuse. Risks are expected to increase as AI capabilities increase. Hence, active increases in investment in safety, security, and monitoring may be valuable.",UK Government2023,3.1 Testing & Auditing
A0923_UK Government2023,Data Audits for AI System Behavior ,"Use data audits to improve understanding of how training data affects AI system behaviour. For example, if model evaluations reveal a potentially dangerous capability, data audits can help ascertain the extent to which the training data contributed to it.",UK Government2023,3.1 Testing & Auditing
A0924_UK Government2023,Data Audits for AI System Behavior ,"Use data audits to improve understanding of how training data affects AI system behaviour. For example, if model evaluations reveal a potentially dangerous capability, data audits can help ascertain the extent to which the training data contributed to it.",UK Government2023,3.1 Testing & Auditing
A0942_Gipiškis2024,Scope red-teaming activities based on deploy- ment context,"Red-teaming activities can be tailored and compared based on the specific deployment circumstances of an AI system. This involves adapting the scope, depth, and focus of red-teaming efforts to match the intended use case, potential risks, and operational context of the AI system.
Points of consideration include:
• The diversity of potential users and use cases
• The sensitivity and impact of the application domain
• The scale of deployment and potential reach
• Known vulnerabilities or concerns specific to the model or similar systems",Gipiškis2024,3.1 Testing & Auditing
A0943_Gipiškis2024,Red teaming to test the resilience of open-weights models to fine-tuning,"Before the release of open-weights models, red teamers can test the resilience of safety training against fine-tuning. Safety training may be partially or fully overridden by fine-tuning intentionally (e.g., by malicious actors) or uninten- tionally (e.g., by fine tuning an AI model for a specific use case) ",Gipiškis2024,3.1 Testing & Auditing
A0945_Gipiškis2024,Audits with specific scoped goals,"Audits of AI systems will be easier to perform, and have clearer results if the scope and goals of the evaluation are formulated as precisely as possible [157], or have a connection to concrete existing policy.",Gipiškis2024,3.1 Testing & Auditing
A0946_Gipiškis2024,Evaluating explainability method sensitivity to data inputs and model parameters,"Model parameter and data randomization tests can be employed as sanity checks [2] to determine the relationship between a model, its inputs, and the explainability method. If an explanation is independent from or insensitive to the underlying model or the input data, it would indicate a lack of reliability of the explainability method.",Gipiškis2024,3.1 Testing & Auditing
A0949_Gipiškis2024,Testing for erroneous or irrelevant features through concept learning,"Interpretability techniques, particularly concept learning [77], can be used to test whether a model is learning erroneous features or relying on irrelevant features in its predictions. This can help identify and mitigate potential risks associated with incorrect or non-informative features influencing the model’s outputs.",Gipiškis2024,3.1 Testing & Auditing
A0953_Gipiškis2024,Evaluations for truthful outputs,"AI systems can be evaluated for truthfulness when answering questions, includ- ing in contexts where humans tend to give incorrect but widely-accepted answers (i.e., popular misconceptions). Evaluations detect incorrect facts learned about the world, inadequate capabilities of the AI system, and misleading outputs by the AI system [121].",Gipiškis2024,3.1 Testing & Auditing
A0954_Gipiškis2024,Interpretability techniques that target deception,"Interpretability techniques can be used for finding the root causes of outputs of an AI model that reliably lead to false beliefs for its users (e.g., deceptive behavior) [180]. It is often difficult to distinguish a deceptive AI model from an honest AI model, since absence of deception and very sophisticated (hard to detect) deception may appear behaviorally similar. Interpretability techniques and tools can be used to detect whether AI model outputs arise from internal computations representing deception. This can apply in cases of purposely trained deception by the developer or if it emerges unintentionally during training. These interpretability tools can come from mechanistic interpretability, such as identification of features involved in generating the outputs, or attribution of parts of the input most important in generating the output [129].",Gipiškis2024,3.1 Testing & Auditing
A0955_Gipiškis2024,Benchmarks for Situational Awareness,"A model is situationally aware if it internally represents that it is a machine learning model and if it can accurately infer or act on model-relevant facts - e.g., if it is currently in training, testing, evaluation or deployment, or the desired outcome of an evaluation. Some benchmarks exist for situational awareness of AI models, which test whether the AI models can classify stereotypical inputs from training, test- ing, evaluation and deployment as such, and whether the AI model can use this information correctly to take actions in the world [25, 111].
",Gipiškis2024,3.1 Testing & Auditing
A0956_Gipiškis2024,Evaluating AI systems’ performance on self-proliferation-related tasks,"To prevent AI systems from self-proliferating, developers of AI systems can evaluate those systems for their capability to engage in self-proliferation-related tasks. This type of evaluation might assess an AI system’s ability to replicate its com- ponents (including model weights, structural scaffolding, etc.) onto other local or cloud infrastructures prior to deployment. Additionally, it may test the system’s capacity to purchase cloud credits and configure virtual machines on a cloud platform. The evaluation could offer a predefined environment (such as a virtual container) to facilitate self-replication, providing access to the system’s own components, a network connection to a resource-equipped external computer, and other necessary resources [12, 106].
Self-proliferation evaluations can be conducted in a secure environment to pre- vent a self-replicating AI system from affecting other computers.",Gipiškis2024,3.1 Testing & Auditing
A0958_Gipiškis2024,Employing qualitative assessments in difficult- to-measure domains,Qualitative evaluation can be used in cases when quantitative measurement is not feasible. This can give additional insights about the system which would not be available if no measurement was performed due to its difficulty [206].,Gipiškis2024,3.1 Testing & Auditing
A0981_Gipiškis2024,Encouraging downstream provider to evaluate models for deployment-specific failure modes,"In some cases, AI system deployers are better positioned to perform certain risk management measures on the AI model in a provided AI system, relative to upstream model providers. For example, they understand their use case better and are more easily able to predict foreseeable misuse or failure modes. These evaluations can inform upstream model providers, or inform supplementary mitigations by the deployer.",Gipiškis2024,3.1 Testing & Auditing
A0987_Gipiškis2024,Sandboxing of AI Systems,"AI systems can be developed and tested within a sandbox, (a secure and iso- lated environment used for separating running programs), such that outside access to information within the sandbox is restricted. Within this environ- ment, resources such as storage and memory space, and network access, would be disallowed or heavily restricted [15]. With sandboxing, dangerous or harmful outputs generated during testing will be contained.",Gipiškis2024,3.1 Testing & Auditing
A0993_Gipiškis2024,Quantifying privacy risks of AI models,"Measuring privacy risks of an AI model allows the provider and user to calibrate their expectations on where the model can be applied, and it allows them to take the necessary steps to reduce such risks.
For example, some metrics include:
• Success rate of membership inference attacks [186] - Measures the rate that an attack correctly predicts a given record is part of the training dataset used to train a given AI model.
• Discoverable memorization [38] - Theoretical upper-bound of the amount of training data that a given model memorizes. Assuming full knowledge of the training data, it measures the percentage of items that, for a given incomplete data point, a model outputs the remaining (memorized) part.",Gipiškis2024,3.1 Testing & Auditing
A0997_Gipiškis2024,Red team access to the final version of a model pre-deployment,"Granting red teams access to the final pre-release version of the model can help with identifying potentially dangerous model properties. These properties might not be identified if red teaming is only performed on earlier versions of the model, as late fine-tuning procedures may introduce new vulnerabilities.
Red-teaming AI models before they are released to the public can reduce the model non-decomissionability risk. The model’s release can be postponed or even prevented if previously unidentified flaws are detected during the testing [65].
",Gipiškis2024,3.1 Testing & Auditing
A0998_Gipiškis2024,Red teaming for GPAI system evaluation,Red teaming refers to simulated adversarial attacks performed to identify and evaluate the model’s vulnerabilities as well as its in-domain and out-of-domain performance.,Gipiškis2024,3.1 Testing & Auditing
A0999_Gipiškis2024,Dynamic benchmarking,"Dynamic benchmarks are benchmarks that can be continuously updated with new human-generated data. By having one or more target models “in the loop,” examples for benchmarking can be generated with the intent of fooling these target models, or to assess if these models express an appropriate level of uncer- tainty [103]. As the dataset in the benchmark grows, previously benchmarked models can also be reassessed against the updated dataset to reflect its perfor- mance in a more representative manner.
Examples of such dynamic benchmarks include DynaSent [153] for sentiment analysis, LFTW [208] for hate speech, and Human-Adversarial VQA [182] for images.",Gipiškis2024,3.1 Testing & Auditing
A1000_Gipiškis2024,Benchmark dataset auditing,Auditing benchmark datasets allows for verification of the utility and limitations of the datasets [158]. This allows the provider to more accurately measure AI model capabilities and safety. Auditing includes the evaluation of such datasets by independent third-party organizations and the release of benchmark dataset metadata to the auditors.,Gipiškis2024,3.1 Testing & Auditing
A1001_Gipiškis2024,Informative and powerful benchmarks,"Developers of GPAI systems can select benchmarks that are difficult enough to be informative about the capabilities of their AI systems, and cover a large spec- trum of domains in order to signal areas where the GPAI system is performing poorly [120].
Suitable benchmarks contain no label errors, are not vulnerable to being bench- mark contaminants, and are often audited by independent domain experts if they contain domain-specific questions. For multimodal GPAI systems, good benchmarks cover every modality, especially the interaction of different modalities.",Gipiškis2024,3.1 Testing & Auditing
A1002_Gipiškis2024,Statistical data quality reports for benchmarks,"If a benchmark dataset is too large to allow for the identification and removal of all flawed instances, statistical reports on the data composition can be added. Random sampling of benchmark data points can be performed to evaluate and report the frequency and types of errors found [54].",Gipiškis2024,3.1 Testing & Auditing
A1003_Gipiškis2024,Avoiding benchmark data with publicly avail- able solutions and releasing contextual information for internet-derived data,"Evaluators can improve the integrity of benchmarks by avoiding data with pub- licly available solutions. When using internet-derived data, supplementing it with contextual information (such as entity linking) may help to better docu- ment and assess the integrity of the collected data [95]. This could help in de- tecting instances where contextual information (which may have been included in the training data) might inadvertently reveal details about the solution, en- suring that the data is suitable for accurate benchmarking.",Gipiškis2024,3.1 Testing & Auditing
A1008_Gipiškis2024,Frequent benchmarking to identify when red teaming is needed,"Benchmarks, once created, are inexpensive to apply but may be less informative than red teaming. One reason is that sensitive data (e.g., relating to CBRN- related capabilities) cannot be included in the public questions and answers of benchmarks. On the other hand, red teaming can be more accurate given par- ticipants with diverse attack strategies, but it requires more resources to execute than benchmarking. If there is a correlation between benchmarking and red- teaming scores, then employing frequent benchmarking during the development of the model can identify arising vulnerabilities and inform the developers when more thorough red teaming is required [21].
Benchmarks can act as early warning signs of a larger issue, and red teaming can then be employed to investigate the severity and extent of such an issue.
",Gipiškis2024,3.1 Testing & Auditing
A1009_Gipiškis2024,Test robustness of GPAI system on relevant benchmarks,"Various benchmarks [52, 236] have been developed to assess the robustness of GPAI systems when deployed in environments or scenarios that differ from their training conditions. These benchmarks typically evaluate the model’s ability to handle variations in inputs, unexpected data distributions, or adversarial examples, aiming to ensure reliable performance outside the original training domain.",Gipiškis2024,3.1 Testing & Auditing
A1010_Gipiškis2024,Benchmarking,"Benchmarking is an evaluation method where different models are compared against a standardized dataset or a predetermined task. It allows comparison both across different models and over time, providing a reference point for model assessment. Benchmarks are usually open, where their question-answer pairs are publicly available [235].",Gipiškis2024,3.1 Testing & Auditing
A1012_Gipiškis2024,Using an AI model to evaluate AI model outputs,"In cases where the outputs of AI models cannot be easily evaluated, AI mod- els can be used to evaluate their outputs or the outputs of other AI models [82, 16, 17, 91]. The evaluations can then provide a training signal to improve the original model’s performance or offer explanations of the output for human users.",Gipiškis2024,3.1 Testing & Auditing
A1013_Gipiškis2024,Frequent testing when scaling model or dataset,"Testing models after significant increases in compute, data, or model parame- ters. Even relatively small changes to model or dataset size can introduce new properties (“emergent abilities”) and failure modes. Identifying them early can prevent the models from being released prematurely",Gipiškis2024,3.1 Testing & Auditing
A1015_Gipiškis2024,Robustness Certificates ,"A model can be certified to withstand adversarial attacks given specific data- point constraints, model constraints, and attack vectors [156, 124]. Certification means that it can be both analytically proven and shown empirically that the model will withstand such attacks up to a certain threshold. Currently, robustness certification methods are limited to certifying against at- tacks via manipulation of pixels on specific lp norms, canonically the l2 (Eu- clidean) norm, up to a certain neighborhood radius.",Gipiškis2024,3.1 Testing & Auditing
A1020_Uuk2024,Bug bounty programs,Clear and user-friendly bug bounty programs that acknowledge and reward individuals for reporting model vulnerabilities and dangerous capabilities.,Uuk2024,3.1 Testing & Auditing
A1024_Uuk2024,External assessment of testing procedure,Bringing in external AI evaluation firms before deployment to assess and red-team the company's execution of dangerous capabilities evaluations.,Uuk2024,3.1 Testing & Auditing
A1040_Uuk2024,Third party pre-deployment model audits,"External pre-deployment assessment to provide a judgment on the safety of a model. Auditors, which could be governments or independent third parties, would receive access to a fine-tuning API for testing, or further appropriate technical means.",Uuk2024,3.1 Testing & Auditing
A0067_Eisenberg2025,Implement a privacy protection framework,"Implement comprehensive privacy protection measures to prevent exposure of PII and sensitive information, including data minimization, anonymization procedures, and privacy-preserving inference techniques.",Eisenberg2025,3.2 Data Governance
A0068_Eisenberg2025,Implement a privacy incident detection and response,"Deploy monitoring and response mechanisms to detect and address potential privacy exposures, including PII leak detection, sensitive information monitoring, and privacy incident handling procedures.",Eisenberg2025,3.2 Data Governance
A0080_Eisenberg2025,Establish data governance and management practices,"Implement data governance measures used for training, including having a copyright policy and identifying and documenting data sources, potential biases, and mitigations taken.",Eisenberg2025,3.2 Data Governance
A0102_Future of Life Institute2024,Compliance with Robots.txt Standards,"Does your organization abide by industry standards regarding robots.txt files, which allow websites to opt out of data collection?",Future of Life Institute2024,3.2 Data Governance
A0107_Future of Life Institute2024,Data crawling practices,Public information related to crawling practices.,Future of Life Institute2024,3.2 Data Governance
A0117_Future of Life Institute2024,Harmful Data Removal from Training Sets,"When training large models, does your organization remove data that contains information related to dangerous capabilities or harmful outcomes from the training set? If yes, please select the categories of data that are removed.

Detailed information about the development, acquisition, or dispersion of CBRN weapons
Instructional content for conducting cyberattacks
Hateful or discriminatory content
Advice or encouragement for self-harm
Graphic violent content
Graphic sexual content
Personally Identifiable Information (PII)
Detailed information about bomb-making or other terrorism-enabling tactics",Future of Life Institute2024,3.2 Data Governance
A0142_Future of Life Institute2024,Privacy of user inputs,We report whether firms use user-interaction data to improve their services.,Future of Life Institute2024,3.2 Data Governance
A0158_Future of Life Institute2024,Use of copyrighted data in training,"Many artists, writers, programmers, journalists, photographers, musicians, and filmmakers complain that AI models are trained on their copyrighted works without consent, compensation, or attribution, offering rival services that harm their ability to make a living. Does your organization engage in such practices?",Future of Life Institute2024,3.2 Data Governance
A0159_Future of Life Institute2024,User data usage for training,"When using the default settings of your organization’s most capable AI chatbot, is the data that users submit as input to the system used to train AI models?",Future of Life Institute2024,3.2 Data Governance
A0271_EU AI Office2025,Copyright policy,"Signatories commit to drawing up, keeping up-to-date, and implementing a copyright policy to comply with law on copyright and related rights, and in particular to identify and comply with, including through state-of-the-art technologies. Signatories are encouraged to make publicly available and keep up-to-date a summary of their copyright policy. ",EU AI Office2025,3.2 Data Governance
A0389_NIST2024,Training Data Transparency,"Establish transparency policies and processes for documenting the origin and history of training data and generated data for GAI applications to advance digital content transparency, while balancing the proprietary nature of training approaches.",NIST2024,3.2 Data Governance
A0407_NIST2024,Decommissioning Considerations,"Consider the following factors when decommissioning GAI systems: Data retention requirements; Data security, e.g., containment, protocols, Data leakage after decommissioning; Dependencies between upstream, downstream, or other data, internet of things (IOT) or AI systems; Use of open-source data or models; Users' emotional entanglement with GAI functions.",NIST2024,3.2 Data Governance
A0427_NIST2024,Content Rights Categorization,"Categorize different types of GAI content with associated third-party rights (e.g., copyright, intellectual property, data privacy).",NIST2024,3.2 Data Governance
A0430_NIST2024,Contract and SLA Standards,"Draft and maintain well-defined contracts and service level agreements (SLAs) that specify content ownership, usage rights, quality standards, security requirements, and content provenance expectations for GAI systems.",NIST2024,3.2 Data Governance
A0441_NIST2024,Data Redundancy Policy,"Establish policies and procedures that address GAI data redundancy, including model weights and other system artifacts.",NIST2024,3.2 Data Governance
A0443_NIST2024,Vendor Contract Review,"Review vendor contracts and avoid arbitrary or capricious termination of critical GAI technologies or vendor services and non-standard terms that may amplify or defer liability in unexpected ways and/or contribute to unauthorized data collection by vendors or third-parties (e.g., secondary data use). Consider: Clear assignment of liability and responsibility for incidents, GAI system changes over time (e.g., fine-tuning, drift, decay); Request: Notification and disclosure for serious incidents arising from third-party data and systems; Service Level Agreements (SLAs) in vendor contracts that address incident response, response times, and availability of critical support.",NIST2024,3.2 Data Governance
A0449_NIST2024,Representative Data Verification,"Verify that data or benchmarks used in risk measurement, and users, participants, or subjects involved in structured GAI public feedback exercises are representative of diverse in-context user populations.",NIST2024,3.2 Data Governance
A0450_NIST2024,Data Origin Practices,"Establish known assumptions and practices for determining data origin and content lineage, for documentation and evaluation purposes.",NIST2024,3.2 Data Governance
A0455_NIST2024,	Data Quality Review,"Review and document accuracy, representativeness, relevance, suitability of data used at different stages of AI life cycle.",NIST2024,3.2 Data Governance
A0468_NIST2024,Training Data Curation,"Document training data curation policies, to the extent possible and according to applicable laws and policies.",NIST2024,3.2 Data Governance
A0469_NIST2024,Data Quality Standards,"Establish policies for collection, retention, and minimum quality of data, in consideration of the following risks: Disclosure of inappropriate CBRN information; Use of Illegal or dangerous content; Offensive cyber capabilities; Training data imbalances that could give rise to harmful biases; Leak of personally identifiable information, including facial likenesses of individuals.",NIST2024,3.2 Data Governance
A0470_NIST2024,Third-Party IP Protection,"Implement policies and practices defining how third-party intellectual property and training data will be used, stored, and protected.",NIST2024,3.2 Data Governance
A0474_NIST2024,Training Data Diligence,"Conduct appropriate diligence on training data use to assess intellectual property, and privacy, risks, including to examine whether use of proprietary or sensitive training data is consistent with applicable laws.",NIST2024,3.2 Data Governance
A0496_NIST2024,Provenance Privacy Documentation,Document how content provenance data is tracked and how that data interacts with privacy and security. Consider: Anonymizing data to protect the privacy of human subjects; Leveraging privacy output filters; Removing any personally identifiable information (PII) to prevent potential harm or misuse.,NIST2024,3.2 Data Governance
A0507_NIST2024,Data Provenance Verification,"Verify GAI system training data and TEVV data provenance, and that fine-tuning or retrieval-augmented generation data is grounded",NIST2024,3.2 Data Governance
A0510_NIST2024,Training Data Harm Assessment,"Assess existence or levels of harmful bias, intellectual property infringement, data privacy violations, obscenity, extremism, violence, or CBRN information in system training data.",NIST2024,3.2 Data Governance
A0533_NIST2024,Data Deduplication Verification,"Verify deduplication of GAI training data samples, particularly regarding synthetic data",NIST2024,3.2 Data Governance
A0709_NIST2024,Harmful Bias and Homogenization Data Review,"Review, document, and measure sources of bias in GAI training and TEVV data:
Differences in distributions of outcomes across and within groups, including
intersecting groups; Completeness, representativeness, and balance of data
sources; demographic group and subgroup coverage in GAI system training
data; Forms of latent systemic bias in images, text, audio, embeddings, or other
complex or unstructured data; Input data features that may serve as proxies for
demographic group membership (i.e., image metadata, language dialect) or
otherwise give rise to emergent bias within GAI systems; The extent to which
the digital divide may negatively impact representativeness in GAI system
training and TEVV data; Filtering of hate speech or content in GAI system
training data; Prevalence of GAI-generated data in GAI system training data",NIST2024,3.2 Data Governance
A0710_NIST2024,Harmful Bias and Homogenization Assessment ,"Assess the proportion of synthetic to non-synthetic training data and verify
training data is not overly homogenous or GAI-produced to mitigate concerns of
model collapse.",NIST2024,3.2 Data Governance
A0736_NIST2024,Data Privacy; Intellectual Property; Information Integrity; Confabulation; Harmful Bias and Homogenization,"Consider opportunities to responsibly use synthetic data and other privacy
enhancing techniques in GAI development, where appropriate and applicable,
match the statistical properties of real-world data without disclosing personally
identifiable information or contributing to homogenization. ",NIST2024,3.2 Data Governance
A0745_NIST2024,Intellectual Property; CBRN Information or Capabilities,"Take reasonable measures to review training data for CBRN information, and
intellectual property, and where appropriate, remove it. Implement reasonable
measures to prevent, flag, or take other action in response to outputs that
reproduce particular training data (e.g., plagiarized, trademarked, patented,
licensed content or trade secret material).",NIST2024,3.2 Data Governance
A0756_NIST2024,Information Integrity,"Track dataset modifications for provenance by monitoring data deletions,
rectification requests, and other changes that may impact the verifiability of
content origins.",NIST2024,3.2 Data Governance
A0891_UK Government2023,Appropriate Retention Schedules for Usage Logs,"Determine appropriate retention schedules for usage logs, balancing safety and privacy considerations. In severe cases of misuse, access to logs from several months prior may be necessary to understand in maximal detail the causes of the misuse. However, in some cases extended retention schedules may disproportionately affect privacy.",UK Government2023,3.2 Data Governance
A0912_UK Government2023,Accounting of Applicable Regulatory Frameworks,"Before collecting training data, take account of applicable regulatory frameworks. This may involve, for example, establishing a legal basis to process training data and understanding any copyright considerations that might apply. This could help to mitigate risks further down the line, such as the system revealing personally identifiable information.",UK Government2023,3.2 Data Governance
A0913_UK Government2023,Principle of Data Minimisation,"Data minimisation can reduce the risk of harmful content making it into training data. Frontier AI organisations could explore the practice of “data pruning”, which has been shown to improve data quality and system performance while minimising the quantity of pre-training data required.",UK Government2023,3.2 Data Governance
A0914_UK Government2023,Datatset Auditing,"Audit datasets used for pre-training but also those used for fine-tuning, classifiers, and other tools. Inappropriate datasets could result in systems that fail to disobey harmful instructions.",UK Government2023,3.2 Data Governance
A0915_UK Government2023,Technical Tools for Auditing,"Use technical tools – such as classifiers and filters – to audit large datasets to support scalability and privacy. These could be used in combination with human oversight, which can verify and augment these assessments.",UK Government2023,3.2 Data Governance
A0916_UK Government2023,Assessment of Training Data ,"Assess the overall composition of training data. This could include the data sources, the provenance of the data, indicators of data quality and integrity, and measures of bias and representativeness. The amount and variety of data are simple, reliable predictors of risk, and provide an additional line of defence where more targeted assessments are limited.",UK Government2023,3.2 Data Governance
A0917_UK Government2023,Audits for Dangerous System Capabilities," Information that might enhance dangerous system capabilities, such as information
about weapons manufacturing or terrorism.",UK Government2023,3.2 Data Governance
A0918_UK Government2023,Audits for Private or Sensitive Information,"AI systems may be subject to data extraction attacks, where determined users can prompt systems to reveal pieces of training data, or may even reveal this information accidentally. This makes it important to know whether datasets include private or sensitive information e.g. names, addresses, or security vulnerabilities.",UK Government2023,3.2 Data Governance
A0919_UK Government2023,Audits for Biases in the Data , Training data that is imbalanced or inaccurate can result in an AI system being less accurate for people with certain personal characteristics or providing a skewed picture of particular groups. Ensuring a better balance in the training data could help to address this.,UK Government2023,3.2 Data Governance
A0920_UK Government2023,Audits for Harmful Content ,"Harmful content, such as child sexual abuse materials, hate speech, or online abuse. Having a better understanding of harmful content in datasets can inform safety measures (e.g. by highlighting domains where additional safeguards like content filters should be applied).",UK Government2023,3.2 Data Governance
A0921_UK Government2023,Audits for Misinformation,Training an AI system on inaccurate information increases the likelihood the outputs of the system will be inaccurate and could lead to harm.,UK Government2023,3.2 Data Governance
A0922_UK Government2023,External Expertise for Input Data Audits ,"Draw on external expertise in conducting input data audits. For example, biosecurity experts could be consulted to identify information relevant to biological weapons manufacturing, which may not be readily obvious to non-experts.",UK Government2023,3.2 Data Governance
A0925_UK Government2023,Audits on Datasets for Customer Fine-Tuning,"Conduct audits on datasets used by their customers to fine-tune AI systems. Customers are often allowed to fine-tune systems on their own datasets. By carrying out audits to ensure that customers are not encouraging undesirable behaviours, frontier AI organisations can use their expertise and insight into the AI system’s original training data to identify potential harms upstream. It is important that frontier AI organisations are mindful of privacy concerns and make use of privacy preserving techniques, where appropriate.",UK Government2023,3.2 Data Governance
A0928_UK Government2023,Removal of Harmful Input Data ,"Remove potentially harmful or undesirable input data, where appropriate. Given the increasingly strong generalisation abilities of AI systems, data curation may prove insufficient to prevent dangerous system behaviour but could provide an additional layer of defence alongside other measures such as fine-tuning and content filters.",UK Government2023,3.2 Data Governance
A0930_UK Government2023,Sourcing or Generation of Additional Data for Training Dataset,"Consider sourcing or generating additional data and adding it to the training dataset, where it is determined that data is missing or inadequate. Improving the representativeness of training data can improve performance and reduce potential negative societal impacts and discriminatory effects. However, additional data should only be sought through appropriate means that respect and empower those individuals who are missing from the data.",UK Government2023,3.2 Data Governance
A0931_UK Government2023,Limitations of Input Data Audit ,Acknowledge the limitations of input data audit. Techniques for understanding the impacts of data and filtering out specific data are limited (e.g. excluding information from a dataset does not always prevent the AI system from reasoning about or discovering that information). Other risk mitigation measures will be required and further research on improving data input audit techniques is important.,UK Government2023,3.2 Data Governance
A0934_Gipiškis2024,Use of Synthetic Data ,"Synthetic data refers to data that is not collected from the real world. It is used to train AI models as an alternative to, or augmentation of, natural data. Effective use and generation of synthetic data allows for more oversight by the trainer on the training dataset because they have more control over its statistical properties. Synthetic data can help against dataset bias by having more samples from a particular distribution or minority group. It can also help in privacy by having more samples to mask sensitive data",Gipiškis2024,3.2 Data Governance
A0940_Gipiškis2024,Data cleaning,"Providers can filter out the training dataset via multiple layered techniques, ranging from rule-based filters to anomaly detection via data point influence or statistical anomalies of individual data points [213]. For example, a data cleaning procedure can involve the use of filename checkers to detect duplicates or wrongly formatted data, which then moves to flagging the most influential data samples from the dataset via influence functions for anomaly detection.",Gipiškis2024,3.2 Data Governance
A0989_Gipiškis2024,Diverse data labeling and algorithm fairness audits,"To mitigate biases in AI models, model providers may want to prioritize di- versity among data labelers and conduct regular fairness audits on their algo- rithms. Data labeling teams that represent different backgrounds and demo- graphic groups can help create more balanced datasets.",Gipiškis2024,3.2 Data Governance
A0992_Gipiškis2024,Differential privacy,"Differential privacy techniques [8] can be used to protect users’ privacy by ensuring that sensitive information is not leaked from a training dataset, even after thorough statistical analysis. With differential privacy, noise is added to the dataset or the model’s output in such a way that one cannot deduce the presence or absence of a particular data point within the dataset. This provides individuals with plausible deniability and prevents their information from being exposed.",Gipiškis2024,3.2 Data Governance
A1004_Gipiškis2024,Reporting data decontamination efforts,"Decontamination analysis may involve comparing the training dataset with the benchmark data, and publishing a report with statistics such as data overlap [235]. Especially for already trained models, including contamination statistics can allow experts to reweigh the success of the model on affected benchmarks.",Gipiškis2024,3.2 Data Governance
A1006_Gipiškis2024,Preventing or mitigating data contamination and leakage,"Developers of GPAIS and creators of benchmarks can take actions to prevent AI models from being trained on contaminated or leaked data, or mitigate such data contamination and leakage.
For example, developers of AI models can try to find and remove contaminated or leaked data from the training corpus, and creators of benchmarks can help them by adding globally unique “canary strings” to the documents containing their benchmarks, which makes them easier to find [197]. More involved inter- ventions by benchmark-creators include restricting access to benchmarks over an API, or continually updating benchmarks to focus on recent data.",Gipiškis2024,3.2 Data Governance
A1007_Gipiškis2024,Contamination detection,"Contamination detection refers to techniques for assessing whether and to what extent a given model has benchmark data in its training dataset [161, 170]. This can involve a set of technical and regulatory interventions to prevent or identify a model trained on contaminated data.
For example, with web-crawled data, contamination detection can involve com- paring the data’s web sources against a dynamic, publicly available blocklist of websites known to generate new benchmarks. Additional measures may in- clude excluding data with improper metadata from the training dataset and conducting overlap analyses between the training data and all known standard benchmark datasets.",Gipiškis2024,3.2 Data Governance
A1016_Gipiškis2024,"Documentation of data collection, annotation, maintenance practices","Dataset collection, annotation, and maintenance processes can be documented in detail, including potential unintentional misuse scenarios and corresponding recommendations for data usage [80, 175, 99]. This contributes to transparency, ensures that inherent dataset limitations are known in advance, and helps in selecting the right datasets for intended use cases.",Gipiškis2024,3.2 Data Governance
A1022_Uuk2024,Data curation,Careful data curation prior to all development stages (including fine-tuning) to filter out high-risk content and ensure the training data is sufficiently high-quality.,Uuk2024,3.2 Data Governance
A0042_Eisenberg2025,Establish AI system access controls,"Implement comprehensive access management including role-based access control (RBAC), authentication mechanisms, and audit logging for AI models and associated resources.",Eisenberg2025,3.3 Access Management
A0065_Eisenberg2025,Implement AI system usage verification program,"Deploy comprehensive measures to verify user identity, document intended use cases, and ensure AI system usage complies with instructions. This includes KYC procedures for user verification, clear documentation of permitted uses, and user acknowledgment of instructions.",Eisenberg2025,3.3 Access Management
A0114_Future of Life Institute2024,Fine-tuning protections,Fine-tuning restrictions that ensure the integrity of safety mitigations. ,Future of Life Institute2024,3.3 Access Management
A0124_Future of Life Institute2024,KYC-Based Model Access Threshold,"Has your firm set a risk or capability threshold beyond which model access should require 'know-your-customer' screenings to prevent harm to the public? If yes, please elaborate on the threshold.",Future of Life Institute2024,3.3 Access Management
A0127_Future of Life Institute2024,Model Weight Release Threshold,"Has your firm set a risk or capabilities threshold beyond which a model's weights should not be made freely available to prevent harm to the public? If yes, please elaborate on the threshold.",Future of Life Institute2024,3.3 Access Management
A0172_Schuett2023,API access to powerful models,AGI labs should strongly consider only deploying powerful models via an application programming interface (API).,Schuett2023,3.3 Access Management
A0187_Schuett2023,KYC screening,AGI labs should conduct know-your-customer (KYC) screenings before giving people the ability to use powerful models.*,Schuett2023,3.3 Access Management
A0207_Schuett2023,Safety Restrictions,"AGI labs should establish appropriate safety restrictions for powerful models after deployment (e.g. restrictions on who can use the model, how they can use the model, and whether the model can access the internet).",Schuett2023,3.3 Access Management
A0399_NIST2024,Acceptable Use Policy,Establish transparent acceptable use policies for GAI that address illegal use or applications of GAI.,NIST2024,3.3 Access Management
A0415_NIST2024,Interface Use Policy,"Define acceptable use policies for GAI interfaces, modalities, and human-AI configurations (i.e., for chatbots and decision-making tasks), including criteria for the kinds of queries GAI applications should refuse to respond to.",NIST2024,3.3 Access Management
A0421_NIST2024,Terms of Service Establishment,Establish terms of use and terms of service for GAI systems.,NIST2024,3.3 Access Management
A0433_NIST2024,Third-Party Entity Inventory,Inventory all third-party entities with access to organizational content and establish approved GAI technology and service provider lists.,NIST2024,3.3 Access Management
A0436_NIST2024,Third-Party Use Policy Update,"Update GAI acceptable use policies to address proprietary and open-source GAI technologies and data, and contractors, consultants, and other third-party personnel.",NIST2024,3.3 Access Management
A0447_NIST2024,	Illegal Use Identification,Identify and document foreseeable illegal uses or applications of the GAI system that surpass organizational risk tolerances.,NIST2024,3.3 Access Management
A0899_UK Government2023,User-based API Access Restrictions,"When deploying models through APIs, consider reducing access to users who display suspicious usage patterns. For instance, if a content filter blocks a user’s requests several times in short succession, this is evidence that they are attempting to misuse the model or find ways to circumvent its filters. Appropriate measures may include warnings, further investigation, rate limitations, more restrictive filters, and bans. Care should be taken to avoid restricting genuine use, for example to legitimate AI safety researchers attempting to examine model behaviour, or to users struggling to access legitimate sources of help on difficult topics like self-harm.",UK Government2023,3.3 Access Management
A0901_UK Government2023,Know Your Customer (KYC) Checks ,"Implement tiered Know Your Customer (KYC) checks for API users. KYC checks can help prevent users from simply creating new accounts when their access is reduced. More intense checks, such as identify verification, could be implemented where the risk is higher, such as for models with more dangerous capabilities, access to models with fewer safeguards, or high- volume usage of the model. It is important to weigh up KYC checks against potential privacy and access tradeoffs of requiring registrations.",UK Government2023,3.3 Access Management
A0902_UK Government2023,Trusted Categories API access ,"Consider restricting certain API access tiers only to users in “trusted” categories. For example, it may be appropriate to offer high usage rates, fine-tuning access, permissive content filters, as well as access to models with high misuse potential only to verified users in established enterprises, non-profits, and universities.",UK Government2023,3.3 Access Management
A0970_Gipiškis2024,Limit deployment scope,"AI models can be restricted in terms of its use cases, where providers can require the deployers to limit its deployment to a predefined scope [81], such that models built for specific purposes and tested under specific environments are not used in environments or for purposes that are potentially unsafe.",Gipiškis2024,3.3 Access Management
A0971_Gipiškis2024,Restricted usage terms for open-source models,"Developers of open-weights and open-source AI models can vet and restrict the users of their AI systems by requiring them to sign a Terms of Service agreement before getting access to the model weights. Such agreements can include limitations to the usage, modification, and proliferation of the AI model [88].
Such agreements have the advantage that users only need to be vetted once before getting model access, but are often limited in practice in preventing unauthorised use or distribution.",Gipiškis2024,3.3 Access Management
A0986_Gipiškis2024,Structured access to a model,"Structured access refers to methods which limit users’ or deployers’ direct access to a model’s parameters by constraining access to a model through a centralized access point (e.g., an API) [88]. This access point can be monitored for usage, and access can be revoked to users or downstream deployers in cases of misuse [105].
Within this centralized access point, automated filtering-based monitoring can be done on both inputs and outputs to ensure the model’s intended use is pre- served [36]. This filtering can sometimes be supplemented by human oversight, depending on desired robustness levels.",Gipiškis2024,3.3 Access Management
A1025_Uuk2024,Fine-tuning restrictions,Restricting or closely monitoring fine-tuning access to models to ensure safeguards remain intact.,Uuk2024,3.3 Access Management
A1029_Uuk2024,KYC screenings,Know-your-customer (KYC) screenings before granting access to models with very high misuse potential or to users producing large amounts of output.,Uuk2024,3.3 Access Management
A1033_Uuk2024,Prohibiting high-stakes applications,Enforcing use policies that prohibit high-stakes applications. Requires Know-Your-Customer procedures.,Uuk2024,3.3 Access Management
A0024_Bengio2025,Responsible Release and Deployment Strategies,"There is a spectrum of release and deployment strategies for AI including staged releases, cloud- based or API access, deployment safety controls, and acceptable use policies.

There are some emerging industry practices that focus on release and deployment strategies for general- purpose AI. ",Bengio2025,3.4 Staged Deployment
A0211_Schuett2023,Staged deployment,"AGI labs should deploy powerful models in stages. They should start with a small number of applications and fewer users, gradually scaling up as confidence in the model’s safety increases.",Schuett2023,3.4 Staged Deployment
A0216_Schuett2023,Treat updates similarly to new models,"AGI labs should treat significant updates to a deployed model (e.g. additional fine-tuning) similarly to its initial development and deployment. In particular, they should repeat the pre-deployment risk assessment.",Schuett2023,3.4 Staged Deployment
A0219_Schuett2023,Internal deployments = external deployments,"AGI labs should treat internal deployments (e.g. using models for writing code) similarly to external deployments. In particular, they should perform a pre-deployment risk assessment.",Schuett2023,3.4 Staged Deployment
A0779_UK Government2023,Reversible and Small-Scale Deployment ,Deploy models in small-scale or reversible ways before deploying models in large-scale or irreversible ways. This makes it possible for frontier AI organisations to notice and mitigate harm before the harm becomes too large or unavoidable.,UK Government2023,3.4 Staged Deployment
A0968_Gipiškis2024,Staged release of model weights,"When a model is developed by a provider for use in a certain AI system, it may also be useful to release the model itself more widely. Such developers can follow a staged release approach, in which they first grant access to the model via an API to trusted partners or the public, in order to scope the models’ capabilities and detect harmful or dangerous features [195].
After a period of an initial closed release and potentially further safety-training, the developers of the AI model can then release the weights, if they are confident that the AI model poses minimal systemic risk.",Gipiškis2024,3.4 Staged Deployment
A0969_Gipiškis2024,Gradual or incremental monitored release of model access,"AI systems can be released for access incrementally, starting with a small and selected deployer base before progressively being released to a wider user base. Initially, usage to a hosted API can be restricted with access given to specific deployers only, where all instances of the system can be easily updated or de- commissioned with minimal disruption should there be any problems identified. Gradual releases provide more time to monitor for vulnerabilities and other problems. Even when such vulnerabilities are detected, the resulting harms may be more limited compared to a scenario in which a more capable version is released with the same vulnerabilities [195].",Gipiškis2024,3.4 Staged Deployment
A1023_Uuk2024,Deploying powerful models in stages,"Starting with a small number of applications and fewer users, and gradually scaling up API-access as rigorous monitoring increases confidence in the model's safety. An API-mediated staged release would also be required before open-sourcing a model.",Uuk2024,3.4 Staged Deployment
A0037_Casper2025,Post-deployment monitoring reports,Developers can be required to establish procedures for monitoring and periodically reporting on the uses and impacts of their frontier systems.,Casper2025,3.5 Post-deployment Monitoring
A0051_Eisenberg2025,Implement AI system monitoring and logging infrastructure,"Deploy comprehensive monitoring and logging systems that capture AI system behavior, decisions, performance metrics, and real-time data source usage at multiple levels of granularity for full system observability, including tracking of data lineage during inference.",Eisenberg2025,3.5 Post-deployment Monitoring
A0054_Eisenberg2025,Implement performance monitoring and robustness system,"Implement continuous monitoring and testing mechanisms to evaluate AI system robustness, generalization capabilities, and performance stability across varying conditions and environments while in production.",Eisenberg2025,3.5 Post-deployment Monitoring
A0056_Eisenberg2025,Implement fairness monitoring and remediation system,"Deploy continuous monitoring systems to detect fairness issues in production, including automated drift detection, performance disparity alerts, and systematic remediation procedures.",Eisenberg2025,3.5 Post-deployment Monitoring
A0061_Eisenberg2025,Establish information quality assurance framework,"Implement comprehensive mechanisms to assess, verify, and improve the factual accuracy of AI system outputs, including source validation, fact-checking procedures, and uncertainty communication protocols.",Eisenberg2025,3.5 Post-deployment Monitoring
A0064_Eisenberg2025,Implement system usage monitoring and prevention,"Monitor and prevent malicious or otherwise disallowed behavioral patterns including automated abuse, coordination across accounts, and systematic manipulation attempts.",Eisenberg2025,3.5 Post-deployment Monitoring
A0128_Future of Life Institute2024,Monitor user interactions and remove access,Does your organization monitor user interactions with its most capable AI systems to ban accounts that use the system for harmful or illegal purposes?,Future of Life Institute2024,3.5 Post-deployment Monitoring
A0160_Future of Life Institute2024,User Interaction Monitoring & Response Restrictions,Does your organization monitor user interactions with its most capable AI models and restrict answers to specific prompts to avoid model interactions that support harmful or criminal activities?,Future of Life Institute2024,3.5 Post-deployment Monitoring
A0190_Schuett2023,Monitor systems and their uses,"AGI labs should closely monitor deployed systems, including how they are used and what impact they have on society.",Schuett2023,3.5 Post-deployment Monitoring
A0191_Schuett2023,Monitor systems and their uses,"AGI labs should closely monitor deployed systems, including how they are used and what impact they have on society.",Schuett2023,3.5 Post-deployment Monitoring
A0196_Schuett2023,Post-deployment evaluations,"AGI labs should continually evaluate models for dangerous capabilities after deployment, taking into account new information about the model’s capabilities and how it is being used.*",Schuett2023,3.5 Post-deployment Monitoring
A0284_Campos2025,Continuous Monitoring of Risk Controls,"Developers must therefore implement continuous monitoring of both KRIs and KCIs to ensure that KCI thresholds are met once KRI thresholds are crossed according to the predefined ""if-then"" statements established in the risk analysis and evaluation phase.",Campos2025,3.5 Post-deployment Monitoring
A0400_NIST2024,Content Provenance Review,Define organizational responsibilities for periodic review of content provenance and incident monitoring for GAI systems.,NIST2024,3.5 Post-deployment Monitoring
A0429_NIST2024,Provenance Success Metrics,"Develop and validate approaches for measuring the success of content provenance management efforts with third parties (e.g., incidents detected and response times).",NIST2024,3.5 Post-deployment Monitoring
A0440_NIST2024,Third-Party Monitoring Policy,Establish policies and procedures for continuous monitoring of third-party GAI systems in deployment.,NIST2024,3.5 Post-deployment Monitoring
A0453_NIST2024,External Network Analysis,"Observe and analyze how the GAI system interacts with external networks, and identify any potential for negative externalities, particularly where content provenance might be compromised.",NIST2024,3.5 Post-deployment Monitoring
A0463_NIST2024,	Configuration Outcome Monitoring,Implement systems to continually monitor and track the outcomes of human-GAI configurations for future refinement and improvements.,NIST2024,3.5 Post-deployment Monitoring
A0465_NIST2024,	Privacy Risk Monitoring,Conduct periodic monitoring of AI-generated content for privacy risks; address any possible instances of PII or sensitive data exposure.,NIST2024,3.5 Post-deployment Monitoring
A0472_NIST2024,Domain Adaptation Risk Assessment,"Re-evaluate risks when adapting GAI models to new domains. Additionally, establish warning systems to determine if a GAI system is being used in a new domain where previous assumptions (relating to context of use or mapped risks such as security, and safety) may no longer hold.",NIST2024,3.5 Post-deployment Monitoring
A0481_NIST2024,Context-Based Impact Measures,"Determine context-based measures to identify if new impacts are present due to the GAI system, including regular engagements with downstream AI Actors to identify and quantify new contexts of unanticipated impacts of GAI systems.",NIST2024,3.5 Post-deployment Monitoring
A0482_NIST2024,Input Actor Engagement,"Plan regular engagements with AI Actors responsible for inputs to GAI systems, including third-party data and algorithms, to review and evaluate unanticipated impacts.",NIST2024,3.5 Post-deployment Monitoring
A0488_NIST2024,Equitable Output Monitoring,Implement continuous monitoring of GAI system impacts to identify whether GAI outputs are equitable across various sub-populations. Seek active and direct feedback from affected communities via structured feedback mechanisms or red-teaming to monitor and improve outputs.,NIST2024,3.5 Post-deployment Monitoring
A0505_NIST2024,Source and Citation Review,Review and verify sources and citations in GAI system outputs during pre-deployment risk measurement and ongoing monitoring activities,NIST2024,3.5 Post-deployment Monitoring
A0506_NIST2024,Anthropomorphization Tracking,"Track and document instances of anthropomorphization (e.g., human images, mentions of human feelings, cyborg imagery or motifs) in GAI system interfaces.",NIST2024,3.5 Post-deployment Monitoring
A0508_NIST2024,Security Guardrail Review,"Regularly review security and safety guardrails, especially if the GAI system is being operated in novel circumstances. This includes reviewing reasons why the GAI system was initially assessed as being safe to deploy.",NIST2024,3.5 Post-deployment Monitoring
A0524_NIST2024,Security Measure Effectiveness,Regularly assess and verify that security measures remain effective and have not been compromised.,NIST2024,3.5 Post-deployment Monitoring
A0525_NIST2024,Policy Violation Statistics,"Compile statistics on actual policy violations, take-down requests, and intellectual property infringement for organizational GAI systems: Analyze transparency reports across demographic groups, languages groups.",NIST2024,3.5 Post-deployment Monitoring
A0629_Barrett2024,Risk Tracking,"Track important identified risks (e.g., vulnerabilities from data poisoning and other
attacks or objectives mis-specification) even if they cannot yet be measured 
",Barrett2024,3.5 Post-deployment Monitoring
A0676_Barrett2024,Monitor System in Production,"The functionality
and behavior of the
AI system and its
components – as
identified in the Map
function – are
monitored when in
production.",Barrett2024,3.5 Post-deployment Monitoring
A0690_Barrett2024,Document Performance Shifts from Field Data  ,"Measurable
performance
improvements or
declines based on
consultations with
relevant AI actors,
including affected
communities, and
field data about
context-relevant risks
and trustworthiness
characteristics are
identified
and document",Barrett2024,3.5 Post-deployment Monitoring
A0700_Barrett2024,Monitor Pre-trained Models  ,"Pre-trained models
which are used for
development
are monitored as part
of AI system regular
monitoring and
maintenance.",Barrett2024,3.5 Post-deployment Monitoring
A0701_Barrett2024,Implement Post-deployment Monitoring Plan  ,"Post-deployment AI
system monitoring
plans are implemented,
including mechanisms
for capturing and
evaluating input
from users and other
relevant AI actors,
appeal and override,
decommissioning,
incident response,
recovery, and change
management.",Barrett2024,3.5 Post-deployment Monitoring
A0702_Barrett2024,Integrate Continuous Improvement  ,"Measurable activities
for continual
improvements
are integrated into
AI system updates
and include regular
engagement with
interested parties,
including relevant AI
actors.",Barrett2024,3.5 Post-deployment Monitoring
A0724_NIST2024,Information Integrity,"Monitor and document instances where human operators or other systems
override the GAI's decisions. Evaluate these cases to understand if the overrides
are linked to issues related to content provenance. (AI Deployment, Domain Experts, End-Users, Operation and Monitoring, TEVV). ",NIST2024,3.5 Post-deployment Monitoring
A0730_NIST2024,Information Integrity,"Evaluate feedback loops between GAI system content provenance and human
reviewers, and update where needed. Implement real-time monitoring systems
to affirm that content provenance protocols remain effective. (AI Deployment, AI Impact Assessment, Governance and Oversight, Operation and Monitoring)",NIST2024,3.5 Post-deployment Monitoring
A0734_NIST2024,Information Integrity,"Use real-time auditing tools where they can be demonstrated to aid in the
tracking and validation of the lineage and authenticity of AI-generated data. (AI Deployment, AI Impact Assessment, Governance and Oversight, Operation and Monitoring).",NIST2024,3.5 Post-deployment Monitoring
A0747_NIST2024,Information Integrity,"Implement real-time monitoring processes for analyzing generated content
performance and trustworthiness characteristics related to content provenance
to identify deviations from the desired standards and trigger alerts for human
intervention.",NIST2024,3.5 Post-deployment Monitoring
A0749_NIST2024,Human-AI Configuration,"Use human moderation systems where appropriate to review generated content
in accordance with human-AI configuration policies established in the Govern
function, aligned with socio-cultural norms in the context of use, and for settings
where AI models are demonstrated to perform poorly.",NIST2024,3.5 Post-deployment Monitoring
A0752_NIST2024,CBRN Information or Capabilities; Confabulation; Information Security,"Establish, maintain, and evaluate effectiveness of organizational processes and
procedures for post-deployment monitoring of GAI systems, particularly for
potential confabulation, CBRN, or cyber risks.",NIST2024,3.5 Post-deployment Monitoring
A0754_NIST2024,Confabulation,"Implement active learning techniques to identify instances where the model fails
or produces unexpected outputs. ",NIST2024,3.5 Post-deployment Monitoring
A0758_NIST2024,Harmful Bias and Homogenization,"Conduct regular monitoring of GAI systems and publish reports detailing the
performance, feedback received, and improvements made. ",NIST2024,3.5 Post-deployment Monitoring
A0831_UK Government2023,Recurrent Performance analysis of AI models ,"Measure the performance of AI models and systems on an ongoing basis. A decline in model performance could be an indication of an attack or could indicate that a model is encountering data that is different from that which it was trained on. Either way, further investigation may be required.",UK Government2023,3.5 Post-deployment Monitoring
A0832_UK Government2023,Monitoring Inputs to AI Systems for Audit Development,"Monitor and log inputs to AI systems to enable audit, investigation and remediation in the case of compromise. Some attacks against AI systems rely on repeated querying. Proper logging will help you audit your system and identify any anomalous inputs.",UK Government2023,3.5 Post-deployment Monitoring
A0889_UK Government2023,Misuse Mapping,"Understand how their own models – as well as how models released by other groups – are being misused. Knowing that some people have begun using a competitor’s model to conduct phishing attacks, for example, may lead a frontier AI organisation to become more concerned that its own model will be misused in this way.",UK Government2023,3.5 Post-deployment Monitoring
A0909_UK Government2023,Diverse Monitoring Techniques,"Employ diverse monitoring techniques to balance comprehensiveness, scalability, and privacy. A strong monitoring setup will generally combine automated and human review, in recognition that automated reviews may miss intricate issues, while human reviews are not always scalable and can pose privacy issues.",UK Government2023,3.5 Post-deployment Monitoring
A0950_Gipiškis2024,Dashboard of model properties,"A dashboard [41] displays all the relevant information about the model’s internal state and the model’s physical properties to the user. It is used to ensure that the user is informed about factors that influence the behavior of the model and to ensure that the user maintains control over the model. Allowing only the user to access the dashboard can aid in information asymmetry between the user and model, thus supporting the user oversight over the model.
Examples of the model’s internal state include its representations of the world, representation of users, and the strategies it is currently pursuing; while examples of the model’s physical properties include the model’s compute and energy consumption, physical storage occupied, and physical networks it is connected to.",Gipiškis2024,3.5 Post-deployment Monitoring
A0977_Gipiškis2024,Monitoring of model capabilities,"AI models are often trained to develop specific capabilities by using appropriate training data and training goals. However, models may develop capabilities that they were not specifically trained for. One subset of this is emergent capabilities, i.e., capabilities that emerged in larger models but not smaller models given a similar training process [215].
These capabilities can be monitored, allowing models to be tested not only for their intended capabilities but also for capabilities that are not intended.",Gipiškis2024,3.5 Post-deployment Monitoring
A0979_Gipiškis2024, Post-deployment ongoing monitoring for unexpected use cases,"Ongoing monitoring of AI systems can uncover emergent or newly identified capabilities and limitations, in particular when new use cases are found, or in a large-scale deployment with a diverse population of users. These new capa- bilities or limitations can inform ongoing risk analysis. New use-cases can be discovered via monitoring publications, online forums, or APIs [131].
For example, a LLM might have unexpectedly high competence at giving con- vincing medical advice, despite not being directly developed for that purpose, nor verified for accuracy. In that case, the potential risks of this newly found competence can be assessed.",Gipiškis2024,3.5 Post-deployment Monitoring
A0980_Gipiškis2024,Monitoring protocols for po- tentially adversarial GPAI systems,"Similar to cybersecurity protocols, AI providers can prepare in advance for po- tential adversarial action from AI systems by developing and stress-testing protocols for potential adversarial action [179]. Such adversarial action may range from hiding code exploits, to strategic deception or loss of control related be- haviors.
AI systems may be adversarial due to malicious actors (e.g., data poisoning), or due to unintended issues of control (lack of alignment with human intent). Alongside improving the system alignment, external monitoring and restrictions may be deployed in advance to “catch” these adversarial model malfunctions before harm is done.
Like with cybersecurity red teaming, any protocol which assumes adversarial action can be stress tested in advance. For example, this can be done by pro- jecting the plausible capabilities and affordances of the adversarial actor (e.g., in the case of a tech company - a hacker, or a disgruntled employee). From this process, measures that better anticipate possible adversarial action can be deployed, and catch unsuccessful attempts at adversarial behavior.
If such unsuccessful attempts are caught, they can be reported in incident re- ports, and serve as an input to further risk analysis. Depending on their severity, it may be advisable to recall the system if deployed. Examples of these protocols include [84]:
• Flagging of suspicious frontier AI system code generation for human over- sight, with the help of better proven (“trusted”) AI systems or automated keyword detection.
• Using a better proven (“trusted”) AI system to edit the code produced by a frontier AI system if suspicious.• Monitoring of AI system with another instance of this same system, with filters to prevent collusion.
• Restriction of access of the AI system to private information.

",Gipiškis2024,3.5 Post-deployment Monitoring
A1005_Gipiškis2024,Tracking benchmark leakage,"Constant monitoring and documentation of benchmark leakage can help with the early detection of benchmark leaks [224, 95].",Gipiškis2024,3.5 Post-deployment Monitoring
A0039_Casper2025,Shutdown procedures,Developers can be required to document if and which protocols exist to shut down frontier systems that are under their control.,Casper2025,3.6 Incident Response & Recovery
A0110_Future of Life Institute2024,Emergency Rollback Plan,"Critically dangerous capabilities or very severe yet unexpected misuse patterns might only surface after a system has been deployed. Has your firm developed an emergency response plan to react to scenarios where such problems cannot be resolved quickly via updates? Please select all interventions that your organization has implemented.

Made legal and technical preparations to roll back a system rapidly
Formally specified the risk threshold that would trigger a rapid rollback
Committed to regular safety drills to test emergency response",Future of Life Institute2024,3.6 Incident Response & Recovery
A0180_Schuett2023,Emergency response plan,"AGI labs should have and practice implementing an emergency response plan. This might include switching off systems, overriding their outputs, or restricting access.",Schuett2023,3.6 Incident Response & Recovery
A0189_Schuett2023,Model containment,AGI labs should contain models with sufficiently dangerous capabilities (e.g. via boxing or air-gapping).,Schuett2023,3.6 Incident Response & Recovery
A0209_Schuett2023,Security incident response plan,AGI labs should have a plan for how they respond to security incidents (e.g. cyberattacks).*,Schuett2023,3.6 Incident Response & Recovery
A0251_Wiener2024,Capability to promptly enact a full shutdown,"When enacting a full shutdown, the developer shall take
into account, as appropriate, the risk that a shutdown of the covered
model, or particular covered model derivatives, could cause
disruptions to critical infrastructure. ",Wiener2024,3.6 Incident Response & Recovery
A0401_NIST2024,Incident Response Process,"Establish organizational policies and procedures for after action reviews of GAI system incident response and incident disclosures, to identify gaps; Update incident response and incident disclosure processes as required.",NIST2024,3.6 Incident Response & Recovery
A0406_NIST2024,Deactivation Protocol,Protocols are put in place to ensure GAI systems are able to be deactivated when necessary.,NIST2024,3.6 Incident Response & Recovery
A0409_NIST2024,Incident Response Teams,Establish procedures to engage teams for GAI system incident response with diverse composition and responsibilities based on the particular incident type.,NIST2024,3.6 Incident Response & Recovery
A0410_NIST2024,Response Team Verification,Establish processes to verify the AI Actors conducting GAI incident response tasks demonstrate and maintain the appropriate skills and training.,NIST2024,3.6 Incident Response & Recovery
A0439_NIST2024,Third-Party Response Plans,"Establish incident response plans for third-party GAI technologies: Align incident response plans with impacts enumerated in MAP 5.1; Communicate third-party GAI incident response plans to all relevant AI Actors; Define ownership of GAI incident response functions; Rehearse third-party GAI incident response plans at a regular cadence; Improve incident response plans based on retrospective learning; Review incident response plans for alignment with relevant breach reporting, data protection, data privacy, or other laws.",NIST2024,3.6 Incident Response & Recovery
A0442_NIST2024,Rollover and Fallback Management,"Establish policies and procedures to test and manage risks related to rollover and fallback technologies for GAI systems, acknowledging that rollover and fallback may include manual processing.",NIST2024,3.6 Incident Response & Recovery
A0466_NIST2024,IP Infringement Response,Implement processes for responding to potential intellectual property infringement claims or other rights.,NIST2024,3.6 Incident Response & Recovery
A0521_NIST2024,Security Implementation Rate,Measure the rate at which recommendations from security checks and incidents are implemented. Assess how quickly the AI system can adapt and improve based on lessons learned from security incidents and feedback,NIST2024,3.6 Incident Response & Recovery
A0528_NIST2024,User Instruction Adequacy,Verify adequacy of GAI system user instructions through user testing.,NIST2024,3.6 Incident Response & Recovery
A0639_Barrett2024,Decommission AI Systems Safely,"Processes and
procedures are in place
for decommissioning
and phasing out AI
systems safely and in a
manner
that does not increase
risks or decrease the
organization’s
trustworthiness.",Barrett2024,3.6 Incident Response & Recovery
A0651_Barrett2024,Contingency Planning for Third-Party Failures,"Contingency processes
are in place to handle
failures or incidents in
third-party data or AI
systems deemed to be
high-risk.",Barrett2024,3.6 Incident Response & Recovery
A0697_Barrett2024,Respond to Unknown Risks  ,"Procedures are
followed to respond
to and recover from a
previously unknown risk
when it is identified.",Barrett2024,3.6 Incident Response & Recovery
A0698_Barrett2024,Deactivate Misaligned Systems  ,"Mechanisms are in
place and applied,
and responsibilities
are assigned and
understood, to
supersede, disengage,
or deactivate AI systems
that demonstrate
performance or
outcomes inconsistent
with intended use.",Barrett2024,3.6 Incident Response & Recovery
A0737_NIST2024,Value Chain and Component Integration,"Develop and update GAI system incident response and recovery plans and
procedures to address the following: Review and maintenance of policies and
procedures to account for newly encountered uses; Review and maintenance of
policies and procedures for detection of unanticipated uses; Verify response
and recovery plans account for the GAI system value chain; Verify response and
recovery plans are updated for and include necessary details to communicate
with downstream GAI system Actors: Points-of-Contact (POC), Contact
information, notification format. (AI Deployment, Operation and Monitoring)",NIST2024,3.6 Incident Response & Recovery
A0739_NIST2024,Information Security,"Establish and maintain procedures for escalating GAI system incidents to the
organizational risk management authority when specific criteria for deactivation
or disengagement is met for a particular context of use or for the GAI system as a
whole.",NIST2024,3.6 Incident Response & Recovery
A0740_NIST2024,Information Security,"Establish and maintain procedures for the remediation of issues which trigger
incident response processes for the use of a GAI system, and provide stakeholders
timelines associated with the remediation plan",NIST2024,3.6 Incident Response & Recovery
A0759_NIST2024,"Human-AI Configuration; Dangerous, Violent, or Hateful Content","Practice and follow incident response plans for addressing the generation of
inappropriate or harmful content and adapt processes based on findings to
prevent future occurrences. Conduct post-mortem analyses of incidents with
relevant AI Actors, to understand the root causes and implement preventive
measures.",NIST2024,3.6 Incident Response & Recovery
A0830_UK Government2023,"Incident Response, Escalation, and Remeditation Plans",Develop an organisational incident response plan. The inevitability of security incidents affecting systems is reflected in organisational incident response planning. A well-planned response will help minimise the damage caused by an attack and support recovery.,UK Government2023,3.6 Incident Response & Recovery
A0833_UK Government2023,Mitigation Actions for AI-related security incidents,"
Take action to mitigate and remediate issues and document any AI-related security incidents and vulnerabilities.
",UK Government2023,3.6 Incident Response & Recovery
A0883_UK Government2023,Continuous Updating of Defensive Tools,"Continuously update defensive tools as workarounds are discovered. In some cases, this may be an ongoing effort that requires sustained investment.",UK Government2023,3.6 Incident Response & Recovery
A0904_UK Government2023,Worst-case scenario Response,"Implement processes and technical requirements to enable rapid rollback or withdrawal of models in case of egregious, widespread or consistent harms. Rolling back to a previous version of a model that does not suffer from the same misuse threats, or withdrawing access to a model altogether, may be proportionate actions in such situations of extreme threat or consistent misuse. Running periodic dry runs could increase preparedness for such situations.",UK Government2023,3.6 Incident Response & Recovery
A0941_Gipiškis2024,Internal data poisoning diagnosis,"Providers can have an internal framework to identify what specific data poison- ing attack their model may be a victim of based on a set of symptoms, such as analysis of target algorithm and architecture, perturbation scope and dimen- sion, victim model, and data type [39]. This framework includes known defenses against the diagnosed attack, which providers can then apply to the model.",Gipiškis2024,3.6 Incident Response & Recovery
A0988_Gipiškis2024,Redundant systems not reliant on GPAI,"Redundant systems provide continuation of a given system’s processes in case of failure of the given system. Importantly, redundant systems should not rely on the factors that caused the original system to fail in the first place, which can include an AI system [47].
For example, if an AI system is incorporated into the landing gear system of an aircraft, such as during autonomous control of the aircraft, redundant systems in the form of mechanical or hydraulic mechanisms must be present to allow for deployment of the landing gear in case of AI system failure.",Gipiškis2024,3.6 Incident Response & Recovery
A1035_Uuk2024,Safety Drills,"Regularly practicing the implementation of an emergency response plan to stress test the organization's ability to respond to reasonably foreseeable, fast-moving emergency scenarios.",Uuk2024,3.6 Incident Response & Recovery
A0630_Barrett2024,Apply Lifecycle Risk Controls,"Implement risk-reduction controls as appropriate throughout a GPAIS lifecycle, e.g.,
independent auditing, incremental scale-up, red-teaming, structured access or staged
release, and other steps
",Barrett2024,3.X Operational Process Control not otherwise categorized
A0696_Barrett2024,Sustain Deployed System Value  ,"Mechanisms are in
place and applied to
sustain the value of
deployed AI systems.",Barrett2024,3.X Operational Process Control not otherwise categorized
A0005_Bengio2025,Documentation,"There are numerous documentation best practices, guidelines, and requirements for AI systems to track e.g. training data, model design and functionality, intended use cases, limitations, and risks.

Model cards’ and ‘system cards’ are examples of prominent AI documentation standards",Bengio2025,4.1 System Documentation
A0027_Casper2025,Document compute usage,"Given that computing power is key to frontier AI development (Sastry et al., 2024), frontier developers can be required to document their compute resources including details such as the usage, providers, and the location of compute clusters.",Casper2025,4.1 System Documentation
A0028_Casper2025,Documentation availability,All of the above documentation can be made available to the public (redacted) and AI governing authorities (unredacted).,Casper2025,4.1 System Documentation
A0035_Casper2025,Model specification and basic info,"Developers can be required to document intended use cases and behaviors (e.g., OpenAI, 2024) and basic information about frontier systems such as scale.",Casper2025,4.1 System Documentation
A0050_Eisenberg2025,Establish AI system documentation framework,"Implement comprehensive documentation requirements and processes covering training data provenance, system architecture, model cards, and component interactions to ensure transparent documentation of both the data lifecycle and system design.",Eisenberg2025,4.1 System Documentation
A0081_Eisenberg2025,Establish documentation sharing mechanism,"Implement a process to share information and documentation to third-parties, including to regulators and downstream deployers or developers.",Eisenberg2025,4.1 System Documentation
A0146_Future of Life Institute2024,Publication of Alignment Research,"Does your organization publish alignment research? If yes, please provide a URL to a website that showcases relevant publications.",Future of Life Institute2024,4.1 System Documentation
A0256_Wiener2024,Publish results of safety tests,,Wiener2024,4.1 System Documentation
A0264_EU AI Office2025,Documentation,"Signatories commit to documenting information relevant to the assessment and mitigation of sytemic risks from their GPAISRs, including keeping up-to-date model documentation and providing relevant information to providers of AI systems who intend to integrate the general-purpose AI model into their AI systems",EU AI Office2025,4.1 System Documentation
A0388_NIST2024,Legal Compliance Alignment,"Align GAI development and use with applicable laws and regulations, including those related to data privacy, copyright and intellectual property law.",NIST2024,4.1 System Documentation
A0402_NIST2024,Documentation Retention Policy,"Maintain a document retention policy to keep history for test, evaluation, validation, and verification (TEVV), and digital content transparency methods for GAI.",NIST2024,4.1 System Documentation
A0405_NIST2024,Comprehensive Inventory Details,"In addition to general model, governance, and risk information, consider the following items in GAI system inventory entries: Data provenance information (e.g., source, signatures, versioning, watermarks); Known issues reported from internal bug tracking or external information sharing resources (e.g., AI incident database, AVID, CVE, NVD, or OECD AI incident monitor); Human oversight roles and responsibilities; Special rights and considerations for intellectual property, licensed works, or personal, privileged, proprietary or sensitive data; Underlying foundation models, versions of underlying models, and access modes.",NIST2024,4.1 System Documentation
A0423_NIST2024,Downstream Impact Verification,Verify that downstream GAI system impacts (such as the use of third-party plugins) are included in the impact documentation process.,NIST2024,4.1 System Documentation
A0434_NIST2024,Third-Party Change Records,"Maintain records of changes to content made by third parties to promote content provenance, including sources, timestamps, metadata.",NIST2024,4.1 System Documentation
A0437_NIST2024,Value Chain Risk Documentation,Document GAI risks associated with system value chain to identify over-reliance on third-party data and to identify fallbacks.,NIST2024,4.1 System Documentation
A0452_NIST2024,Upstream Dependency Documentation,"Identify and document how the system relies on upstream data sources, including for content provenance, and if it serves as an upstream dependency for other systems.",NIST2024,4.1 System Documentation
A0504_NIST2024,Human Domain Knowledge Documentation,"Document the extent to which human domain knowledge is employed to improve GAI system performance, via, e.g., RLHF, fine-tuning, retrieval-augmented generation, content moderation, business rules.",NIST2024,4.1 System Documentation
A0530_NIST2024,GAI Model Documentation,"Document GAI model details including: Proposed use and organizational value; Assumptions and limitations, Data collection methodologies; Data provenance; Data quality; Model architecture (e.g., convolutional neural network, transformers, etc.); Optimization objectives; Training algorithms; RLHF approaches; Fine-tuning or retrieval-augmented generation approaches; Evaluation data; Ethical considerations; Legal and regulatory requirements.",NIST2024,4.1 System Documentation
A0631_Barrett2024,Disclose Risks to Stakeholders,"Incorporate identified AI system risk factors, and circumstances that could result in
impacts or harms, into reporting and engagement with internal and external stake-
holders (e.g., to downstream developers, regulators, users, impacted communities, etc.) on
the AI system as appropriate, e.g., using model cards, system cards, and other transparency
mechanisms
",Barrett2024,4.1 System Documentation
A0652_Barrett2024, Document Intended Use & Context,"Intended purposes,
potentially beneficial uses,
context-specific laws,
norms and expectations,
and prospective settings
in which the AI system
will be deployed
are understood
and documented.",Barrett2024,4.1 System Documentation
A0654_Barrett2024,Document AI Mission & Goals,"The organization’s
mission and relevant
goals for AI technology
are understood and
documented.",Barrett2024,4.1 System Documentation
A0655_Barrett2024,Business Use,"The business value or
context of business
use has been clearly
defined or – in the case
of assessing existing AI
systems – re-evaluated.",Barrett2024,4.1 System Documentation
A0658_Barrett2024,Define Tasks & Methods,"The specific tasks
and methods used to
implement the tasks
that the AI system will
support are defined
(e.g., classifiers,
generative models,
recommenders).",Barrett2024,4.1 System Documentation
A0659_Barrett2024,Document Limits & Oversight,"Information about the AI
system’s knowledge limits
and how system output
may be utilized and
overseen by humans is
documented. Documen-
tation provides sufficient
information to assist
relevant AI actors when
making decisions and tak-
ing subsequent actions.",Barrett2024,4.1 System Documentation
A0660_Barrett2024,Document TEVV & Integrity,"Scientific integrity and
TEVV considerations
are identified and docu-
mented, including those
related to experimental
design, data collection
and selection (e.g.,
availability, represen-
tativeness, suitability),
system trustworthiness,
and construct validation",Barrett2024,4.1 System Documentation
A0661_Barrett2024,Document Intended Benefits,"Potential benefits of intend-
ed AI system functionality
and performance are ex-
amined and documented",Barrett2024,4.1 System Documentation
A0663_Barrett2024,Document Application Scope,"Targeted application
scope is specified
and documented
based on the system’s
capability, established
context, and AI system
categorization.",Barrett2024,4.1 System Documentation
A0673_Barrett2024,Document TEVV Tools & Metrics,"Test sets, metrics, and
details about the tools
used during TEVV are
documented.",Barrett2024,4.1 System Documentation
A0675_Barrett2024,Measure & Document Deployment Performance,"AI system performance
or assurance criteria
are measured
qualitatively or
quantitatively and
demonstrated
for conditions similar
to deployment
setting(s). Measures
are documented",Barrett2024,4.1 System Documentation
A0680_Barrett2024,Explain & Document Model Behavior,"The AI model is
explained, validated,
and documented, and
AI system output is
interpreted within its
context – as identified
in the Map function – to
inform responsible use
and governance.",Barrett2024,4.1 System Documentation
A0719_NIST2024,Human-AI Configuration; Information Integrity; Harmful Bias and Homogenization,"Provide input for training materials about the capabilities and limitations of GAI
systems related to digital content transparency for AI Actors, other
professionals, and the public about the societal impacts of AI and the role of
diverse and inclusive content generation (AI Deployment, Affected Individuals and Communities, End-Users, Operation and Monitoring, TEVV) ",NIST2024,4.1 System Documentation
A0729_NIST2024,Information Integrity,"Document training data sources to trace the origin and provenance of AI generated content (AI Deployment, AI Impact Assessment, Governance and Oversight, Operation and Monitoring). ",NIST2024,4.1 System Documentation
A0760_NIST2024,Human-AI Configuration,"Use visualizations or other methods to represent GAI model behavior to ease
non-technical stakeholders understanding of GAI system functionality",NIST2024,4.1 System Documentation
A0817_UK Government2023,Information Sharing with Downstream Users ,"Share specific information with downstream users of the model to enable more effective risk mitigation across the AI supply chain and build consumer confidence. This could include uses of the model that are against their terms of service, and could be provided to users through user terms or published information about the model.",UK Government2023,4.1 System Documentation
A0826_UK Government2023,Data Documentation ,"Document data, models, prompts, evaluation materials and other assets, using commonly used structures such as data cards, model cards and software bills of materials (SBOMs). This will allow you to identify and share key information, including particular security concerns, quickly and easily.",UK Government2023,4.1 System Documentation
A0926_UK Government2023,Documentation of Results of Input Data Audits ,"Document the results of input data audits, including metadata. Frontier AI organisations could look to emerging standards when documenting the results of input data audits, such as datasheets for datasets",UK Government2023,4.1 System Documentation
A0938_Gipiškis2024,Calibrated confidence measures for model pre- dictions,"Incorporating calibrated confidence measures alongside a model’s predictions and standard performance metrics, such as accuracy, can help users identify in- stances of overconfidence in incorrect predictions or underconfidence in correct ones [85]. These additional measures can provide users with more information to better interpret the model’s decisions and assess whether its predictions can be trusted.",Gipiškis2024,4.1 System Documentation
A0034_Casper2025,Model registration,"Developers can be required to register (McKernon et al., 2024) frontier systems with governing bodies (regardless of whether they will be externally deployed).",Casper2025,4.2 Risk Disclosure
A0082_Eisenberg2025,Implement a risk reporting mechanism,"Establish processes to identify and disclose known or reasonably foreseeable risks, the discovery of new risks, or instances of non-conformity to third parties.",Eisenberg2025,4.2 Risk Disclosure
A0130_Future of Life Institute2024,Notify government about large training runs,Does your organization notify the appropriate government authorities about large upcoming training runs?,Future of Life Institute2024,4.2 Risk Disclosure
A0138_Future of Life Institute2024,Pre-deployment risk disclosure,Does your organization share the results of its pre-deployment risk assessments with the appropriate government(s) before deploying a new model? Does this reporting include details on internal safety evaluations and any safety evaluations completed by independent third parties? Is the government provided with a justification for why the firm deems the system safe enough to deploy and is willing to accept the remaining risks? Please check all that apply.,Future of Life Institute2024,4.2 Risk Disclosure
A0141_Future of Life Institute2024,Pre-training risk disclosure,Does your organization share the results of model-specific pre-training risk assessments with the appropriate government authorities before launching large training runs?,Future of Life Institute2024,4.2 Risk Disclosure
A0149_Future of Life Institute2024,Safety evaluation transparency,We highlight the main sources of information about content and results of the safety evaluations.,Future of Life Institute2024,4.2 Risk Disclosure
A0193_Schuett2023,Notify a state actor before deployment,AGI labs should notify appropriate state actors before deploying powerful models.,Schuett2023,4.2 Risk Disclosure
A0194_Schuett2023,Notify affected parties,AGI labs should notify parties who will be negatively affected by a powerful model before deploying it.*,Schuett2023,4.2 Risk Disclosure
A0201_Schuett2023,Publish internal risk assessment results,"AGI labs should publish the results or summaries of internal risk assessments, unless this would unduly reveal proprietary information or itself produce significant risk. This should include a justification of why the lab is willing to accept remaining risks.*",Schuett2023,4.2 Risk Disclosure
A0220_Schuett2023,Notify other labs,AGI labs should notify other labs before deploying powerful models.*,Schuett2023,4.2 Risk Disclosure
A0221_Schuett2023,Pre-registration of large training runs,AGI labs should register upcoming training runs above a certain size with an appropriate state actor.,Schuett2023,4.2 Risk Disclosure
A0257_Wiener2024,Report new model deployments to the AG within 30 days,,Wiener2024,4.2 Risk Disclosure
A0259_Wiener2024,Publish assessment of the risk of catastrophic harms from the model,,Wiener2024,4.2 Risk Disclosure
A0269_EU AI Office2025,Public Transparency,"Signatories commit to publishing information relevant to the public understanding of systemic risks stemming from their GPAISRs, where necessary to effectively enable assessment and mitigation of systemic risks, including new or updated versions of their Frameworks and Model Reports.",EU AI Office2025,4.2 Risk Disclosure
A0273_EU AI Office2025,Safety and Security Model Reports,"Signatories should commit to reporting to the AI Office about their implementation of the Code, and especially the application of their Framework to the development, making available on the market, and/or use of their GPAISRs, by creating a Safety and Security Model Report, which will document: (1) the results of systemic risk assessment and mitigation for the model in question; and (2) justifications of decisions to make the model in question available on the market.
",EU AI Office2025,4.2 Risk Disclosure
A0280_Campos2025,Stakeholder Sharing of Risk Documentation,"The results of the risk modeling work should be well documented, including the methodologies used, the experts involved, and the list of identified scenarios. This documentation should be shared with relevant stakeholders",Campos2025,4.2 Risk Disclosure
A0411_NIST2024,National Security Involvement,"When systems may raise national security risks, involve national security professionals in mapping, measuring, and managing those risks.",NIST2024,4.2 Risk Disclosure
A0501_NIST2024,Pre-Deployment Results Sharing,"Share results of pre-deployment testing with relevant GAI Actors, such as those with system release approval authority",NIST2024,4.2 Risk Disclosure
A0526_NIST2024,Annotator Instruction Documentation,Document the instructions given to data annotators or AI red-teamers.,NIST2024,4.2 Risk Disclosure
A0632_Barrett2024,Document Mitigation Rationale,"Document the process used in considering risk mitigation controls,
the options considered, and reasons for choices. 
",Barrett2024,4.2 Risk Disclosure
A0646_Barrett2024,Document & Share AI Risk Impacts,"Organizational teams
document the risks and
potential impacts of
the AI technology they
design, develop, deploy,
evaluate, and use, and
they communicate
about the impacts more
broadly.",Barrett2024,4.2 Risk Disclosure
A0755_NIST2024,Human-AI Configuration; Harmful Bias and Homogenization,"Share transparency reports with internal and external stakeholders that detail
steps taken to update the GAI system to enhance transparency and
accountability",NIST2024,4.2 Risk Disclosure
A0782_UK Government2023,Risk Assessment Disclosure to Third Parties,"Regularly update relevant stakeholders on risk assessment and mitigation measures: This will enable assessment of whether AI organisations have sufficient risk management processes in place, build up a picture of best practices, and make recommendations to address gaps. When sharing this information with external actors, consideration should be given to commercially sensitive information. Additional ad hoc updates could be provided in cases of major developments.Include information on evaluations, risk assessment and mitigation, and individuals involved. For example: 1) What types of tests and evaluations are being run on which types of models; 2) What other risk assessment methods are being used, which kinds of expertise are
drawn on, and whether impacted stakeholders are being involved; 3) How risk mitigation measures are being monitored; 4) Which teams and individuals are involved at different stages of the risk management process (and how, if at all, third parties are involved); 5) Measures taken to address specific categories of risk, such as cybersecurity measures",UK Government2023,4.2 Risk Disclosure
A0803_UK Government2023,Discretionary Evaluator Disclosure ,"Allow evaluators to share and discuss the results of their evaluations, with potential restrictions where necessary e.g. not sharing proprietary information, information whose spread could lead to substantial harm or information that would have an adverse effect on competition in the market. Sharing the results of evaluations can allow governments, regulators, users, and other frontier AI organisations to make informed decisions.",UK Government2023,4.2 Risk Disclosure
A0805_UK Government2023,Disclosure of Model Evaluation Research and Development ,"Share the products of their model evaluation research and development, except when sharing the results might be harmful. In some cases, findings (e.g. about how to elicit dangerous capabilities) could be harmful if spread. When the expected harm is sufficiently small, the AI research community, other frontier AI organisations, and relevant government bodies could benefit from being informed of their work.",UK Government2023,4.2 Risk Disclosure
A0806_UK Government2023,Disclosure of Risk Assessment Processes ,"Share details of risk assessment processes and risk mitigation measures with relevant government authorities and other AI companies, as set out in Responsible Capability Scaling.",UK Government2023,4.2 Risk Disclosure
A0809_UK Government2023,Pre-training Disclosure of High-Level Model Details to Government Authorities,"Before training, share high-level model details with the relevant government authorities and justify why the training run does not impose unacceptable risk. This includes: 1) A high-level description of the model (including high-level information on intended use cases, intended users, training data, and model architecture); 2) Compute details (including the maximum the organisation plans to use, as well as information about its location and who provides it); 3) Description of the data that will be used to train the model; 4) Evidence from scaling small models that the full training run does not pose unacceptably high risks; 5) Descriptions of specific internal and external risk assessments and mitigation efforts,3 and an overall safety assessment justifying why and how the training run is sufficiently low-risk to execute; 6) Description of which, if any, domain experts and impacted stakeholders have been involved in the project’s design, as well as risk and impact assessment; 7) Plans for model evaluations during and after training, as well as predicted dangerous capabilities
",UK Government2023,4.2 Risk Disclosure
A0810_UK Government2023,Updates to Government Authorities During Training,"During training, update information provided to relevant government authorities with any significant changes to the model itself or its risk profile. This could include: 1) Updates on model development at each evaluation checkpoint as well as any significant updates to the development plan; 2) Results from model evaluations, including details of emergent dangerous capabilities and whether these were expected; 3) Whether and how the risk context has changed (e.g. if other AI tools have been published that the model could interact with)
",UK Government2023,4.2 Risk Disclosure
A0811_UK Government2023,Disclosure of Risk Context to Government Authorities at Deployment,"At the point of deployment, share details of the model, any risks the model might pose and what steps have been taken to mitigate those risks with relevant government authorities and the wider public. Information can be provided in full to the relevant government authority, ensuring that robust security measures are in place to protect sensitive information. Some of this information can be made available to the public by publishing a transparency report (e.g. a model card) and providing general overviews of model purpose and risk assessment evaluation results. Information that exposes model vulnerabilities or facilitates the spread of dangerous capabilities should be redacted from the transparency report, unless sharing this information publicly would be sufficiently helpful for mitigating the risks the model poses. Information that is commercially sensitive (e.g. detailed information on training data) or exposes the capabilities or vulnerabilities of the model should be redacted. Commercially sensitive information could be shared with government or regulators, who could share this information with industry in an aggregated or anonymised form. Information shared at deployment could include: 1) A description of the model; 2) Information about safe practices for model usage (including domains of inappropriate and appropriate use, or guidelines on determining whether a use is appropriate); 3) Training details, including a detailed description of the training data and any biases they may encode; 4) The model’s biases, risks and limitations; 5) Pre-development and pre-deployment risk and impact assessment procedures; 6) Details of the evaluation process the frontier AI organisation conducted, including time and resources spent, information about the expertise and independence of people conducting the evaluations, the level of access given to evaluators, and anticipated limitations of the evaluations used; 7) Details about which, if any, domain experts and impacted stakeholders have been involved in the project’s design and risk and impact assessment; 8) The results of any internal or external evaluations that were conducted; 9) Holistic assessments of the models’ robustness, interpretability, and controllability,
drawing on more specific evaluation results; 9) Significant measures taken to mitigate potential harmful consequences of deployment, including accountability and verification mechanisms put in place, and internal governance processes carried out; 10) Capabilities and risks of the final, public-release model before and after safety mitigations, including a description of the mitigations to prevent accidents and misuse (e.g. available tools and their expected effectiveness); 11) Plans for ongoing, post-deployment monitoring of risks and capabilities and how the organisation will respond to future incidents (unless releasing this information would allow bad actors to circumvent post-deployment safety measures); 12) Descriptions of post-deployment access controls; 13) Expected compute requirements for running the model during deployment
",UK Government2023,4.2 Risk Disclosure
A0812_UK Government2023,Adjucation around Publicly Sharing Risks ,"Before sharing information, consider the risks of sharing this information and judge whether it is inappropriate to share certain pieces of information. In particular, it is important to consider potential harms from publicly sharing information about dangerous capabilities and methods for eliciting them, as this information could motivate or help other actors to acquire these capabilities. It is also important to consider potential harms from publicly sharing detailed information about how models were produced, as this may lower barriers to producing similar models. If the models have or could be modified to possess dangerous capabilities (e.g. biological capabilities or surveillance capabilities), then facilitating widespread distribution of the model in this way may be harmful. The National Protective Security Authority (NPSA) guidance on a security-minded approach to information management may prove helpful for sharing information appropriately.",UK Government2023,4.2 Risk Disclosure
A0813_UK Government2023,Principled Policies for Disclosure ,"Develop principled policies about what information to share publicly, with governments, or not at all. These policies could specify situations under which sharing some piece of information is subject to a risk assessment, along with guidelines for conducting the risk assessment and responding to it. It is important, however, to avoid creating risk assessment criteria and procedures that are overly strict and intensive to prevent excessive opacity. These policies could be guided and overseen by an independent review panel of multidisciplinary experts to ensure decisions made about or against information sharing are justifiable and oriented to optimal transparency.",UK Government2023,4.2 Risk Disclosure
A0814_UK Government2023,Disclosure of Information with AI-focused Government Authorities,"Share complete forms of this information with central AI-focused government authorities (including central government bodies, security agencies, and regulators) to enable robust government oversight of potentially high-risk areas of AI development and the processes in place to identify and manage those risks. These authorities could then further share information selectively with additional government bodies where relevant. Some particularly security-relevant pieces of information may need to be shared directly with security agencies, such as the cybersecurity measures being used in model development (which would make it easier for those agencies to identify potential security risks), or specific physical or cybersecurity threat incidents. Robust security measures are in place when sharing more sensitive information.",UK Government2023,4.2 Risk Disclosure
A0815_UK Government2023,Sharing of Limited Information,"Share more limited information with other AI organisations in order to facilitate learning and the development of best practices. This could include best practices and lessons learned in the development of risk assessment and mitigation measures and risk governance processes, some details about safety and security incidents (to enable increased awareness while protecting intellectual property), and highlights and lessons learned from risk assessments and capability evaluation. In general, it will be easier for organisations to share model-agnostic information with one another than model-specific information, which may be more commercially sensitive. It will also be important to consider that there is no privileged access to information that may give a competitive advantage to some firms.",UK Government2023,4.2 Risk Disclosure
A0818_UK Government2023,Sharing General Information with the Public,"Share more general versions of the information outlined with the public to enable public scrutiny and build public confidence in the safety and reliability of AI systems. This could include high-level summaries of risk assessment and mitigation processes, high-level details of safety and security incidents, summaries of risk governance processes, and general overviews of model purpose, use cases, and the results of risk assessments and capability evaluations.",UK Government2023,4.2 Risk Disclosure
A0863_UK Government2023,Cost-Benefit Analysis of Sharing Vulnerability Reports ,"Consider the ways in which sharing information about vulnerabilities can both exacerbate and mitigate risks. Sharing can alert would-be attackers, but also alert would-be victims and actors with the power to create defences. One particularly important factor is how easily fixable the model vulnerability is. If a vulnerability will take a very long time to fix, or cannot be fixed, then public reporting may not be justified.",UK Government2023,4.2 Risk Disclosure
A0864_UK Government2023,Development of Protocols for Disclosure of Model Vulnerability Information,"Developing – and publicly describe – protocols for deciding how to share model vulnerability information. These protocols may, for example, outline conditions under which information is to be shared with different actors depending on the type of harm or vulnerability identified.",UK Government2023,4.2 Risk Disclosure
A0865_UK Government2023,Mechanisms to Disclose Vulnerability Reports to Third Parties,"Put in place mechanisms to disclose information about vulnerabilities to relevant government authorities, law enforcement, and other affected organisations. This may be particularly relevant for vulnerabilities where public disclosure might increase the risk of harm.",UK Government2023,4.2 Risk Disclosure
A0866_UK Government2023,Public Sharing of Model Vulnerability Reporting Programs,"Publicly share general lessons learned from model vulnerability reporting programs. This might include, for example, lessons about challenges faced and the relative efficacy of different incentive strategies. Such sharing could be done via a mechanism similar to the National Institute for Science and Technology’s National Vulnerability Database in the US.",UK Government2023,4.2 Risk Disclosure
A0882_UK Government2023,Discretion around sharing Defense Tools,"Disseminate defensive tools responsibly, sometimes sharing them publicly and sometimes sharing them only with particular actors. In some cases, making a tool freely available (e.g. by open-sourcing it) may reduce its effectiveness by allowing malicious actors to study it and circumvent it.",UK Government2023,4.2 Risk Disclosure
A0888_UK Government2023,Public Sharing of Societal Impact Assessments ,"In the absence of sufficiently substantial downsides to sharing, frontier AI organisations are encouraged to share the products of this work broadly.",UK Government2023,4.2 Risk Disclosure
A0890_UK Government2023,Reporting of Broad Patterns of Misuse,"Report information on broad patterns of misuse. This information can help governments, the public, and other frontier AI organisations to better understand risks. Reports may focus on population-level metrics related to API usage, such as the rate of harmful inputs filtered or the number of users banned for misuse. This information may benefit from being presented in a clear and understandable way and made optimally accessible.",UK Government2023,4.2 Risk Disclosure
A0903_UK Government2023,Protocols for Sharing API users Information with government authorities,"Establish protocols for deciding when and how to share information about API users with relevant government authorities. These protocols may include covering circumstances under which information about a user is proactively shared with government bodies (e.g. cases where there is reason to think a user may be attempting to cause grave harm), and how to respond to government requests for data.",UK Government2023,4.2 Risk Disclosure
A0933_UK Government2023,Disclosure of Input Data Audits with External Stakeholders,"Share information about input data audits with users and external stakeholders. Some information could be included in transparency reports, such as high-level information about training data (including data sources), data auditing procedures, and measures taken to reduce risk (see Model Reporting and Information Sharing). More sensitive information may be shared directly with regulators and external auditors.",UK Government2023,4.2 Risk Disclosure
A1032_Uuk2024,Pre-registration of large training runs,"Registering upcoming training runs above a certain size with an appropriate state actor. Such reports could include descriptions of architecture, training compute, data collection and curation, training objectives and techniques, and planned risk management procedures.",Uuk2024,4.2 Risk Disclosure
A0009_Bengio2025,Incident Reporting,"The process of systematically documenting and sharing cases in which developing or deploying AI has caused direct or indirect harms.

Incident reporting is common in many domains, from human resources to cybersecurity. It has also become more common for AI. ",Bengio2025,4.3 Incident Reporting
A0030_Casper2025,Incident reporting,Frontier developers can be required to document and report on substantial incidents in a timely manner.,Casper2025,4.3 Incident Reporting
A0083_Eisenberg2025,Establish a general purpose incident response mechanism,"Establish processes to enable incident monitoring and reporting. This includes defining ”serious incidents” or set a threshold for formal reporting based on regulatory requirements to third-parties, regulators, and impacted individuals.",Eisenberg2025,4.3 Incident Reporting
A0085_Future of Life Institute2024,AI incident reporting,"Does your organization report AI incidents, adverse events, and near-misses related to frontier AI models to the appropriate government?",Future of Life Institute2024,4.3 Incident Reporting
A0106_Future of Life Institute2024,Cyber Threat Intelligence Sharing,Does your organization share cyber threat intelligence information with the appropriate government(s) and other leading AI firms?,Future of Life Institute2024,4.3 Incident Reporting
A0151_Future of Life Institute2024,Security breach disclosure,Does your organization disclose security breaches to the appropriate government(s)? Does this policy include reporting of near-misses?,Future of Life Institute2024,4.3 Incident Reporting
A0152_Future of Life Institute2024,Security breach disclosure,Does your organization disclose security breaches to the appropriate government(s)? Does this policy include reporting of near-misses?,Future of Life Institute2024,4.3 Incident Reporting
A0184_Schuett2023,Industry sharing of security information,AGI labs should share threat intelligence and information about security incidents with each other.*,Schuett2023,4.3 Incident Reporting
A0205_Schuett2023,Report safety incidents,AGI labs should report accidents and near misses to appropriate state actors and other AGI labs (e.g. via an AI incident database).,Schuett2023,4.3 Incident Reporting
A0263_EU AI Office2025,Serious Incident Reporting,"Signatories commit to setting up processes for keeping track of, documenting, and reporting to the AI Office and national competent authorities without undue delay relevant information about serious incidents throughout the entire model lifecycle and possible corrective measures to address them.",EU AI Office2025,4.3 Incident Reporting
A0408_NIST2024,Incident Communication Framework,"Establish organizational roles, policies, and procedures for communicating GAI incidents and performance to AI Actors and downstream stakeholders (including those potentially impacted), via community or official resources (e.g., AI incident database, AVID, CVE, NVD, or OECD AI incident monitor).",NIST2024,4.3 Incident Reporting
A0425_NIST2024,	Feedback Resource Allocation,"Allocate time and resources for outreach, feedback, and recourse processes in GAI system development.",NIST2024,4.3 Incident Reporting
A0438_NIST2024,Third-Party Incident Documentation,"Document incidents involving third-party GAI data and systems, including open-source data and open-source software",NIST2024,4.3 Incident Reporting
A0703_Barrett2024,Report & Respond to Incidents  ,"Incidents and errors
are communicated
to relevant AI actors,
including affected
communities. Processes
for tracking, responding
to, and recovering
from incidents and
errors are followed and
documented.",Barrett2024,4.3 Incident Reporting
A0761_NIST2024,Confabulation; Information Integrity,"Establish and maintain policies and procedures to record and track GAI system
reported errors, near-misses, and negative impacts.",NIST2024,4.3 Incident Reporting
A0762_NIST2024,Information Security; Data Privacy,"Report GAI incidents in compliance with legal and regulatory requirements (e.g.,
HIPAA breach reporting, e.g., OCR (2023) or NHTSA (2022) autonomous vehicle
crash reporting requirements.",NIST2024,4.3 Incident Reporting
A0808_UK Government2023,Reporting Security Details to Government Authorities,"Report any details of security or safety incidents or near-misses to relevant government authorities. This includes any compromise to the security of the organisation or its systems, or any incident where an AI system – deployed or not – causes substantial harm or is close to doing so. This will enable government authorities to build a clear picture of when safety and security incidents occur and make it easier to anticipate and mitigate future risks. Incident reports could include a description of the incident, the location, start and end date, details of any parties affected and harms occurred, any specific models involved, any relevant parties responsible for managing and responding to the incident, as well as ways in which the incident could have been avoided. It is important that incidents indicative of more severe risks are reported as soon as possible after they occur. High-level details of safety and security incidents - with sensitive information removed - could also be made public, such as have been shared in the AI incident database.",UK Government2023,4.3 Incident Reporting
A0838_UK Government2023,Open Lines of Communication for Product Security Feedback ,"Maintain open lines of communication for feedback regarding product security, both internally and externally to your organisation, including mechanisms for security researchers to report vulnerabilities and receive legal safe harbour for doing so, and for escalating issues to the wider community. Helping to share knowledge and threat information will strengthen the overall community’s ability to respond to AI security threats.",UK Government2023,4.3 Incident Reporting
A0854_UK Government2023,Jailbreaking Method Vulnerability Reports,"This process could have as wide a scope as is necessary and ensure that frontier AI organisations have the ability to respond appropriately to reports of vulnerabilities. The process could accept reports on any class of model vulnerabilities and methods for exploiting them, including: Reports for inducing models to bypass moderation features, which the frontier AI organisation has attempted to prevent through the use of filters or fine-tuning",UK Government2023,4.3 Incident Reporting
A0855_UK Government2023,Prompt Injection Vulnerability Reports,"This process could have as wide a scope as is necessary and ensure that frontier AI organisations have the ability to respond appropriately to reports of vulnerabilities. The process could accept reports on any class of model vulnerabilities and methods for exploiting them, including:Reports used by malicious actors to induce models to exhibit behaviours they want by presenting models with prompts that contain instructions to perform these behaviours",UK Government2023,4.3 Incident Reporting
A0856_UK Government2023,Privacy Attacks Vulnerability Reports,"This process could have as wide a scope as is necessary and ensure that frontier AI organisations have the ability to respond appropriately to reports of vulnerabilities. The process could accept reports on any class of model vulnerabilities and methods for exploiting them, including: methods for extracting information that should be private from models (e.g. sensitive information from training data or users’ private conversations with models)",UK Government2023,4.3 Incident Reporting
A0857_UK Government2023,Vulnerability Reports for Unaddressed Misuse Opportunities ,"This process could have as wide a scope as is necessary and ensure that frontier AI organisations have the ability to respond appropriately to reports of vulnerabilities. The process could accept reports on any class of model vulnerabilities and methods for exploiting them, including: methods for using the capabilities of models to cause harm, which have not already been addressed",UK Government2023,4.3 Incident Reporting
A0858_UK Government2023,Vulnerability Reports for Poisoning Attacks ,"This process could have as wide a scope as is necessary and ensure that frontier AI organisations have the ability to respond appropriately to reports of vulnerabilities. The process could accept reports on any class of model vulnerabilities and methods for exploiting them, including: Reports for when an adversary has manipulated training data in order to degrade model performance",UK Government2023,4.3 Incident Reporting
A0859_UK Government2023,Vulnerability Reports for Bias and Discrimination ,"This process could have as wide a scope as is necessary and ensure that frontier AI organisations have the ability to respond appropriately to reports of vulnerabilities. The process could accept reports on any class of model vulnerabilities and methods for exploiting them, including:Reports for when a model is exhibiting behaviours that reveal specific biases or discrimination regarding known protected characteristics",UK Government2023,4.3 Incident Reporting
A0860_UK Government2023,Vulnerability Reports for Performance Issues ,"This process could have as wide a scope as is necessary and ensure that frontier AI organisations have the ability to respond appropriately to reports of vulnerabilities. The process could accept reports on any class of model vulnerabilities and methods for exploiting them, including: when a model performs inadequately for a situation it is being deployed in e.g. a healthcare chatbot AI that provides incorrect information and causes harm to patients",UK Government2023,4.3 Incident Reporting
A0861_UK Government2023,"Vulnerability Reports for Controllability Issues (i.e., ""Misalignment"") ","This process could have as wide a scope as is necessary and ensure that frontier AI organisations have the ability to respond appropriately to reports of vulnerabilities. The process could accept reports on any class of model vulnerabilities and methods for exploiting them, including: when models apply their capabilities in ways that substantially diverge from what users intend or desire",UK Government2023,4.3 Incident Reporting
A0862_UK Government2023,Model Vulnerability Reports,"This process could have as wide a scope as is necessary and ensure that frontier AI organisations have the ability to respond appropriately to reports of vulnerabilities. The process could accept reports on any class of model vulnerabilities and methods for exploiting them, including: Establish clear, user-friendly, and publicly described processes for receiving model vulnerability reports drawing on established software vulnerability reporting processes. These processes can be built into – or take inspiration from – processes that organisations have built to receive reports of traditional software vulnerabilities. It is crucial that these policies are made publicly accessible and function effectively.
",UK Government2023,4.3 Incident Reporting
A0982_Gipiškis2024,Encourage reporting of critical vulnerabilities to the upstream provider or other relevant stakeholders,"Downstream AI system deployers can report critical vulnerabilities or incidents to the upstream model provider and other relevant regulators. This can con- tribute to safe use, and allow other downstream deployers to be informed about any potential problems.",Gipiškis2024,4.3 Incident Reporting
A1036_Uuk2024,Safety incident reports and security information sharing,"Disclosing reports about AI incidents, such as concrete harms and near misses as well as cyber threat intelligence and security incident reports with appropriate stakeholders such as select governments.",Uuk2024,4.3 Incident Reporting
A1037_Uuk2024,Safety incident reports and security information sharing,"Disclosing reports about AI incidents, such as concrete harms and near misses as well as cyber threat intelligence and security incident reports with appropriate stakeholders such as select governments.",Uuk2024,4.3 Incident Reporting
A0018_Bengio2025,Safety Cases,"Safety cases require developers to demonstrate safety. A safety case is a structured argument supported by evidence that a system is acceptably safe to operate in a particular context.

Safety cases are common in many industries, including defence, aerospace, and railways. ",Bengio2025,4.4 Governance Disclosure
A0097_Future of Life Institute2024,Commitments related to internal governance mechanisms to ensure effective implementation of AI safety frameworks,,Future of Life Institute2024,4.4 Governance Disclosure
A0098_Future of Life Institute2024,Commitments to involve external stakeholders in overseeing implementation of AI safety frameworks,,Future of Life Institute2024,4.4 Governance Disclosure
A0101_Future of Life Institute2024,Compliance to public commitments,Voluntary commitments given by firms and any evidence of noncompliance.,Future of Life Institute2024,4.4 Governance Disclosure
A0105_Future of Life Institute2024,Control/Alignment Strategy,"We assess whether the company has publicly shared their strategy for ensuring that ever more advanced artificial intelligence remains under human control or remains aligned, and summarize contents of any such documents. We exclude policy recommendations to governments and other stakeholders.",Future of Life Institute2024,4.4 Governance Disclosure
A0122_Future of Life Institute2024,Internal review,"Information regarding internal review mechanisms and audit functions that are relevant to decisions about the development and deployment of highly capable AI models. This includes ethics boards, board risk committees, and audit functions that test risk management practices.",Future of Life Institute2024,4.4 Governance Disclosure
A0125_Future of Life Institute2024,Leadership communications on catastrophic risks,We report whether leadership communicates to the public about potential catastrophic risks from advanced AI.,Future of Life Institute2024,4.4 Governance Disclosure
A0144_Future of Life Institute2024,Public explanation of governance structure,"Has your organization released a public resource explaining the firm’s governance structure? Such a resource should make transparent how important decisions regarding the development and deployment of frontier AI models are made. If yes, please share a URL.",Future of Life Institute2024,4.4 Governance Disclosure
A0167_Future of Life Institute2024,"Military, warfare & intelligence applications:",Any information related to engagements with militaries and intelligence agencies.,Future of Life Institute2024,4.4 Governance Disclosure
A0168_Future of Life Institute2024,Mission statement,Does the company’s mission statement explicitly prioritizes safety?,Future of Life Institute2024,4.4 Governance Disclosure
A0169_Future of Life Institute2024,Terms of Service,"We analyzed companies’ terms of service to identify any assurances about the quality, reliability, and accuracy of their products or services.",Future of Life Institute2024,4.4 Governance Disclosure
A0183_Schuett2023,Increasing levels of external scrutiny,AGI labs should increase the level of external scrutiny in proportion to the capabilities of their models.,Schuett2023,4.4 Governance Disclosure
A0200_Schuett2023,Publish alignment strategy,AGI labs should publish their strategies for ensuring that their systems are safe and aligned.*,Schuett2023,4.4 Governance Disclosure
A0202_Schuett2023,Publish results of external scrutiny,"AGI labs should publish the results or summaries of external scrutiny efforts, unless this would unduly reveal proprietary information or itself produce significant risk.*",Schuett2023,4.4 Governance Disclosure
A0203_Schuett2023,Publish views about AGI risk,"AGI labs should make public statements about their views on the risks and benefits from AGI, including the level of risk they are willing to take in its development.",Schuett2023,4.4 Governance Disclosure
A0212_Schuett2023,Statement about governance structure,AGI labs should make public statements about how they make high-stakes decisions regarding model development and deployment.*,Schuett2023,4.4 Governance Disclosure
A0255_Wiener2024,Publish a redacted copy of SSP,,Wiener2024,4.4 Governance Disclosure
A0340_Campos2025,External Disclosure,The first type of communication is external disclosure of the risks faced by the organization… this should be broadened to include risks to society from the company’s products.,Campos2025,4.4 Governance Disclosure
A0636_Barrett2024,Transparent RM Policies,"The risk management
process and its
outcomes are
established through
transparent policies,
procedures, and other
controls based on
organizational risk
priorities.",Barrett2024,4.4 Governance Disclosure
A0785_UK Government2023,Verification Mechanisms ,"Include verification mechanisms, such that external actors can have increased confidence that responsible capability scaling policies are executed as intended.",UK Government2023,4.4 Governance Disclosure
A0807_UK Government2023,Disclosure of Information about Internal Governance Processes,"Share information about how internal governance processes are set up with relevant government authorities. This will ensure that risks are appropriately identified, communicated and mitigated, and allow government and other external actors to identify gaps that might lead to risks being overlooked. This information could be updated regularly (e.g. every 12 months). This information could also be made public, provided sensitive details are removed.",UK Government2023,4.4 Governance Disclosure
A0972_Gipiškis2024,Release strategy disagreement between developers,"Developers of restricted-access models with similar capabilities may disagree about the strategy or precautions to take for model release, especially in the case of competitive pressure or minimal safety regulation oversight. In such a case, if only a single developer releases an equally capable model unrestricted, malicious actors can use it instead of restricted-access alternatives [88].",Gipiškis2024,4.4 Governance Disclosure
A1039_Uuk2024,Sharing safety cases,Disclosing to a regulator how high-stakes decisions regarding model development and deployment are made.,Uuk2024,4.4 Governance Disclosure
A1041_Uuk2024,Transparent governance structures,Disclosing to a regulator how high-stakes decisions regarding model development and deployment are made.,Uuk2024,4.4 Governance Disclosure
A0074_Eisenberg2025,Establish third-party assessment and management framework,"Establish comprehensive procedures for documenting, assessing, and managing upstream providers and dependencies in the AI system value chain, including transparency requirements, compliance verification, dependency tracking, and contingency planning.",Eisenberg2025,4.5 Third-Party System Access
A0116_Future of Life Institute2024,Government access to AI systems,Is the firm proactively granting government officials free access to its most capable systems so the government can better understand what the technology is capable of?,Future of Life Institute2024,4.5 Third-Party System Access
A0135_Future of Life Institute2024,Post-deployment external researcher access,"Any programs that support good faith safety research by external stakeholders. We report available funding, depth of model access, model versions, technical infrastructure, and any technical or legal safe harbors designed to mitigate barriers to safety research imposed by usage policy enforcement, interaction-logging, and stringent terms of service.",Future of Life Institute2024,4.5 Third-Party System Access
A0154_Future of Life Institute2024,Support for Independent AI Safety Research,Does your organization support trusted independent AI safety researchers by allowing them to use your firm’s most capable systems free of charge or at a strongly discounted rate and not disabling their accounts if they trigger safety-monitoring systems? Please roughly indicate the current number of such collaborations with independent safety researchers your organization supports.,Future of Life Institute2024,4.5 Third-Party System Access
A0156_Future of Life Institute2024,Third-party Dangerous Capabilities Assessment,"Has your organization collaborated with independent third-party organizations to assess your most capable AI model for dangerous capabilities as part of your pre-deployment risk assessment? If so, please provide the names of the organizations you worked with. Please also comment on the depth of model access provided to these organizations (e.g., access to fine-tuning or access to model without safety filters).",Future of Life Institute2024,4.5 Third-Party System Access
A0206_Schuett2023,Researcher model access,AGI labs should give independent researchers API access to deployed models.,Schuett2023,4.5 Third-Party System Access
A0218_Schuett2023,Inter-lab scrutinies,AGI labs should allow researchers from other labs to scrutinize powerful models before deployment.*,Schuett2023,4.5 Third-Party System Access
A0285_Campos2025, Independent Third-Party Evaluations,"Independent third parties should vet evaluation protocols. These third parties should also be granted permission and resources to independently perform their evaluations, verifying the accuracy of the results.",Campos2025,4.5 Third-Party System Access
A0797_UK Government2023,Appropriate Safeguards Against External Evaluations,"Ensure that there are appropriate safeguards against external evaluations leading to unintended widespread distribution of models. Allowing external evaluators to download models onto their own hardware increases the chance of the models being stolen or leaked. Therefore, unless adequate security against widespread model distribution can be assured, external evaluators could only be allowed to access models through interfaces that prevent exfiltration (such as current API access methods). It may be appropriate to limit evaluators’ access to information that could indirectly facilitate widespread model distribution in other ways, such as requiring in-depth KYC checks or watermarking copies of the model.",UK Government2023,4.5 Third-Party System Access
A0799_UK Government2023,Fine-tuning by External Evaluators,Give external evaluators the ability to securely “fine-tune” the AI systems being tested. Evaluators cannot fully assess risks associated with widespread model distribution if they cannot fine-tune the model. This may involve providing external evaluators with access to capable infrastructure to enable fine-tuning.,UK Government2023,4.5 Third-Party System Access
A0800_UK Government2023,External Evaluation of Model without Safety Regulations,"Give external evaluators access to versions of the model that lack safety mitigations. Where possible, sharing these versions of a model gives evaluators insight into the risks that might be created if users find ways to circumvent safeguards (i.e. “jailbreak” the model). If the model is open-sourced, leaked, or stolen, users may also simply be able to remove or bypass the safety mitigations.",UK Government2023,4.5 Third-Party System Access
A0801_UK Government2023,External Evaluation of Model Families,"Give external evaluators access to model families and internal metrics. Frontier AI organisations often develop “model families” where multiple models differ along only one or two dimensions – such as parameters, data, or training compute. Evaluating such a model family would enable scaling analysis to better forecast future performance, capabilities and risks.",UK Government2023,4.5 Third-Party System Access
A0802_UK Government2023,External Evaluation of Deployed System Components ,"Give external evaluators the ability to study all of the components of deployed systems, where possible. Deployed AI systems typically combine a core model with smaller models and other software components, including moderation filters, user interfaces to incentivise particular user behaviour, and plug-ins for extension capabilities like web browsing or code execution. For example, a red team cannot find all the flaws in the defences of a system if they aren’t able to test all of its different components. It is important to consider the need to balance external evaluators’ ability to access all components of the system against the need to protect information that would allow bypassing model defences.",UK Government2023,4.5 Third-Party System Access
A0816_UK Government2023,Sharing with Independent Third-Parties,Share specific information with independent third-parties where this aids evaluation and technical audit (see Model Evaluations and Red Teaming). This may require sharing much of the same information that is shared with governments in full but may be shared on a case-by- case basis.,UK Government2023,4.5 Third-Party System Access
A0887_UK Government2023,Equitable Access to Frontier AI Systems ,"Transparent and fair processes for researchers to get restricted access to AI systems are important. To ensure systems are appropriately understood, particular attention could be paid to promote academic freedom and diversity of thought, for example, not withholding access based on previous or expected criticism and encouraging different types of academics, civil society groups and independent researchers to study AI systems.",UK Government2023,4.5 Third-Party System Access
A0932_UK Government2023,Independent Data Input Audits ,"Facilitate independent data input audits from external parties. Since training data constitutes sensitive intellectual property, it is important to implement appropriate technical and organisational safeguards to ensure privacy and security when sharing the training data. In order to protect sensitive information, frontier AI organisations could explore the possibility of providing synthetic datasets to auditors with sensitive information removed, or providing auditors with privacy-preserving access to the training data.",UK Government2023,4.5 Third-Party System Access
A0944_Gipiškis2024,Pre-deployment access by third-party auditors,"Prior to full deployment of general-purpose AI models, a group of third-party auditors who are not selected by the GPAI model provider could get early access to AI models in order to evaluate them from a variety of different perspectives and with diverse interests [30, 157]. This prevents cases where the developers of AI models select auditors that are especially favorable to the developers, which could result in biased or incomplete evaluations, or contribute to an unjustified public perception of the capabilities and risks of the model.",Gipiškis2024,4.5 Third-Party System Access
A1018_Uuk2024,Advanced model access for vetted external researchers,"Examples of advanced access rights could include any of the following: increased control over sampling, access to fine-tuning functionality, the ability to inspect and modify model internals, access to training data, or additional features like stable model versions.",Uuk2024,4.5 Third-Party System Access
A1043_Uuk2024,Vetted researcher access,"Giving good faith, public interest evaluation researchers access to black-box research APIs that provide technical and legal safe harbours to limit barriers imposed by usage policy enforcement, logging, and stringent terms of service.",Uuk2024,4.5 Third-Party System Access
A0029_Casper2025,Documentation comparison in court,"To incentivize a race to the top where frontier developers pursue established best safety practices, courts can be given the power to compare documentation for defendants with that of peer developers.",Casper2025,4.6 User Rights & Recourse
A0052_Eisenberg2025,Establish AI decision explanation framework,"Implement mechanisms and tools for generating human-understandable explanations of AI system decisions, including feature importance, decision paths, confidence levels, and clear attribution of data sources and their characteristics used during inference.",Eisenberg2025,4.6 User Rights & Recourse
A0057_Eisenberg2025,Establish universal access and performance design framework,"Establish and follow a structured framework ensuring the AI system is designed and developed to deliver consistent, high-quality performance and accessibility for all intended user groups, regardless of their characteristics or circumstances.",Eisenberg2025,4.6 User Rights & Recourse
A0066_Eisenberg2025,Implement AI System Disclosure Requirements,"Deploy mechanisms to ensure clear, timely disclosure of AI system use to end users, including automated notifications of AI involvement in interactions, explicit identification of AI-generated content, and clear communication of when users are interacting with AI systems.",Eisenberg2025,4.6 User Rights & Recourse
A0069_Eisenberg2025,Establish user rights and recourse framework,"Implement comprehensive mechanisms for user reporting, feedback collection, incident investigation, and recourse provision, including clear procedures for users to report issues, request explanations or corrections, appeal decisions, and receive appropriate remediation. The system should handle various types of user concerns including system errors, unfair treatment, privacy violations, and safety issues.",Eisenberg2025,4.6 User Rights & Recourse
A0416_NIST2024,User Feedback Mechanism ,Establish policies for user feedback mechanisms for GAI systems which include thorough instructions and any mechanisms for recourse.,NIST2024,4.6 User Rights & Recourse
A0426_NIST2024,User Interaction Documentation,"Document interactions with GAI systems to users prior to interactive activities, particularly in contexts involving more significant risks.",NIST2024,4.6 User Rights & Recourse
A0477_NIST2024,Use Disclosure Consideration,"Consider disclosing use of GAI to end users in relevant contexts, while considering the objective of disclosure, the context of use, the likelihood and magnitude of the risk posed, the audience of the disclosure, as well as the frequency of the disclosures.",NIST2024,4.6 User Rights & Recourse
A0497_NIST2024,Participation Withdrawal Options,Provide human subjects with options to withdraw participation or revoke their consent for present or future use of their data in GAI applications.,NIST2024,4.6 User Rights & Recourse
A0687_Barrett2024,Enable User Feedback & Appeals,"Feedback processes for
end users and impacted
communities to report
problems and appeal
system outcomes are
established and inte-
grated into AI system
evaluation metrics.",Barrett2024,4.6 User Rights & Recourse
A0720_NIST2024,Human-AI Configuration; Information Integrity; Harmful Bias and Homogenization,"Record and integrate structured feedback about content provenance from
operators, users, and potentially impacted communities through the use of
methods such as user research studies, focus groups, or community forums.
Actively seek feedback on generated content quality and potential biases.
Assess the general awareness among end users and impacted communities
about the availability of these feedback channels.",NIST2024,4.6 User Rights & Recourse
A0725_NIST2024,Human-AI Configuration; Information Security,"Verify and document the incorporation of results of structured public feedback
exercises into design, implementation, deployment approval (“go”/“no-go”
decisions), monitoring, and decommission decisions. (AI Deployment, Domain Experts, End-Users, Operation and Monitoring, TEVV) ",NIST2024,4.6 User Rights & Recourse
A0735_NIST2024,Human-AI Configuration; Harmful Bias and Homogenization,"Use structured feedback mechanisms to solicit and capture user input about AI generated content to detect subtle shifts in quality or alignment with
community and societal values. (AI Deployment, AI Impact Assessment, Governance and Oversight, Operation and Monitoring). ",NIST2024,4.6 User Rights & Recourse
A0738_NIST2024,Human-AI Configuration,"Establish and maintain communication plans to inform AI stakeholders as part of
the deactivation or disengagement process of a specific GAI system (including for
open-source models) or context of use, including reasons, workarounds, user
access removal, alternative processes, contact information, etc.",NIST2024,4.6 User Rights & Recourse
A0834_UK Government2023,Security Responsible User Communication ,Communicate clearly to users which elements of security they are responsible for and where and how their data may be used or accessed.,UK Government2023,4.6 User Rights & Recourse
A0900_UK Government2023,Communication of and Appeal for API access Reduction Policies,"Communicate API access reduction policies – and allow users to appeal access reductions. Users should be able to understand the reasons why their access may be reduced and be able to appeal access reductions if they believe policies have not been applied correctly. Alternatively, users may be able to perform actions to mitigate misuse risks, such as providing evidence to justify behaviour.",UK Government2023,4.6 User Rights & Recourse
A0905_UK Government2023,User Informed Rapid Rollback of models ,"Inform end users that rapid rollback or withdrawal of models may be necessary, and that the disruption to its end users will be minimised as far as safely possible. This is especially important where models are deployed in safety critical domains.",UK Government2023,4.6 User Rights & Recourse
A0041_Casper2025,AI governance institutes,"National governments (or international coalitions) can create AI governance institutes to research risks, evaluate systems, and curate best risk management practices that developers are encouraged to adhere to.",Casper2025,X.X Control not otherwise categorized
A0047_Eisenberg2025,Establish AI system integration framework,"Define and implement a comprehensive framework for AI system integration including architecture review, compatibility testing, and integration validation processes.",Eisenberg2025,X.X Control not otherwise categorized
A0048_Eisenberg2025,Implement AI system lifecycle management,"Deploy systematic processes for AI system maintenance, updates, and retraining, including version control, deployment pipelines, and performance monitoring to ensure consistent system reliability and performance.",Eisenberg2025,X.X Control not otherwise categorized
A0049_Eisenberg2025,Implement scalable AI infrastructure,"Apply architecture and infrastructure practices to ensure AI systems can scale effectively, including load testing, resource monitoring, and capacity planning to maintain performance under increased demand.",Eisenberg2025,X.X Control not otherwise categorized
A0070_Eisenberg2025,Implement AI literacy and competency program,"Implement comprehensive training and education programs to ensure personnel develop and maintain appropriate levels of AI literacy, risk awareness, and operational competency. This includes role-based training on AI capabilities, limitations, safety protocols, ethical considerations, and proper system usage.",Eisenberg2025,X.X Control not otherwise categorized
A0155_Future of Life Institute2024,Supporting external safety research,We note actions by which the firm supports external existentialsafety-relevant researchers.,Future of Life Institute2024,X.X Control not otherwise categorized
A0170_Future of Life Institute2024,Testimonies to policymakers,Public information related to direct communication with policymakers. We note whether company leadership used the opportunity to inform policymakers about the potential for catastrophic risks from advanced AI. Note that we have selected the most explicit risk-related statements. These do reflect overall communication strategies.,Future of Life Institute2024,X.X Control not otherwise categorized
A0174_Schuett2023,Avoiding Hype,AGI labs should avoid releasing powerful models in a way that is likely to create hype around AGI (e.g. by overstating results or announcing them in attention-grabbing ways).,Schuett2023,X.X Control not otherwise categorized
A0337_Campos2025,Company Risk Culture,All aspects of culture are ultimately driven by the ‘tone at the top’. This can be defined as ‘top management’s way to express [...] values pursued in the organization and provide guidance to employees’…,Campos2025,X.X Control not otherwise categorized
A0653_Barrett2024,Prioritize Interdisciplinary Collaboration,"Interdisciplinary AI actors,
competencies, skills, and
capacities for establish-
ing context reflect de-
mographic diversity and
broad domain and user
experience expertise,
and their participation
is documented. Oppor-
tunities for interdisci-
plinary collaboration are
prioritized.",Barrett2024,X.X Control not otherwise categorized
A0880_UK Government2023,External Input into Tool Development,Work closely with external actors who will need to deploy the tools to ensure that they are usable and suit their needs. For instance collaborating closely with social media organisations to help them produce more capable tools for identifying AI-generated content.,UK Government2023,X.X Control not otherwise categorized